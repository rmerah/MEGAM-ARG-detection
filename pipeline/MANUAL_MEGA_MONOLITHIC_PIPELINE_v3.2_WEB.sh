#!/bin/bash
#===============================================================================
#
#   â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ–ˆâ•—
#   â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ•‘
#   â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ–ˆâ–ˆâ•”â–ˆâ–ˆâ•‘
#   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘
#   â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â•šâ•â• â–ˆâ–ˆâ•‘
#   â•šâ•â•     â•šâ•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•šâ•â•     â•šâ•â•
#
#            ARG DETECTION PIPELINE v3.2
#
#   Antimicrobial Resistance Genes Detection & Analysis
#   Multi-Input Support (SRA, GenBank, Assemblies, Local FASTA)
#
#===============================================================================
#
#   Auteur    : Rachid Merah
#   Email     : rachid.merah77@gmail.com
#   Version   : 3.2
#   Date      : 2026-01-28
#   Licence   : MIT
#
#===============================================================================
#
#   DESCRIPTION:
#   Pipeline complet pour la dÃ©tection et l'analyse des gÃ¨nes de rÃ©sistance
#   aux antimicrobiens (ARG) Ã  partir de donnÃ©es gÃ©nomiques.
#
#   FONCTIONNALITÃ‰S v3.2:
#   âœ… Support multi-entrÃ©es : SRA (SRR*), GenBank (CP*, NC*, NZ_*),
#      Assemblages (GCA_*), fichiers FASTA locaux
#   âœ… Argument CLI : bash script.sh <SAMPLE_ID ou chemin FASTA>
#   âœ… Mode interactif si aucun argument fourni
#   âœ… VÃ©rification/crÃ©ation automatique de l'architecture
#   âœ… SystÃ¨me de gestion des versions (timestamp)
#   âœ… DÃ©tection automatique des bases de donnÃ©es
#   âœ… TÃ©lÃ©chargement automatique des bases manquantes
#   âœ… Menu interactif de gestion
#   âœ… Archivage automatique
#   âœ… Nettoyage des anciens rÃ©sultats
#
#   USAGE:
#     bash script.sh SRR28083254      # DonnÃ©es SRA (FASTQ)
#     bash script.sh CP133916.1       # SÃ©quence GenBank (FASTA)
#     bash script.sh GCA_000005845.2  # Assemblage NCBI (FASTA)
#     bash script.sh /chemin/vers/assembly.fasta  # Fichier local
#     bash script.sh                  # Mode interactif
#
#   MODULES:
#     0. TÃ©lÃ©chargement/PrÃ©paration des donnÃ©es
#     1. ContrÃ´le qualitÃ© (FastQC, fastp, Kraken2, MultiQC)
#     2. Assemblage (SPAdes, QUAST)
#     3. Annotation (Prokka)
#     4. DÃ©tection ARG (AMRFinderPlus, ResFinder, CARD, etc.)
#     5. Variant Calling (Snippy)
#     6. Analyse et rapports
#
#===============================================================================

#===============================================================================
# SECTION 1 : INITIALISATION ET CONFIGURATION CRITIQUE
#===============================================================================

# ArrÃªter immÃ©diatement en cas d'erreur
set -euo pipefail

# Initialiser conda AVANT de l'utiliser
eval "$(conda shell.bash hook)" 2>/dev/null || true

# Trap pour afficher les erreurs
trap 'echo "âŒ ERREUR: Script Ã©chouÃ© Ã  la ligne $LINENO"; exit 1' ERR

#===============================================================================
# SECTION 2 : PARSING DES ARGUMENTS ET MODE INTERACTIF
#===============================================================================

# Fonction d'aide
show_help() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "PIPELINE ARG v3.2 - AIDE"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "USAGE:"
    echo "  bash $0 <SAMPLE_ID ou chemin_fichier> [OPTIONS]"
    echo ""
    echo "TYPES D'ENTRÃ‰ES SUPPORTÃ‰S:"
    echo "  SRR*, ERR*, DRR*     â†’ DonnÃ©es SRA (reads FASTQ)"
    echo "  CP*, NC*, NZ_*       â†’ SÃ©quence GenBank (FASTA assemblÃ©)"
    echo "  GCA_*, GCF_*         â†’ Assemblage NCBI (FASTA assemblÃ©)"
    echo "  /chemin/fichier.fasta â†’ Fichier FASTA local"
    echo "  (aucun argument)     â†’ Mode interactif"
    echo ""
    echo "EXEMPLES:"
    echo "  bash $0 SRR28083254"
    echo "  bash $0 CP133916.1"
    echo "  bash $0 GCA_000005845.2"
    echo "  bash $0 /home/user/my_assembly.fasta"
    echo ""
    echo "COMMANDES:"
    echo "  update               Mettre Ã  jour toutes les bases de donnÃ©es"
    echo "  update kraken        Mettre Ã  jour uniquement Kraken2"
    echo "  update amrfinder     Mettre Ã  jour uniquement AMRFinder"
    echo "  update card          Mettre Ã  jour uniquement CARD (RGI)"
    echo "  update mlst          Mettre Ã  jour uniquement MLST"
    echo "  update pointfinder   Mettre Ã  jour uniquement PointFinder"
    echo "  update kma           Mettre Ã  jour uniquement KMA/ResFinder"
    echo ""
    echo "OPTIONS:"
    echo "  -h, --help           Afficher cette aide"
    echo "  -t, --threads N      Nombre de threads (dÃ©faut: 8)"
    echo "  -w, --workdir PATH   RÃ©pertoire de travail"
    echo "  -f, --force, -y      Mode non-interactif (accepte automatiquement)"
    echo ""
    echo "OPTIONS PROKKA (annotation):"
    echo "  --prokka-mode MODE   Mode d'annotation Prokka:"
    echo "                         auto    â†’ DÃ©tecte l'espÃ¨ce via Kraken2 (dÃ©faut)"
    echo "                         generic â†’ Mode universel (toutes bactÃ©ries)"
    echo "                         ecoli   â†’ Escherichia coli K-12 (legacy)"
    echo "                         custom  â†’ Utilise --prokka-genus/species"
    echo "  --prokka-genus STR   Genre bactÃ©rien (avec --prokka-mode custom)"
    echo "  --prokka-species STR EspÃ¨ce bactÃ©rienne (avec --prokka-mode custom)"
    echo ""
    echo "EXEMPLES AVANCÃ‰S:"
    echo "  bash $0 SRR28083254 --prokka-mode auto"
    echo "  bash $0 CP133916.1 --prokka-mode generic"
    echo "  bash $0 GCA_000005845.2 --prokka-mode custom --prokka-genus Salmonella --prokka-species enterica"
    echo ""
    exit 0
}

# Parsing des arguments
INPUT_ARG=""
INPUT_ARG2=""
THREADS="${THREADS:-8}"
# RÃ©pertoire du script (permet l'exÃ©cution portable depuis n'importe oÃ¹)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
WORK_DIR="${WORK_DIR:-$SCRIPT_DIR}"
# RÃ©pertoire contenant les scripts Python
PYTHON_DIR="$(dirname "$SCRIPT_DIR")/python"
FORCE_MODE=true  # Default true for web interface
# Mode Prokka : "auto" (dÃ©tection Kraken2), "generic" (universel), "ecoli" (E. coli par dÃ©faut)
PROKKA_MODE="${PROKKA_MODE:-auto}"
# Variables pour Prokka (peuvent Ãªtre dÃ©finies par l'utilisateur)
PROKKA_GENUS=""
PROKKA_SPECIES=""

while [[ $# -gt 0 ]]; do
    case $1 in
        -h|--help)
            show_help
            ;;
        -t|--threads)
            THREADS="$2"
            shift 2
            ;;
        -w|--workdir)
            WORK_DIR="$2"
            shift 2
            ;;
        -f|--force|-y|--yes)
            FORCE_MODE=true
            shift
            ;;
        --prokka-mode)
            PROKKA_MODE="$2"
            if [[ ! "$PROKKA_MODE" =~ ^(auto|generic|ecoli|custom)$ ]]; then
                echo "âŒ Mode Prokka invalide: $PROKKA_MODE"
                echo "   Valeurs acceptÃ©es: auto, generic, ecoli, custom"
                exit 1
            fi
            shift 2
            ;;
        --prokka-genus)
            PROKKA_GENUS="$2"
            shift 2
            ;;
        --prokka-species)
            PROKKA_SPECIES="$2"
            shift 2
            ;;
        -*)
            echo "Option inconnue: $1"
            show_help
            ;;
        *)
            if [[ -z "$INPUT_ARG" ]]; then
                INPUT_ARG="$1"
            else
                INPUT_ARG2="$1"
            fi
            shift
            ;;
    esac
done

# Variable pour stocker la commande update (sera traitÃ©e aprÃ¨s dÃ©finition des fonctions)
UPDATE_MODE=false
if [[ "$INPUT_ARG" == "update" ]]; then
    UPDATE_MODE=true
    # Sourcer les fonctions nÃ©cessaires et traiter la commande update immÃ©diatement
    # (Le reste du script sera ignorÃ© via la gestion plus loin)
fi

# Si mode update, on skip le mode interactif et la dÃ©tection de type
if [[ "$UPDATE_MODE" == true ]]; then
    # Continuer vers les dÃ©finitions de fonctions, le traitement se fera lÃ -bas
    :
# Si aucun argument, mode interactif
elif [[ -z "$INPUT_ARG" ]]; then
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "PIPELINE ARG v3.2 - MODE INTERACTIF"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Types d'entrÃ©es supportÃ©s:"
    echo "  1) SRR*, ERR*, DRR*     â†’ DonnÃ©es SRA (reads FASTQ)"
    echo "  2) CP*, NC*, NZ_*       â†’ SÃ©quence GenBank (FASTA)"
    echo "  3) GCA_*, GCF_*         â†’ Assemblage NCBI (FASTA)"
    echo "  4) /chemin/fichier.fasta â†’ Fichier FASTA local"
    echo ""
    read -p "Entrez le SAMPLE_ID ou le chemin du fichier FASTA: " INPUT_ARG

    if [[ -z "$INPUT_ARG" ]]; then
        echo "âŒ ERREUR: Aucune entrÃ©e fournie"
        exit 1
    fi

    # Choix du mode Prokka en mode interactif
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "MODE D'ANNOTATION PROKKA"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Choisissez le mode d'annotation pour Prokka:"
    echo "  1) auto    â†’ DÃ©tection automatique de l'espÃ¨ce via Kraken2 (recommandÃ©)"
    echo "  2) generic â†’ Mode universel (toutes bactÃ©ries, sans spÃ©cifier l'espÃ¨ce)"
    echo "  3) ecoli   â†’ Escherichia coli K-12 (mode legacy)"
    echo "  4) custom  â†’ SpÃ©cifier manuellement le genre et l'espÃ¨ce"
    echo ""
    read -p "Votre choix (1-4) [dÃ©faut: 1]: " prokka_choice

    case "${prokka_choice:-1}" in
        1)
            PROKKA_MODE="auto"
            echo "âœ… Mode Prokka: auto (dÃ©tection Kraken2)"
            ;;
        2)
            PROKKA_MODE="generic"
            echo "âœ… Mode Prokka: generic (universel)"
            ;;
        3)
            PROKKA_MODE="ecoli"
            echo "âœ… Mode Prokka: ecoli (E. coli K-12)"
            ;;
        4)
            PROKKA_MODE="custom"
            read -p "Genre bactÃ©rien (ex: Salmonella): " PROKKA_GENUS
            read -p "EspÃ¨ce bactÃ©rienne (ex: enterica): " PROKKA_SPECIES
            if [[ -z "$PROKKA_GENUS" ]]; then
                echo "âš ï¸  Genre non spÃ©cifiÃ©, passage en mode generic"
                PROKKA_MODE="generic"
            else
                echo "âœ… Mode Prokka: custom ($PROKKA_GENUS $PROKKA_SPECIES)"
            fi
            ;;
        *)
            PROKKA_MODE="auto"
            echo "âœ… Mode Prokka: auto (dÃ©faut)"
            ;;
    esac
fi

#===============================================================================
# SECTION 3 : DÃ‰TECTION DU TYPE D'ENTRÃ‰E
#===============================================================================

detect_input_type() {
    local input="$1"

    # Fichier local existant
    if [[ -f "$input" ]]; then
        echo "local_fasta"
        return 0
    fi

    # SRA (SRR, ERR, DRR)
    if [[ "$input" =~ ^[SED]RR[0-9]+ ]]; then
        echo "sra"
        return 0
    fi

    # GenBank sequence (CP, NC, NZ_)
    if [[ "$input" =~ ^(CP|NC_|NZ_)[0-9]+ ]]; then
        echo "genbank"
        return 0
    fi

    # NCBI Assembly (GCA_, GCF_)
    if [[ "$input" =~ ^GC[AF]_[0-9]+ ]]; then
        echo "assembly"
        return 0
    fi

    # Chemin de fichier qui n'existe pas encore
    if [[ "$input" == *"/"* ]] || [[ "$input" == *".fasta"* ]] || [[ "$input" == *".fna"* ]]; then
        echo "local_fasta"
        return 0
    fi

    # Type inconnu
    echo "unknown"
    return 1
}

# Skip la dÃ©tection de type si on est en mode update
if [[ "$UPDATE_MODE" == true ]]; then
    INPUT_TYPE="update"
else
    INPUT_TYPE=$(detect_input_type "$INPUT_ARG")

    if [[ "$INPUT_TYPE" == "unknown" ]]; then
        echo "âŒ ERREUR: Type d'entrÃ©e non reconnu: $INPUT_ARG"
        echo "   Types supportÃ©s: SRR*, CP*, NC*, NZ_*, GCA_*, GCF_*, ou fichier FASTA"
        exit 1
    fi
fi

# DÃ©finir SAMPLE_ID selon le type (skip si mode update)
if [[ "$UPDATE_MODE" == true ]]; then
    SAMPLE_ID="update"
else
    case "$INPUT_TYPE" in
        local_fasta)
            # Extraire le nom du fichier sans extension
            SAMPLE_ID=$(basename "$INPUT_ARG" | sed 's/\.\(fasta\|fna\|fa\)$//')
            LOCAL_FASTA_PATH="$INPUT_ARG"
            ;;
        *)
            SAMPLE_ID="$INPUT_ARG"
            ;;
    esac

    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "DÃ‰TECTION DU TYPE D'ENTRÃ‰E"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "  EntrÃ©e: $INPUT_ARG"
    echo "  Type dÃ©tectÃ©: $INPUT_TYPE"
    echo "  Sample ID: $SAMPLE_ID"
    echo ""
fi

#===============================================================================
# SECTION 4 : VARIABLES DE CONFIGURATION
#===============================================================================

# VERSIONING - SystÃ¨me simplifiÃ© avec compteur d'essais
# Fonction pour trouver le prochain numÃ©ro d'essai
get_next_run_number() {
    local sample_id="$1"
    local outputs_dir="$WORK_DIR/outputs"

    # Si le dossier outputs n'existe pas encore
    if [[ ! -d "$outputs_dir" ]]; then
        echo "1"
        return
    fi

    # Trouver le plus grand numÃ©ro de run existant au format exact SAMPLE_N
    # Les anciens formats (ex: SAMPLE_v3.2_20260128_124016) sont ignorÃ©s
    # IMPORTANT: Cet algorithme doit rester synchronisÃ© avec
    # get_next_run_number() dans backend/pipeline_launcher.py
    local max_run=0
    for dir in "$outputs_dir"/${sample_id}_*/; do
        [[ -d "$dir" ]] || continue
        local dirname
        dirname=$(basename "$dir")
        local suffix="${dirname#${sample_id}_}"
        # VÃ©rifier que le suffixe est uniquement un entier
        if [[ "$suffix" =~ ^[0-9]+$ ]]; then
            if (( suffix > max_run )); then
                max_run=$suffix
            fi
        fi
    done

    echo "$((max_run + 1))"
}

# DÃ©terminer le numÃ©ro d'essai
RUN_NUMBER=$(get_next_run_number "$SAMPLE_ID")
RESULTS_VERSION="${RESULTS_VERSION:-${RUN_NUMBER}}"

# Timestamp pour les logs (conservÃ© pour traÃ§abilitÃ© interne)
TIMESTAMP=$(date '+%Y%m%d_%H%M%S')

# RÃ©pertoires principaux (nomenclature simplifiÃ©e)
DATA_DIR="$WORK_DIR/data"
RESULTS_DIR="$WORK_DIR/outputs/${SAMPLE_ID}_${RESULTS_VERSION}"
DB_DIR="$WORK_DIR/databases"
REFERENCE_DIR="$WORK_DIR/references"
ARCHIVE_DIR="$WORK_DIR/archives"
LOG_DIR="$RESULTS_DIR/logs"

# Bases de donnÃ©es (seront configurÃ©es par interactive_database_setup)
KRAKEN_DB=""
AMRFINDER_DB=""
CARD_DB=""
POINTFINDER_DB=""
MLST_DB=""
REFERENCE_GENOME="$REFERENCE_DIR/ecoli_k12.fasta"

# Fichiers de log
LOG_FILE="$LOG_DIR/pipeline_${TIMESTAMP}.log"
ERROR_LOG="$LOG_DIR/pipeline_errors.log"

# Variable pour indiquer si on utilise un FASTA prÃ©-assemblÃ©
IS_ASSEMBLED_INPUT=false
if [[ "$INPUT_TYPE" == "genbank" ]] || [[ "$INPUT_TYPE" == "assembly" ]] || [[ "$INPUT_TYPE" == "local_fasta" ]]; then
    IS_ASSEMBLED_INPUT=true
fi

# Variable pour l'espÃ¨ce dÃ©tectÃ©e par Kraken2 (initialisÃ©e vide)
DETECTED_SPECIES=""

#===============================================================================
# SECTION 5 : VÃ‰RIFICATION ET CRÃ‰ATION DE L'ARCHITECTURE
#===============================================================================

# Si mode update, on saute directement vers le traitement (aprÃ¨s dÃ©finition des fonctions)
# Le code qui suit est pour le pipeline normal uniquement
if [[ "$UPDATE_MODE" == true ]]; then
    # Les variables essentielles sont dÃ©finies, on peut continuer
    # Le traitement se fera aprÃ¨s la dÃ©finition des fonctions de mise Ã  jour
    :
else
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "VÃ‰RIFICATION ET CRÃ‰ATION DE L'ARCHITECTURE"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
fi

# Fonction pour vÃ©rifier et crÃ©er l'architecture
setup_directory_structure() {
    local missing_dirs=0
    local created_dirs=0

    # Liste des rÃ©pertoires requis
    local required_dirs=(
        "$WORK_DIR"
        "$DATA_DIR"
        "$DB_DIR"
        "$DB_DIR/kraken2_db"
        "$REFERENCE_DIR"
        "$ARCHIVE_DIR"
        "$RESULTS_DIR"
        "$LOG_DIR"
        "$RESULTS_DIR/01_qc/fastqc_raw"
        "$RESULTS_DIR/01_qc/fastqc_clean"
        "$RESULTS_DIR/01_qc/fastp"
        "$RESULTS_DIR/01_qc/kraken2"
        "$RESULTS_DIR/01_qc/multiqc"
        "$RESULTS_DIR/02_assembly/spades"
        "$RESULTS_DIR/02_assembly/filtered"
        "$RESULTS_DIR/02_assembly/quast"
        "$RESULTS_DIR/03_annotation/prokka"
        "$RESULTS_DIR/03_annotation/stats"
        "$RESULTS_DIR/04_arg_detection/amrfinderplus"
        "$RESULTS_DIR/04_arg_detection/resfinder"
        "$RESULTS_DIR/04_arg_detection/plasmidfinder"
        "$RESULTS_DIR/04_arg_detection/card"
        "$RESULTS_DIR/04_arg_detection/ncbi"
        "$RESULTS_DIR/04_arg_detection/synthesis"
        "$RESULTS_DIR/05_variant_calling/snippy"
        "$RESULTS_DIR/05_variant_calling/stats"
        "$RESULTS_DIR/06_analysis/reports"
        "$RESULTS_DIR/06_analysis/figures"
        "$RESULTS_DIR/06_analysis/statistics"
        "$RESULTS_DIR/07_rag_ready/structured"
        "$RESULTS_DIR/07_rag_ready/chunks"
        "$RESULTS_DIR/07_rag_ready/metadata"
        "$RESULTS_DIR/08_rag_export"
    )

    echo ""
    echo "VÃ©rification de l'architecture des rÃ©pertoires..."
    echo ""

    for dir in "${required_dirs[@]}"; do
        if [[ ! -d "$dir" ]]; then
            missing_dirs=$((missing_dirs + 1))
            mkdir -p "$dir"
            created_dirs=$((created_dirs + 1))
            echo "  âœ… CrÃ©Ã©: $dir"
        fi
    done

    if [[ $created_dirs -eq 0 ]]; then
        echo "  âœ… Architecture complÃ¨te - Aucun rÃ©pertoire manquant"
    else
        echo ""
        echo "  ğŸ“ $created_dirs rÃ©pertoire(s) crÃ©Ã©(s)"
    fi

    echo ""
}

# ExÃ©cuter la vÃ©rification/crÃ©ation de l'architecture (sauf en mode update)
if [[ "$UPDATE_MODE" != true ]]; then
    setup_directory_structure

    # Maintenant que LOG_DIR existe, on peut crÃ©er les fichiers de log
    touch "$LOG_FILE" 2>/dev/null || true
    touch "$ERROR_LOG" 2>/dev/null || true
fi

#===============================================================================
# SECTION 6 : FONCTIONS UTILITAIRES
#===============================================================================

# Fonction de logging
log_message() {
    local level=$1
    shift
    local message="$@"
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[${timestamp}] [${level}] ${message}" | tee -a "$LOG_FILE"
}

log_info() {
    log_message "INFO" "$@"
}

log_warn() {
    log_message "WARN" "$@"
}

log_error() {
    log_message "ERROR" "$@"
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [ERROR] $@" >> "$ERROR_LOG"
}

log_success() {
    log_message "SUCCESS" "$@"
}

# Fonction utilitaire pour encoder les URLs (utilisÃ©e pour les requÃªtes NCBI)
urlencode() {
    local raw="$1"
    local encoded=""
    local i c
    for (( i=0; i<${#raw}; i++ )); do
        c=${raw:i:1}
        case "$c" in
            [a-zA-Z0-9.~_-]) encoded+="$c" ;;
            ' ') encoded+='+' ;;
            *) printf -v hex '%%%02X' "'$c"; encoded+="$hex" ;;
        esac
    done
    echo "$encoded"
}

# Fonction pour ouvrir les fichiers (GUI-safe)
open_file_safe() {
    local file_path=$1
    local description="${2:-Fichier}"
    
    if [[ ! -f "$file_path" ]]; then
        log_warn "Fichier introuvable: $file_path"
        return 1
    fi
    
    if command -v xdg-open > /dev/null 2>&1; then
        log_info "Ouverture de: $description"
        # xdg-open (disabled for web) "$file_path" 2>/dev/null || log_warn "Impossible d'ouvrir avec xdg-open"
    else
        log_info "Rapport disponible (xdg-open non disponible): $file_path"
    fi
}

# Fonction pour extraire l'espÃ¨ce depuis un rapport Kraken2
# Met Ã  jour les variables globales DETECTED_SPECIES, PROKKA_GENUS, PROKKA_SPECIES
extract_species_from_kraken2() {
    local kraken_report="$1"

    if [[ ! -f "$kraken_report" ]]; then
        log_warn "Rapport Kraken2 non trouvÃ©: $kraken_report"
        return 1
    fi

    log_info "Extraction de l'espÃ¨ce depuis le rapport Kraken2..."

    # Chercher la premiÃ¨re ligne avec un pourcentage significatif (>1%) pour une espÃ¨ce (S)
    local top_species_line=$(grep -E "^\s*[0-9]" "$kraken_report" 2>/dev/null | \
        awk -F'\t' '$4 == "S" && $1 > 1.0 {print; exit}' 2>/dev/null || true)

    if [[ -z "$top_species_line" ]]; then
        # Fallback: prendre la premiÃ¨re espÃ¨ce trouvÃ©e
        top_species_line=$(grep -E "^\s*[0-9]" "$kraken_report" 2>/dev/null | \
            awk -F'\t' '$4 == "S" {print; exit}' 2>/dev/null || true)
    fi

    if [[ -n "$top_species_line" ]]; then
        # Extraire le nom scientifique (derniÃ¨re colonne, peut contenir des espaces)
        DETECTED_SPECIES=$(echo "$top_species_line" | awk -F'\t' '{gsub(/^[ \t]+|[ \t]+$/, "", $6); print $6}' 2>/dev/null || echo "")

        if [[ -n "$DETECTED_SPECIES" ]]; then
            # Extraire genre et espÃ¨ce
            PROKKA_GENUS=$(echo "$DETECTED_SPECIES" | awk '{print $1}')
            PROKKA_SPECIES=$(echo "$DETECTED_SPECIES" | awk '{print $2}')

            # Nettoyer les valeurs
            PROKKA_GENUS="${PROKKA_GENUS:-Bacteria}"
            PROKKA_SPECIES="${PROKKA_SPECIES:-sp.}"

            log_success "EspÃ¨ce dÃ©tectÃ©e par Kraken2: $DETECTED_SPECIES"
            log_info "  â†’ Genre: $PROKKA_GENUS"
            log_info "  â†’ EspÃ¨ce: $PROKKA_SPECIES"
            return 0
        fi
    fi

    # Si aucune espÃ¨ce trouvÃ©e, essayer avec le genre (G)
    local top_genus_line=$(grep -E "^\s*[0-9]" "$kraken_report" 2>/dev/null | \
        awk -F'\t' '$4 == "G" && $1 > 1.0 {print; exit}' 2>/dev/null || true)

    if [[ -n "$top_genus_line" ]]; then
        PROKKA_GENUS=$(echo "$top_genus_line" | awk -F'\t' '{gsub(/^[ \t]+|[ \t]+$/, "", $6); print $6}' 2>/dev/null || echo "")
        PROKKA_SPECIES="sp."
        DETECTED_SPECIES="$PROKKA_GENUS sp."

        if [[ -n "$PROKKA_GENUS" ]]; then
            log_success "Genre dÃ©tectÃ© par Kraken2: $PROKKA_GENUS"
            return 0
        fi
    fi

    log_warn "Aucune espÃ¨ce/genre dÃ©tectÃ© dans le rapport Kraken2"
    DETECTED_SPECIES=""
    PROKKA_GENUS=""
    PROKKA_SPECIES=""
    return 1
}

#===============================================================================
# SECTION 6.4 : TÃ‰LÃ‰CHARGEMENT AUTOMATIQUE DES RÃ‰FÃ‰RENCES
#===============================================================================

# Fonction pour tÃ©lÃ©charger le gÃ©nome de rÃ©fÃ©rence d'une espÃ¨ce
# Utilise NCBI Assembly pour trouver un gÃ©nome de rÃ©fÃ©rence ou reprÃ©sentatif
# Met Ã  jour la variable globale REFERENCE_GENOME
#===============================================================================
# SECTION 6.4 : TÃ‰LÃ‰CHARGEMENT AUTOMATIQUE DES RÃ‰FÃ‰RENCES (CORRIGÃ‰E)
#===============================================================================

# Fonction pour tÃ©lÃ©charger le gÃ©nome de rÃ©fÃ©rence d'une espÃ¨ce
# Utilise NCBI Assembly pour trouver un gÃ©nome de rÃ©fÃ©rence ou reprÃ©sentatif
# Met Ã  jour la variable globale REFERENCE_GENOME
download_reference_genome() {
    local genus="$1"
    local species="$2"
    local output_dir="${3:-$REFERENCE_DIR}"

    # Normaliser les noms (minuscules, sans espaces multiples)
    genus=$(echo "$genus" | tr '[:upper:]' '[:lower:]' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
    species=$(echo "$species" | tr '[:upper:]' '[:lower:]' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')

    # Nom du fichier de rÃ©fÃ©rence
    local ref_filename="${genus}_${species}.fasta"
    local ref_path="$output_dir/$ref_filename"

    log_info "Recherche de rÃ©fÃ©rence pour: $genus $species"

    # VÃ©rifier si la rÃ©fÃ©rence existe dÃ©jÃ 
    if [[ -f "$ref_path" ]] && [[ -s "$ref_path" ]]; then
        log_success "RÃ©fÃ©rence existante trouvÃ©e: $ref_path"
        REFERENCE_GENOME="$ref_path"
        return 0
    fi

    # VÃ©rifier aussi avec d'autres extensions possibles
    for ext in fasta fna fa; do
        local alt_path="$output_dir/${genus}_${species}.$ext"
        if [[ -f "$alt_path" ]] && [[ -s "$alt_path" ]]; then
            log_success "RÃ©fÃ©rence existante trouvÃ©e: $alt_path"
            REFERENCE_GENOME="$alt_path"
            return 0
        fi
    done

    log_info "RÃ©fÃ©rence non trouvÃ©e localement, tÃ©lÃ©chargement depuis NCBI..."
    mkdir -p "$output_dir"

    # MÃ©thode 1: Recherche via NCBI Datasets API (si disponible)
    if command -v datasets > /dev/null 2>&1; then
        log_info "  Utilisation de NCBI datasets CLI..."

        local temp_dir=$(mktemp -d)
        if datasets download genome taxon "${genus} ${species}" \
            --reference \
            --include genome \
            --filename "$temp_dir/genome.zip" 2>> "$LOG_FILE"; then

            if [[ -f "$temp_dir/genome.zip" ]]; then
                unzip -q -o "$temp_dir/genome.zip" -d "$temp_dir" 2>> "$LOG_FILE"
                local fna_file=$(find "$temp_dir" -name "*.fna" -type f 2>/dev/null | head -1)

                if [[ -n "$fna_file" ]] && [[ -s "$fna_file" ]]; then
                    cp "$fna_file" "$ref_path"
                    rm -rf "$temp_dir"
                    log_success "RÃ©fÃ©rence tÃ©lÃ©chargÃ©e via datasets: $ref_path"
                    REFERENCE_GENOME="$ref_path"
                    return 0
                fi
            fi
        fi
        rm -rf "$temp_dir"
    fi

    # MÃ©thode 2: Recherche via NCBI E-utilities
    log_info "  Recherche via NCBI E-utilities..."

    local esearch_url="https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"

    # Termes de recherche encodÃ©s (utilise la fonction globale urlencode)
    local term_rep=$(urlencode "${genus} ${species}[Organism] AND representative genome[Filter]")
    local term_ref=$(urlencode "${genus} ${species}[Organism] AND reference genome[Filter]")
    local term_any=$(urlencode "${genus} ${species}[Organism] AND complete genome[Title]")

    # --- 1ï¸âƒ£ Recherche 'Representative genome' ---
    # Ajout de || true pour Ã©viter le crash du mode set -e si aucune correspondance n'est trouvÃ©e
    local search_result=$(wget -q --timeout=30 -O - "${esearch_url}?db=assembly&term=${term_rep}&retmax=1" 2>>"$LOG_FILE" || echo "")
    local assembly_id=$(echo "$search_result" | grep -oP '(?<=<Id>)[^<]+' | head -1 || true)

    # --- 2ï¸âƒ£ Recherche 'Reference genome' (si 1 Ã©choue) ---
    if [[ -z "$assembly_id" ]]; then
        search_result=$(wget -q --timeout=30 -O - "${esearch_url}?db=assembly&term=${term_ref}&retmax=1" 2>>"$LOG_FILE" || echo "")
        assembly_id=$(echo "$search_result" | grep -oP '(?<=<Id>)[^<]+' | head -1 || true)
    fi

    # --- 3ï¸âƒ£ Recherche 'Any complete genome' (si 1 et 2 Ã©chouent) ---
    if [[ -z "$assembly_id" ]]; then
        search_result=$(wget -q --timeout=30 -O - "${esearch_url}?db=assembly&term=${term_any}&retmax=1" 2>>"$LOG_FILE" || echo "")
        assembly_id=$(echo "$search_result" | grep -oP '(?<=<Id>)[^<]+' | head -1 || true)
    fi

    # --- Validation finale et tÃ©lÃ©chargement de l'ID trouvÃ© ---
    if [[ -n "$assembly_id" ]]; then
        log_info "  Assembly ID trouvÃ©: $assembly_id"

        local esummary_url="https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
        local summary=$(wget -q --timeout=30 -O - "${esummary_url}?db=assembly&id=${assembly_id}" 2>>"$LOG_FILE" || echo "")
        
        # SÃ©curisation de l'extraction de l'accession
        local accession=$(echo "$summary" | grep -oP 'GC[AF]_[0-9]+\.[0-9]+' | head -1 || true)

        if [[ -n "$accession" ]]; then
            log_info "  Accession trouvÃ©e: $accession"
            download_ncbi_assembly "$accession" "$output_dir"

            if [[ -n "$DOWNLOADED_FILE" && -f "$DOWNLOADED_FILE" ]]; then
                mv "$DOWNLOADED_FILE" "$ref_path" 2>/dev/null || cp "$DOWNLOADED_FILE" "$ref_path"
                log_success "RÃ©fÃ©rence tÃ©lÃ©chargÃ©e: $ref_path"
                REFERENCE_GENOME="$ref_path"
                return 0
            fi
        fi
    fi

    # MÃ©thode 3: Recherche directe dans nuccore pour un gÃ©nome complet (Dernier recours)
    log_info "  Recherche alternative dans nuccore..."
    local nuccore_search=$(urlencode "${genus} ${species}[Organism] AND complete genome[Title]")
    search_result=$(wget -q --timeout=30 -O - "${esearch_url}?db=nuccore&term=${nuccore_search}&retmax=1" 2>> "$LOG_FILE" || echo "")
    local nuccore_id=$(echo "$search_result" | grep -oP '(?<=<Id>)[^<]+' | head -1 || true)

    if [[ -n "$nuccore_id" ]]; then
        log_info "  Nuccore ID trouvÃ©: $nuccore_id"
        local efetch_url="https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nuccore&id=${nuccore_id}&rettype=fasta&retmode=text"
        wget -q --timeout=60 -O "$ref_path" "$efetch_url" 2>> "$LOG_FILE"

        if [[ -f "$ref_path" ]] && [[ -s "$ref_path" ]]; then
            if head -1 "$ref_path" | grep -q "^>"; then
                log_success "RÃ©fÃ©rence tÃ©lÃ©chargÃ©e depuis nuccore: $ref_path"
                REFERENCE_GENOME="$ref_path"
                return 0
            fi
        fi
    fi

    log_warn "Impossible de tÃ©lÃ©charger la rÃ©fÃ©rence pour $genus $species"
    log_warn "Le pipeline continuera sans rÃ©fÃ©rence spÃ©cifique (comparaison limitÃ©e)"
    REFERENCE_GENOME=""
    return 1
}

# Fonction pour obtenir ou tÃ©lÃ©charger la rÃ©fÃ©rence appropriÃ©e
# Retourne le chemin via REFERENCE_GENOME
get_or_download_reference() {
    local genus="${1:-}"
    local species="${2:-}"

    # Si genre/espÃ¨ce non fournis, vÃ©rifier les variables globales
    if [[ -z "$genus" ]]; then
        genus="$PROKKA_GENUS"
    fi
    if [[ -z "$species" ]]; then
        species="$PROKKA_SPECIES"
    fi

    # Si toujours pas d'espÃ¨ce dÃ©tectÃ©e
    if [[ -z "$genus" ]] || [[ "$genus" == "Bacteria" ]]; then
        log_warn "Aucune espÃ¨ce dÃ©tectÃ©e, impossible de tÃ©lÃ©charger une rÃ©fÃ©rence spÃ©cifique"

        # Fallback sur E. coli si disponible
        if [[ -f "$REFERENCE_DIR/ecoli_k12.fasta" ]]; then
            log_info "Utilisation de la rÃ©fÃ©rence par dÃ©faut: E. coli K-12"
            REFERENCE_GENOME="$REFERENCE_DIR/ecoli_k12.fasta"
            return 0
        fi

        REFERENCE_GENOME=""
        return 1
    fi

    # Cas spÃ©cial: E. coli (rÃ©fÃ©rence dÃ©jÃ  prÃ©sente)
    if [[ "$genus" == "Escherichia" ]] && [[ "$species" == "coli" ]]; then
        if [[ -f "$REFERENCE_DIR/ecoli_k12.fasta" ]]; then
            log_info "Utilisation de la rÃ©fÃ©rence E. coli K-12 existante"
            REFERENCE_GENOME="$REFERENCE_DIR/ecoli_k12.fasta"
            return 0
        fi
    fi

    # TÃ©lÃ©charger la rÃ©fÃ©rence pour l'espÃ¨ce dÃ©tectÃ©e
    download_reference_genome "$genus" "$species" "$REFERENCE_DIR"
    return $?
}

# Fonction pour crÃ©er/vÃ©rifier la base de donnÃ©es KMA
# Utilise les sÃ©quences d'abricate pour crÃ©er l'index KMA
setup_kma_database() {
    local kma_db_dir="$DB_DIR/kma_db"

    # VÃ©rifier si KMA est installÃ©
    if ! command -v kma > /dev/null 2>&1; then
        log_warn "KMA non installÃ©, base de donnÃ©es non crÃ©Ã©e"
        return 1
    fi

    # VÃ©rifier si la base existe dÃ©jÃ 
    if [[ -f "$kma_db_dir/resfinder.name" ]]; then
        log_info "Base KMA existante trouvÃ©e: $kma_db_dir/resfinder"
        return 0
    fi

    log_info "CrÃ©ation de la base de donnÃ©es KMA..."
    mkdir -p "$kma_db_dir"

    # RÃ©cupÃ©rer le chemin des bases abricate (plusieurs mÃ©thodes)
    local abricate_db=""

    # MÃ©thode 1: Extraire depuis --help (valeur par dÃ©faut entre crochets)
    abricate_db=$(abricate --help 2>&1 | grep -oP '\-\-datadir.*\[\K[^\]]+' | head -1)

    # MÃ©thode 2: Si Ã©chec, chercher relativement Ã  l'exÃ©cutable abricate
    if [[ -z "$abricate_db" ]] || [[ ! -d "$abricate_db" ]]; then
        local abricate_bin=$(which abricate 2>/dev/null)
        if [[ -n "$abricate_bin" ]]; then
            abricate_db="$(dirname "$abricate_bin")/../db"
            # Normaliser le chemin
            if [[ -d "$abricate_db" ]]; then
                abricate_db=$(cd "$abricate_db" && pwd)
            fi
        fi
    fi

    # MÃ©thode 3: Chemins connus (portables)
    if [[ -z "$abricate_db" ]] || [[ ! -d "$abricate_db" ]]; then
        for path in "$HOME/abricate/db" "/usr/local/share/abricate/db" "/opt/abricate/db" "${CONDA_PREFIX:-}/share/abricate/db"; do
            if [[ -d "$path" ]]; then
                abricate_db="$path"
                break
            fi
        done
    fi

    if [[ -z "$abricate_db" ]] || [[ ! -d "$abricate_db" ]]; then
        log_warn "Bases abricate non trouvÃ©es, impossible de crÃ©er la base KMA"
        log_warn "  VÃ©rifiez l'installation abricate avec: abricate --list"
        return 1
    fi

    log_info "  Bases abricate trouvÃ©es: $abricate_db"

    # CrÃ©er les index KMA pour chaque base
    for db_name in resfinder card ncbi; do
        local seq_file="$abricate_db/$db_name/sequences"

        if [[ -f "$seq_file" ]]; then
            log_info "  Indexation KMA: $db_name..."
            kma index -i "$seq_file" -o "$kma_db_dir/$db_name" 2>> "$LOG_FILE"

            if [[ -f "$kma_db_dir/${db_name}.name" ]]; then
                log_success "  Base KMA crÃ©Ã©e: $db_name"
            else
                log_warn "  Ã‰chec crÃ©ation base KMA: $db_name"
            fi
        else
            log_warn "  SÃ©quences non trouvÃ©es: $db_name"
        fi
    done

    return 0
}

#===============================================================================
# SECTION 6.5 : FONCTIONS DE TÃ‰LÃ‰CHARGEMENT MULTI-SOURCES
#===============================================================================

# Fonction pour tÃ©lÃ©charger une sÃ©quence GenBank (CP*, NC*, NZ_*)
# Retourne le chemin du fichier tÃ©lÃ©chargÃ© via la variable globale DOWNLOADED_FILE
download_genbank_sequence() {
    local accession="$1"
    local output_dir="$2"
    DOWNLOADED_FILE="$output_dir/${accession}.fasta"

    log_info "TÃ©lÃ©chargement de la sÃ©quence GenBank: $accession"

    # MÃ©thode 1: API eutils (mÃ©thode la plus fiable)
    log_info "  TÃ©lÃ©chargement via API NCBI eutils..."
    local eutils_url="https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=nuccore&id=${accession}&rettype=fasta&retmode=text"
    wget -q --timeout=60 -O "$DOWNLOADED_FILE" "$eutils_url" 2>> "$LOG_FILE"

    # VÃ©rifier si le tÃ©lÃ©chargement a rÃ©ussi
    if [[ -f "$DOWNLOADED_FILE" ]] && [[ -s "$DOWNLOADED_FILE" ]]; then
        # VÃ©rifier que c'est bien un fichier FASTA (commence par >)
        if head -1 "$DOWNLOADED_FILE" | grep -q "^>"; then
            log_success "SÃ©quence GenBank tÃ©lÃ©chargÃ©e: $DOWNLOADED_FILE"
            return 0
        fi
    fi

    # MÃ©thode 2: Fallback avec efetch CLI si disponible
    if command -v efetch > /dev/null 2>&1; then
        log_info "  Fallback: Utilisation de efetch (E-utilities CLI)..."
        efetch -db nuccore -id "$accession" -format fasta > "$DOWNLOADED_FILE" 2>> "$LOG_FILE"

        if [[ -f "$DOWNLOADED_FILE" ]] && [[ -s "$DOWNLOADED_FILE" ]]; then
            if head -1 "$DOWNLOADED_FILE" | grep -q "^>"; then
                log_success "SÃ©quence GenBank tÃ©lÃ©chargÃ©e via efetch CLI: $DOWNLOADED_FILE"
                return 0
            fi
        fi
    fi

    # MÃ©thode 3: Fallback avec curl si wget Ã©choue
    log_info "  Fallback: Utilisation de curl..."
    curl -s -o "$DOWNLOADED_FILE" "$eutils_url" 2>> "$LOG_FILE"

    if [[ -f "$DOWNLOADED_FILE" ]] && [[ -s "$DOWNLOADED_FILE" ]]; then
        if head -1 "$DOWNLOADED_FILE" | grep -q "^>"; then
            log_success "SÃ©quence GenBank tÃ©lÃ©chargÃ©e via curl: $DOWNLOADED_FILE"
            return 0
        fi
    fi

    log_error "Ã‰chec du tÃ©lÃ©chargement de $accession (toutes les mÃ©thodes ont Ã©chouÃ©)"
    DOWNLOADED_FILE=""
    return 1
}

# Fonction pour tÃ©lÃ©charger un assemblage NCBI (GCA_*, GCF_*)
# Retourne le chemin du fichier tÃ©lÃ©chargÃ© via la variable globale DOWNLOADED_FILE
download_ncbi_assembly() {
    local accession="$1"
    local output_dir="$2"
    DOWNLOADED_FILE="$output_dir/${accession}_genomic.fna"

    log_info "TÃ©lÃ©chargement de l'assemblage NCBI: $accession"

    # Construire l'URL de l'assemblage
    # Format: GCA_000005845.2 -> GCA/000/005/845/GCA_000005845.2
    local acc_prefix="${accession:0:3}"  # GCA ou GCF
    local acc_number="${accession:4}"     # 000005845.2
    acc_number="${acc_number%%.*}"        # 000005845 (sans version)

    # CrÃ©er le chemin FTP
    local part1="${acc_number:0:3}"
    local part2="${acc_number:3:3}"
    local part3="${acc_number:6:3}"

    local ftp_path="https://ftp.ncbi.nlm.nih.gov/genomes/all/${acc_prefix}/${part1}/${part2}/${part3}"

    log_info "  Recherche de l'assemblage sur NCBI FTP..."

    # Essayer de trouver le rÃ©pertoire exact
    local assembly_dir=$(wget -q --timeout=30 -O - "$ftp_path/" 2>/dev/null | grep -oP "href=\"${accession}[^\"]*\"" | head -1 | tr -d '"' | sed 's/href=//')

    # Nettoyer le nom du rÃ©pertoire (enlever le / Ã  la fin s'il existe)
    assembly_dir="${assembly_dir%/}"

    if [[ -z "$assembly_dir" ]]; then
        # Essayer sans version
        assembly_dir=$(wget -q --timeout=30 -O - "$ftp_path/" 2>/dev/null | grep -oP "href=\"${acc_prefix}_${acc_number}[^\"]*\"" | head -1 | tr -d '"' | sed 's/href=//')
        assembly_dir="${assembly_dir%/}"
    fi

    if [[ -n "$assembly_dir" ]]; then
        local full_url="${ftp_path}/${assembly_dir}/${assembly_dir}_genomic.fna.gz"
        log_info "  TÃ©lÃ©chargement depuis: $full_url"

        # Utiliser || true pour Ã©viter que set -e arrÃªte le script si wget Ã©choue (404, timeout, etc.)
        wget -q --timeout=120 -O "${DOWNLOADED_FILE}.gz" "$full_url" 2>> "$LOG_FILE" || {
            log_warn "  TÃ©lÃ©chargement wget Ã©chouÃ© (URL peut-Ãªtre invalide)"
            rm -f "${DOWNLOADED_FILE}.gz" 2>/dev/null
        }

        if [[ -f "${DOWNLOADED_FILE}.gz" ]] && [[ -s "${DOWNLOADED_FILE}.gz" ]]; then
            gunzip -f "${DOWNLOADED_FILE}.gz" 2>> "$LOG_FILE"
            if [[ -f "$DOWNLOADED_FILE" ]] && [[ -s "$DOWNLOADED_FILE" ]]; then
                log_success "Assemblage tÃ©lÃ©chargÃ©: $DOWNLOADED_FILE"
                return 0
            fi
        fi
    fi

    # Fallback: utiliser datasets CLI de NCBI si disponible
    if command -v datasets > /dev/null 2>&1; then
        log_info "  Fallback: Utilisation de NCBI datasets CLI..."
        datasets download genome accession "$accession" --filename "${output_dir}/${accession}.zip" 2>> "$LOG_FILE"

        if [[ -f "${output_dir}/${accession}.zip" ]]; then
            unzip -q -o "${output_dir}/${accession}.zip" -d "${output_dir}/temp_${accession}" 2>> "$LOG_FILE"
            find "${output_dir}/temp_${accession}" -name "*.fna" -exec cp {} "$DOWNLOADED_FILE" \;
            rm -rf "${output_dir}/temp_${accession}" "${output_dir}/${accession}.zip"

            if [[ -f "$DOWNLOADED_FILE" ]] && [[ -s "$DOWNLOADED_FILE" ]]; then
                log_success "Assemblage tÃ©lÃ©chargÃ© via datasets: $DOWNLOADED_FILE"
                return 0
            fi
        fi
    fi

    log_error "Ã‰chec du tÃ©lÃ©chargement de l'assemblage $accession"
    DOWNLOADED_FILE=""
    return 1
}

# Fonction pour copier un fichier FASTA local
# Retourne le chemin du fichier via la variable globale DOWNLOADED_FILE
setup_local_fasta() {
    local source_file="$1"
    local output_dir="$2"
    local sample_id="$3"
    DOWNLOADED_FILE="$output_dir/${sample_id}.fasta"

    log_info "Configuration du fichier FASTA local: $source_file"

    if [[ ! -f "$source_file" ]]; then
        log_error "Fichier FASTA introuvable: $source_file"
        DOWNLOADED_FILE=""
        return 1
    fi

    # Copier le fichier
    cp "$source_file" "$DOWNLOADED_FILE" 2>> "$LOG_FILE"

    if [[ -f "$DOWNLOADED_FILE" ]]; then
        log_success "Fichier FASTA configurÃ©: $DOWNLOADED_FILE"
        return 0
    else
        log_error "Ã‰chec de la copie du fichier FASTA"
        DOWNLOADED_FILE=""
        return 1
    fi
}

#===============================================================================
# SECTION 6.8 : GESTION DES BASES DE DONNÃ‰ES (KRAKEN2, AMRFINDER)
#===============================================================================

# Emplacements possibles pour les bases de donnÃ©es (ordre de prioritÃ©)
# 1. Variables d'environnement (pour utilisateurs avancÃ©s/serveurs)
# 2. Dans l'architecture du pipeline (portable)
# 3. Dans HOME partagÃ© (Ã©conomie d'espace multi-projets)

DB_SHARED_DIR="$HOME/.local/share/pipeline_arg_databases"

# Fonction pour trouver la base Kraken2
find_kraken2_db() {
    local found_path=""

    # 1. Variable d'environnement
    if [[ -n "${KRAKEN2_DB_PATH:-}" ]] && [[ -d "$KRAKEN2_DB_PATH" ]]; then
        if [[ -f "$KRAKEN2_DB_PATH/hash.k2d" ]]; then
            found_path="$KRAKEN2_DB_PATH"
        fi
    fi

    # 2. Dans l'architecture du pipeline
    if [[ -z "$found_path" ]] && [[ -d "$DB_DIR/kraken2_db" ]]; then
        # Chercher une DB valide (contient hash.k2d)
        local db_candidate=$(find "$DB_DIR/kraken2_db" -name "hash.k2d" -type f 2>/dev/null | head -1)
        if [[ -n "$db_candidate" ]]; then
            found_path=$(dirname "$db_candidate")
        fi
    fi

    # 3. Dans HOME partagÃ©
    if [[ -z "$found_path" ]] && [[ -d "$DB_SHARED_DIR/kraken2_db" ]]; then
        local db_candidate=$(find "$DB_SHARED_DIR/kraken2_db" -name "hash.k2d" -type f 2>/dev/null | head -1)
        if [[ -n "$db_candidate" ]]; then
            found_path=$(dirname "$db_candidate")
        fi
    fi

    echo "$found_path"
}

# Fonction pour trouver la base AMRFinder
find_amrfinder_db() {
    local found_path=""

    # 1. Variable d'environnement
    if [[ -n "${AMRFINDER_DB_PATH:-}" ]] && [[ -d "$AMRFINDER_DB_PATH" ]]; then
        if [[ -f "$AMRFINDER_DB_PATH/AMRProt" ]] || [[ -f "$AMRFINDER_DB_PATH/AMR.LIB" ]]; then
            found_path="$AMRFINDER_DB_PATH"
        fi
    fi

    # 2. Emplacement par dÃ©faut d'AMRFinder (gÃ©rÃ© par amrfinder --force_update)
    if [[ -z "$found_path" ]]; then
        local default_amr="$HOME/.local/share/amrfinder/latest"
        if [[ -d "$default_amr" ]]; then
            found_path="$default_amr"
        fi
    fi

    # 3. Dans l'architecture du pipeline
    if [[ -z "$found_path" ]] && [[ -d "$DB_DIR/amrfinder_db" ]]; then
        if [[ -f "$DB_DIR/amrfinder_db/AMRProt" ]] || [[ -f "$DB_DIR/amrfinder_db/AMR.LIB" ]]; then
            found_path="$DB_DIR/amrfinder_db"
        fi
    fi

    # 4. Dans HOME partagÃ©
    if [[ -z "$found_path" ]] && [[ -d "$DB_SHARED_DIR/amrfinder_db" ]]; then
        if [[ -f "$DB_SHARED_DIR/amrfinder_db/AMRProt" ]] || [[ -f "$DB_SHARED_DIR/amrfinder_db/AMR.LIB" ]]; then
            found_path="$DB_SHARED_DIR/amrfinder_db"
        fi
    fi

    echo "$found_path"
}

# Fonction pour tÃ©lÃ©charger Kraken2 DB
download_kraken2_db() {
    local target_dir="$1"
    local db_type="${2:-standard}"  # standard, minikraken, viral, etc.

    mkdir -p "$target_dir"

    echo ""
    echo "TÃ©lÃ©chargement de la base Kraken2 ($db_type)..."
    echo "Cela peut prendre un certain temps selon votre connexion."
    echo ""

    case "$db_type" in
        standard)
            # Standard DB (~50-70 GB) - ComplÃ¨te
            echo "âš ï¸  La base standard fait ~50-70 GB. TÃ©lÃ©chargement en cours..."
            local db_url="https://genome-idx.s3.amazonaws.com/kraken/k2_standard_20231009.tar.gz"
            wget -c -O "$target_dir/kraken2_db.tar.gz" "$db_url" 2>&1
            ;;
        minikraken)
            # MiniKraken (~8 GB) - Plus lÃ©gÃ¨re, moins prÃ©cise
            echo "TÃ©lÃ©chargement de MiniKraken2 (~8 GB)..."
            local db_url="https://genome-idx.s3.amazonaws.com/kraken/k2_minusb_20231009.tar.gz"
            wget -c -O "$target_dir/kraken2_db.tar.gz" "$db_url" 2>&1
            ;;
        viral)
            # Viral DB (~500 MB) - Virus uniquement
            echo "TÃ©lÃ©chargement de la base virale (~500 MB)..."
            local db_url="https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20231009.tar.gz"
            wget -c -O "$target_dir/kraken2_db.tar.gz" "$db_url" 2>&1
            ;;
        *)
            echo "Type de base inconnu: $db_type"
            return 1
            ;;
    esac

    if [[ -f "$target_dir/kraken2_db.tar.gz" ]]; then
        echo ""
        echo "Extraction de la base de donnÃ©es..."
        tar -xzf "$target_dir/kraken2_db.tar.gz" -C "$target_dir"
        rm -f "$target_dir/kraken2_db.tar.gz"
        echo "âœ… Base Kraken2 installÃ©e dans: $target_dir"
        return 0
    else
        echo "âŒ Ã‰chec du tÃ©lÃ©chargement"
        return 1
    fi
}

# Fonction pour tÃ©lÃ©charger/mettre Ã  jour AMRFinder DB
download_amrfinder_db() {
    local target_dir="$1"
    local download_success=false

    echo ""
    echo "TÃ©lÃ©chargement/Mise Ã  jour de la base AMRFinder..."
    echo ""

    # Activer l'environnement conda contenant amrfinder
    echo "Activation de l'environnement conda arg_detection..."

    # VÃ©rifier si l'environnement existe, sinon le crÃ©er
    if ! conda env list | grep -q "^arg_detection "; then
        echo "CrÃ©ation de l'environnement arg_detection..."
        conda create -n arg_detection -c bioconda -c conda-forge ncbi-amrfinderplus -y 2>&1
    fi

    conda activate arg_detection 2>/dev/null || {
        echo "âŒ Impossible d'activer l'environnement arg_detection"
        return 1
    }

    # AMRFinder gÃ¨re son propre tÃ©lÃ©chargement via --force_update
    if [[ -n "$target_dir" ]]; then
        mkdir -p "$target_dir"
        # Utiliser le rÃ©pertoire spÃ©cifiÃ© avec l'option --database
        echo "TÃ©lÃ©chargement dans: $target_dir"
        if amrfinder_update --force_update --database "$target_dir" 2>&1; then
            download_success=true
        elif amrfinder --force_update --database "$target_dir" 2>&1; then
            download_success=true
        fi
    fi

    # Si Ã©chec avec rÃ©pertoire personnalisÃ©, essayer l'emplacement par dÃ©faut
    if [[ "$download_success" == false ]]; then
        echo "Utilisation de l'emplacement par dÃ©faut AMRFinder..."
        if amrfinder --force_update 2>&1; then
            download_success=true
            # Copier vers le rÃ©pertoire cible si spÃ©cifiÃ©
            if [[ -n "$target_dir" ]] && [[ -d "$HOME/.local/share/amrfinder/latest" ]]; then
                echo "Copie des fichiers vers $target_dir..."
                cp -r "$HOME/.local/share/amrfinder/latest/"* "$target_dir/" 2>/dev/null || true
            fi
        fi
    fi

    # DÃ©sactiver l'environnement
    conda deactivate 2>/dev/null || true

    if [[ "$download_success" == true ]]; then
        echo "âœ… Base AMRFinder installÃ©e"
        return 0
    else
        echo "âŒ Ã‰chec de la mise Ã  jour AMRFinder"
        return 1
    fi
}

# TÃ©lÃ©chargement de la base CARD pour RGI
download_card_db() {
    local target_dir="$1"
    local download_success=false

    echo ""
    echo "TÃ©lÃ©chargement de la base CARD pour RGI..."
    echo ""

    mkdir -p "$target_dir"
    cd "$target_dir" || return 1

    # MÃ‰THODE 1: TÃ©lÃ©chargement direct depuis card.mcmaster.ca
    # URLs des fichiers CARD
    local CARD_URL="https://card.mcmaster.ca/latest/data"
    local CARD_VARIANTS_URL="https://card.mcmaster.ca/latest/variants"

    echo "  [MÃ©thode 1] TÃ©lÃ©chargement direct depuis card.mcmaster.ca..."
    if wget -q --show-progress -O card.tar.bz2 "$CARD_URL" 2>&1; then
        tar -xjf card.tar.bz2 2>/dev/null
        rm -f card.tar.bz2
        download_success=true
        echo "  âœ… TÃ©lÃ©chargement rÃ©ussi"
    else
        echo "  âŒ Ã‰chec du tÃ©lÃ©chargement direct"
    fi

    # MÃ‰THODE 2: Alternative via RGI
    if [[ "$download_success" == false ]]; then
        echo ""
        echo "  [MÃ©thode 2] Tentative via RGI auto_load..."

        if conda activate arg_detection || conda activate megam_arg 2>/dev/null; then
            if command -v rgi &> /dev/null; then
                # Utiliser rgi auto_load qui tÃ©lÃ©charge automatiquement les donnÃ©es
                if rgi auto_load 2>&1; then
                    # Copier les fichiers depuis le rÃ©pertoire RGI vers target_dir
                    local rgi_data_dir=$(python -c "import pkg_resources; print(pkg_resources.resource_filename('app', 'data'))" 2>/dev/null)
                    if [[ -d "$rgi_data_dir" ]] && [[ -f "$rgi_data_dir/card.json" ]]; then
                        cp -r "$rgi_data_dir"/* "$target_dir/"
                        download_success=true
                        echo "  âœ… Base tÃ©lÃ©chargÃ©e via RGI"
                    fi
                fi
            fi
            conda deactivate 2>/dev/null || true
        fi
    fi

    # MÃ‰THODE 3: Alternative via abricate
    if [[ "$download_success" == false ]]; then
        echo ""
        echo "  [MÃ©thode 3] Tentative via abricate..."

        # Activer l'environnement contenant abricate
        if conda activate arg_detection 2>/dev/null || conda activate megam_arg 2>/dev/null; then
            if command -v abricate &> /dev/null; then
                echo "  abricate trouvÃ©, vÃ©rification de la base CARD..."

                # VÃ©rifier si CARD est disponible dans abricate
                if abricate --list 2>/dev/null | grep -q "card"; then
                    echo "  Base CARD trouvÃ©e dans abricate"

                    # Chercher le rÃ©pertoire de la base CARD
                    local abricate_card_dir=""
                    for path in "${CONDA_PREFIX:-}/db/card" "$HOME/abricate/db/card" "/usr/local/share/abricate/db/card" "/opt/abricate/db/card"; do
                        if [[ -d "$path" ]] && [[ -f "$path/sequences" ]]; then
                            abricate_card_dir="$path"
                            echo "  TrouvÃ©e dans: $abricate_card_dir"
                            break
                        fi
                    done

                    if [[ -n "$abricate_card_dir" ]] && [[ -f "$abricate_card_dir/sequences" ]]; then
                        # Copier les sÃ©quences CARD d'abricate
                        echo "  Copie des sÃ©quences CARD d'abricate..."
                        cp "$abricate_card_dir/sequences" "$target_dir/card_sequences.fasta"

                        # Copier aussi les index BLAST si disponibles
                        if ls "$abricate_card_dir"/sequences.n* &> /dev/null; then
                            cp "$abricate_card_dir"/sequences.n* "$target_dir/" 2>/dev/null || true
                        fi

                        echo ""
                        echo "  âœ… Base CARD d'abricate installÃ©e"
                        echo "  âš ï¸  Note: Utilisation des sÃ©quences CARD d'abricate (solution de secours)"
                        echo "  â„¹ï¸  Pour les fonctionnalitÃ©s complÃ¨tes de RGI, le fichier card.json est nÃ©cessaire"
                        echo "  â„¹ï¸  Le pipeline continuera avec les analyses disponibles"
                        download_success=true
                    else
                        echo "  âŒ Impossible de localiser le rÃ©pertoire CARD d'abricate"
                    fi
                else
                    echo "  Base CARD non trouvÃ©e dans abricate, tentative de mise Ã  jour..."
                    if abricate --setupdb 2>&1 | grep -i "card"; then
                        echo "  Base CARD mise Ã  jour, nouvelle tentative..."
                        # RÃ©essayer aprÃ¨s mise Ã  jour
                        for path in "${CONDA_PREFIX:-}/db/card" "$HOME/abricate/db/card"; do
                            if [[ -d "$path" ]] && [[ -f "$path/sequences" ]]; then
                                cp "$path/sequences" "$target_dir/card_sequences.fasta"
                                download_success=true
                                echo "  âœ… Base CARD d'abricate installÃ©e"
                                break
                            fi
                        done
                    fi
                fi
            else
                echo "  abricate non trouvÃ© dans cet environnement"
            fi
            conda deactivate 2>/dev/null || true
        else
            echo "  Impossible d'activer l'environnement conda pour abricate"
        fi
    fi

    # Si tÃ©lÃ©chargement rÃ©ussi avec la mÃ©thode 1, tÃ©lÃ©charger aussi les variants
    if [[ "$download_success" == true ]] && [[ -f "$target_dir/card.json" ]]; then
        echo ""
        echo "  TÃ©lÃ©chargement des variants CARD..."
        if wget -q --show-progress -O variants.tar.bz2 "$CARD_VARIANTS_URL" 2>&1; then
            tar -xjf variants.tar.bz2 2>/dev/null
            rm -f variants.tar.bz2
            echo "  âœ… Variants tÃ©lÃ©chargÃ©s"
        else
            echo "  âš ï¸  Variants non tÃ©lÃ©chargÃ©s (optionnel)"
        fi
    fi

    # Charger la base avec RGI si card.json existe
    if [[ -f "$target_dir/card.json" ]]; then
        echo ""
        echo "  Chargement de la base dans RGI..."

        # Activer l'environnement contenant RGI
        if conda activate arg_detection || conda activate megam_arg 2>/dev/null; then
            # Charger card.json
            rgi load --card_json "$target_dir/card.json" \
                     --local \
                     --data_path "$target_dir" 2>&1 || true

            # Charger les variants si disponibles
            if [[ -d "$target_dir/wildcard" ]] || [[ -f "$target_dir/wildcard_database_v"*.fasta ]]; then
                local wildcard_fasta=$(ls "$target_dir"/wildcard_database_v*.fasta 2>/dev/null | head -1)
                local wildcard_index=$(ls "$target_dir"/wildcard/index-for-model-sequences.txt 2>/dev/null | head -1)

                if [[ -n "$wildcard_fasta" ]]; then
                    rgi load --wildcard_annotation "$wildcard_fasta" \
                             --wildcard_index "$wildcard_index" \
                             --card_annotation "$target_dir/card_database_v"*.fasta \
                             --local \
                             --data_path "$target_dir" 2>&1 || true
                fi
            fi

            # CrÃ©er l'index DIAMOND
            echo "  CrÃ©ation de l'index DIAMOND..."
            if command -v diamond &> /dev/null; then
                rgi card_annotation --local --data_path "$target_dir" 2>&1 || true
                rgi load --card_annotation "$target_dir"/card_database_v*.fasta \
                         --local \
                         --data_path "$target_dir" 2>&1 || true
            fi

            conda deactivate 2>/dev/null || true
        fi
    fi

    cd - > /dev/null

    if [[ "$download_success" == true ]]; then
        if [[ -f "$target_dir/card.json" ]] || [[ -f "$target_dir/card_sequences.fasta" ]]; then
            echo ""
            echo "âœ… Base CARD installÃ©e dans $target_dir"
            return 0
        fi
    fi

    echo ""
    echo "âŒ ERREUR CRITIQUE: Ã‰chec de l'installation CARD avec toutes les mÃ©thodes"
    echo "   La base CARD est essentielle pour ce pipeline."
    echo "   Veuillez vÃ©rifier votre connexion internet et rÃ©essayer."
    return 1
}

# TÃ©lÃ©chargement de la base PointFinder
download_pointfinder_db() {
    local target_dir="$1"

    echo ""
    echo "TÃ©lÃ©chargement de la base PointFinder..."
    echo ""

    mkdir -p "$target_dir"

    # Cloner le repository PointFinder
    if [[ -d "$target_dir/pointfinder_db" ]] && [[ -f "$target_dir/pointfinder_db/config" ]]; then
        echo "  Base PointFinder dÃ©jÃ  prÃ©sente, mise Ã  jour..."
        cd "$target_dir/pointfinder_db" && git pull 2>&1 || true
        cd - > /dev/null
    else
        echo "  Clonage du repository PointFinder..."
        rm -rf "$target_dir/pointfinder_db" 2>/dev/null
        if git clone https://bitbucket.org/genomicepidemiology/pointfinder_db.git "$target_dir/pointfinder_db" 2>&1; then
            echo "âœ… Base PointFinder installÃ©e"
            return 0
        else
            echo "âŒ Ã‰chec du clonage PointFinder"
            return 1
        fi
    fi

    return 0
}

# Fonction pour trouver la base CARD
find_card_db() {
    local found_path=""

    # Chercher dans DB_DIR
    if [[ -d "$DB_DIR/card_db" ]] && [[ -f "$DB_DIR/card_db/card.json" ]]; then
        found_path="$DB_DIR/card_db"
    # Chercher dans localDB (ancien emplacement)
    elif [[ -d "$WORK_DIR/localDB" ]] && [[ -f "$WORK_DIR/localDB/card.json" ]]; then
        found_path="$WORK_DIR/localDB"
    # Chercher dans home
    elif [[ -d "$HOME/.local/share/rgi" ]] && [[ -f "$HOME/.local/share/rgi/card.json" ]]; then
        found_path="$HOME/.local/share/rgi"
    fi

    echo "$found_path"
}

# Fonction pour trouver la base PointFinder
find_pointfinder_db() {
    local found_path=""

    # Chercher dans DB_DIR
    if [[ -d "$DB_DIR/pointfinder_db" ]] && [[ -f "$DB_DIR/pointfinder_db/config" ]]; then
        found_path="$DB_DIR/pointfinder_db"
    # Chercher dans home
    elif [[ -d "$HOME/databases/pointfinder_db" ]] && [[ -f "$HOME/databases/pointfinder_db/config" ]]; then
        found_path="$HOME/databases/pointfinder_db"
    fi

    echo "$found_path"
}

# Fonction pour trouver la base MLST
find_mlst_db() {
    local found_path=""

    # Chercher dans DB_DIR
    if [[ -d "$DB_DIR/mlst_db/db" ]] && [[ -d "$DB_DIR/mlst_db/db/pubmlst" ]]; then
        found_path="$DB_DIR/mlst_db"
    # Chercher dans conda env
    elif [[ -d "${CONDA_PREFIX:-}/share/mlst/db" ]]; then
        found_path="${CONDA_PREFIX:-}/share/mlst"
    fi

    echo "$found_path"
}

# TÃ©lÃ©chargement de la base MLST
download_mlst_db() {
    local target_dir="$1"

    echo ""
    echo "TÃ©lÃ©chargement de la base MLST..."
    echo ""

    mkdir -p "$target_dir"

    # Activer l'environnement assembly_arg qui contient mlst
    if conda activate assembly_arg 2>/dev/null; then
        # Copier la base depuis l'environnement conda
        if [[ -d "${CONDA_PREFIX:-}/share/mlst" ]]; then
            cp -r "${CONDA_PREFIX:-}/share/mlst/"* "$target_dir/"
            echo "âœ… Base MLST copiÃ©e depuis conda"
        else
            # TÃ©lÃ©charger via mlst-download_pub_mlst
            echo "  TÃ©lÃ©chargement des schÃ©mas MLST..."
            mkdir -p "$target_dir/db/pubmlst" "$target_dir/db/blast"
            # mlst tÃ©lÃ©charge automatiquement les schÃ©mas au premier usage
            echo "âš ï¸  La base MLST sera tÃ©lÃ©chargÃ©e automatiquement au premier usage"
        fi
        conda deactivate 2>/dev/null || true
    else
        echo "âŒ Environnement assembly_arg non disponible"
        return 1
    fi

    return 0
}

# VÃ©rification des bases de donnÃ©es abricate
find_abricate_dbs() {
    local abricate_found=false
    local abricate_env=""

    # Essayer de trouver abricate dans les environnements conda
    for env in arg_detection megam_arg annotation_arg; do
        if conda activate $env 2>/dev/null; then
            if command -v abricate &> /dev/null; then
                abricate_found=true
                abricate_env=$env
                break
            fi
            conda deactivate 2>/dev/null || true
        fi
    done

    # Si abricate n'est pas trouvÃ© dans conda, vÃ©rifier dans l'environnement actuel
    if [[ "$abricate_found" == false ]]; then
        if command -v abricate &> /dev/null; then
            abricate_found=true
        else
            echo ""
            return
        fi
    fi

    # VÃ©rifier si les bases abricate sont installÃ©es
    local abricate_list=$(abricate --list 2>/dev/null)

    # DÃ©sactiver l'environnement si on l'a activÃ©
    if [[ -n "$abricate_env" ]]; then
        conda deactivate 2>/dev/null || true
    fi

    if [[ -z "$abricate_list" ]]; then
        echo ""
        return
    fi

    # VÃ©rifier que les bases essentielles sont prÃ©sentes
    local has_resfinder=$(echo "$abricate_list" | grep -w "resfinder" | wc -l)
    local has_card=$(echo "$abricate_list" | grep -w "card" | wc -l)
    local has_ncbi=$(echo "$abricate_list" | grep -w "ncbi" | wc -l)
    local has_plasmidfinder=$(echo "$abricate_list" | grep -w "plasmidfinder" | wc -l)

    # Si toutes les bases essentielles sont prÃ©sentes
    if [[ $has_resfinder -gt 0 ]] && [[ $has_card -gt 0 ]] && [[ $has_ncbi -gt 0 ]] && [[ $has_plasmidfinder -gt 0 ]]; then
        echo "found"
    else
        echo ""
    fi
}

# Installation/mise Ã  jour des bases de donnÃ©es abricate
setup_abricate_dbs() {
    echo ""
    echo "Installation des bases de donnÃ©es abricate..."
    echo ""

    # Activer l'environnement contenant abricate
    local abricate_env=""
    local abricate_found=false

    echo "  Recherche d'abricate dans les environnements conda..."
    for env in arg_detection megam_arg annotation_arg; do
        if conda activate $env 2>/dev/null; then
            if command -v abricate &> /dev/null; then
                abricate_env=$env
                abricate_found=true
                echo "  âœ… Environnement abricate trouvÃ©: $env"
                break
            fi
            conda deactivate 2>/dev/null || true
        fi
    done

    # Si abricate n'est toujours pas trouvÃ©, vÃ©rifier dans l'environnement actuel
    if [[ "$abricate_found" == false ]]; then
        if command -v abricate &> /dev/null; then
            echo "  âœ… abricate trouvÃ© dans l'environnement actuel"
            abricate_found=true
        fi
    fi

    # Si abricate n'est pas trouvÃ© du tout
    if [[ "$abricate_found" == false ]]; then
        echo ""
        echo "âŒ abricate n'est pas installÃ© ou accessible"
        echo "   abricate n'a pas Ã©tÃ© trouvÃ© dans les environnements conda suivants:"
        echo "   - arg_detection"
        echo "   - megam_arg"
        echo "   - annotation_arg"
        echo ""
        echo "   Solutions possibles:"
        echo "   1) Installer abricate dans un environnement existant:"
        echo "      conda activate arg_detection"
        echo "      conda install -c bioconda abricate"
        echo ""
        echo "   2) CrÃ©er un nouvel environnement avec abricate:"
        echo "      conda create -n abricate_env -c bioconda abricate"
        echo ""
        return 1
    fi

    echo "  TÃ©lÃ©chargement et indexation des bases abricate..."
    echo "  Cela peut prendre quelques minutes..."
    echo ""

    # ExÃ©cuter abricate --setupdb
    if abricate --setupdb 2>&1 | tee /tmp/abricate_setup.log; then
        echo ""

        # VÃ©rifier que les bases sont bien installÃ©es
        local installed_dbs=$(abricate --list 2>/dev/null | tail -n +2 | awk '{print $1}')

        if [[ -n "$installed_dbs" ]]; then
            echo "âœ… Bases abricate installÃ©es:"
            abricate --list 2>/dev/null | grep -E "resfinder|card|ncbi|plasmidfinder|vfdb|argannot|megares" | while read line; do
                local db_name=$(echo "$line" | awk '{print $1}')
                local db_seqs=$(echo "$line" | awk '{print $2}')
                echo "   - $db_name ($db_seqs sÃ©quences)"
            done
            echo ""

            # DÃ©sactiver l'environnement si activÃ©
            if [[ -n "$abricate_env" ]]; then
                conda deactivate 2>/dev/null || true
            fi

            return 0
        else
            echo "âš ï¸  Les bases semblent installÃ©es mais ne sont pas listÃ©es"
            if [[ -n "$abricate_env" ]]; then
                conda deactivate 2>/dev/null || true
            fi
            return 1
        fi
    else
        echo ""
        echo "âŒ Ã‰chec de l'installation des bases abricate"
        echo "   Consultez /tmp/abricate_setup.log pour plus de dÃ©tails"

        if [[ -n "$abricate_env" ]]; then
            conda deactivate 2>/dev/null || true
        fi

        return 1
    fi
}

#===============================================================================
# FONCTIONS DE MISE Ã€ JOUR DES BASES DE DONNÃ‰ES
#===============================================================================

# Mise Ã  jour de la base Kraken2
update_kraken_db() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "MISE Ã€ JOUR DE LA BASE KRAKEN2"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    local kraken_path=$(find_kraken2_db)
    if [[ -z "$kraken_path" ]]; then
        kraken_path="$DB_DIR/kraken2_db"
    fi

    echo "Chemin: $kraken_path"
    echo ""
    echo "âš ï¸  Note: Kraken2 nÃ©cessite un re-tÃ©lÃ©chargement complet (~8 Go)"
    read -p "Continuer? (o/n): " confirm
    if [[ "$confirm" =~ ^[oOyY]$ ]]; then
        rm -rf "$kraken_path"/*
        download_kraken2_db "$kraken_path" "minikraken"
        echo "âœ… Base Kraken2 mise Ã  jour"
    else
        echo "Mise Ã  jour annulÃ©e"
    fi
}

# Mise Ã  jour de la base AMRFinder
update_amrfinder_db() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "MISE Ã€ JOUR DE LA BASE AMRFINDER"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    local amr_path=$(find_amrfinder_db)
    if [[ -z "$amr_path" ]]; then
        amr_path="$DB_DIR/amrfinder_db"
        mkdir -p "$amr_path"
    fi

    echo "Chemin: $amr_path"

    # Activer l'environnement
    if conda activate arg_detection 2>/dev/null || conda activate annotation_arg 2>/dev/null; then
        echo "TÃ©lÃ©chargement de la derniÃ¨re version..."
        if amrfinder_update --force_update --database "$amr_path" 2>&1; then
            echo "âœ… Base AMRFinder mise Ã  jour"
        elif amrfinder --force_update --database "$amr_path" 2>&1; then
            echo "âœ… Base AMRFinder mise Ã  jour"
        else
            echo "âŒ Ã‰chec de la mise Ã  jour AMRFinder"
        fi
        conda deactivate 2>/dev/null || true
    else
        echo "âŒ Environnement conda non disponible"
    fi
}

# Mise Ã  jour de la base CARD (RGI)
update_card_db() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "MISE Ã€ JOUR DE LA BASE CARD (RGI)"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    local card_path=$(find_card_db)
    if [[ -z "$card_path" ]]; then
        card_path="$DB_DIR/card_db"
    fi

    echo "Chemin: $card_path"
    echo "TÃ©lÃ©chargement de la derniÃ¨re version depuis card.mcmaster.ca..."

    # Sauvegarder l'ancienne version
    if [[ -d "$card_path" ]] && [[ -f "$card_path/card.json" ]]; then
        local backup_dir="${card_path}_backup_$(date +%Y%m%d)"
        echo "Sauvegarde de l'ancienne version dans: $backup_dir"
        mv "$card_path" "$backup_dir"
    fi

    mkdir -p "$card_path"
    download_card_db "$card_path"

    if [[ -f "$card_path/card.json" ]]; then
        echo "âœ… Base CARD mise Ã  jour"
        # Supprimer la sauvegarde si succÃ¨s
        rm -rf "${card_path}_backup_"* 2>/dev/null || true
    else
        echo "âŒ Ã‰chec - restauration de l'ancienne version"
        rm -rf "$card_path"
        mv "${card_path}_backup_"* "$card_path" 2>/dev/null || true
    fi
}

# Mise Ã  jour de la base MLST
update_mlst_db() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "MISE Ã€ JOUR DE LA BASE MLST"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    local mlst_path=$(find_mlst_db)
    if [[ -z "$mlst_path" ]]; then
        mlst_path="$DB_DIR/mlst_db"
    fi

    echo "Chemin: $mlst_path"

    # Activer l'environnement
    if conda activate assembly_arg 2>/dev/null; then
        # Copier la base mise Ã  jour depuis conda
        if [[ -d "${CONDA_PREFIX:-}/share/mlst" ]]; then
            echo "Copie depuis l'environnement conda..."
            rm -rf "$mlst_path"/*
            cp -r "${CONDA_PREFIX:-}/share/mlst/"* "$mlst_path/"
            echo "âœ… Base MLST mise Ã  jour"
        else
            echo "âŒ Base MLST non trouvÃ©e dans conda"
        fi
        conda deactivate 2>/dev/null || true
    else
        echo "âŒ Environnement assembly_arg non disponible"
    fi
}

# Mise Ã  jour de la base PointFinder
update_pointfinder_db() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "MISE Ã€ JOUR DE LA BASE POINTFINDER"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    local pf_path=$(find_pointfinder_db)
    if [[ -z "$pf_path" ]]; then
        pf_path="$DB_DIR/pointfinder_db"
    fi

    echo "Chemin: $pf_path"

    if [[ -d "$pf_path/.git" ]]; then
        echo "Mise Ã  jour via git pull..."
        cd "$pf_path"
        git pull origin master 2>&1
        cd - > /dev/null
        echo "âœ… Base PointFinder mise Ã  jour"
    else
        echo "Re-clonage du repository..."
        rm -rf "$pf_path"
        git clone https://bitbucket.org/genomicepidemiology/pointfinder_db.git "$pf_path" 2>&1
        echo "âœ… Base PointFinder mise Ã  jour"
    fi
}

# Mise Ã  jour de la base KMA/ResFinder
update_kma_db() {
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "MISE Ã€ JOUR DE LA BASE KMA/RESFINDER"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    local kma_path="$DB_DIR/kma_db"
    mkdir -p "$kma_path"

    echo "Chemin: $kma_path"
    echo "TÃ©lÃ©chargement depuis CGE..."

    # TÃ©lÃ©charger ResFinder database
    local resfinder_url="https://bitbucket.org/genomicepidemiology/resfinder_db/get/master.zip"

    cd "$kma_path"
    if wget -q --show-progress -O resfinder_db.zip "$resfinder_url" 2>&1; then
        unzip -o resfinder_db.zip 2>/dev/null
        rm -f resfinder_db.zip

        # Indexer avec KMA si disponible
        if command -v kma_index &> /dev/null; then
            echo "Indexation avec KMA..."
            local db_dir=$(find . -name "*.fsa" -type f -exec dirname {} \; | head -1)
            if [[ -n "$db_dir" ]]; then
                cat "$db_dir"/*.fsa > resfinder_all.fsa
                kma_index -i resfinder_all.fsa -o resfinder 2>&1
                rm -f resfinder_all.fsa
            fi
        fi
        echo "âœ… Base KMA/ResFinder mise Ã  jour"
    else
        echo "âŒ Ã‰chec du tÃ©lÃ©chargement"
    fi
    cd - > /dev/null
}

# Mise Ã  jour de toutes les bases
update_all_databases() {
    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘         MISE Ã€ JOUR DE TOUTES LES BASES DE DONNÃ‰ES            â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "Bases Ã  mettre Ã  jour:"
    echo "  1. AMRFinder"
    echo "  2. CARD (RGI)"
    echo "  3. MLST"
    echo "  4. PointFinder"
    echo "  5. KMA/ResFinder"
    echo "  6. Kraken2 (optionnel - trÃ¨s volumineux)"
    echo ""
    read -p "Continuer avec la mise Ã  jour? (o/n): " confirm

    if [[ ! "$confirm" =~ ^[oOyY]$ ]]; then
        echo "Mise Ã  jour annulÃ©e"
        exit 0
    fi

    # Mettre Ã  jour chaque base
    update_amrfinder_db
    update_card_db
    update_mlst_db
    update_pointfinder_db
    update_kma_db

    echo ""
    read -p "Mettre Ã  jour aussi Kraken2 (~8 Go)? (o/n): " kraken_confirm
    if [[ "$kraken_confirm" =~ ^[oOyY]$ ]]; then
        update_kraken_db
    fi

    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "âœ… MISE Ã€ JOUR TERMINÃ‰E"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    exit 0
}

# Traitement de la commande "update" (aprÃ¨s dÃ©finition des fonctions)
if [[ "$UPDATE_MODE" == true ]]; then
    UPDATE_TARGET="${INPUT_ARG2:-all}"

    case "$UPDATE_TARGET" in
        kraken|kraken2)
            update_kraken_db
            exit 0
            ;;
        amrfinder|amr)
            update_amrfinder_db
            exit 0
            ;;
        card|rgi)
            update_card_db
            exit 0
            ;;
        mlst)
            update_mlst_db
            exit 0
            ;;
        pointfinder|point)
            update_pointfinder_db
            exit 0
            ;;
        kma|resfinder)
            update_kma_db
            exit 0
            ;;
        all|"")
            update_all_databases
            exit 0
            ;;
        *)
            echo "âŒ Base inconnue: $UPDATE_TARGET"
            echo ""
            echo "Bases disponibles:"
            echo "  kraken, amrfinder, card, mlst, pointfinder, kma"
            echo ""
            echo "Exemple: $0 update card"
            exit 1
            ;;
    esac
fi

# Menu interactif pour la gestion des bases de donnÃ©es
interactive_database_setup() {
    local kraken_found=$(find_kraken2_db)
    local amrfinder_found=$(find_amrfinder_db)
    local card_found=$(find_card_db)
    local pointfinder_found=$(find_pointfinder_db)
    local mlst_found=$(find_mlst_db)
    local abricate_found=$(find_abricate_dbs)
    local need_setup=false

    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "VÃ‰RIFICATION DES BASES DE DONNÃ‰ES"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""

    # VÃ©rifier Kraken2
    if [[ -n "$kraken_found" ]]; then
        echo "âœ… Base Kraken2 trouvÃ©e: $kraken_found"
        KRAKEN_DB="$kraken_found"
    else
        echo "âš ï¸  Base Kraken2 NON TROUVÃ‰E"
        need_setup=true
    fi

    # VÃ©rifier AMRFinder
    if [[ -n "$amrfinder_found" ]]; then
        echo "âœ… Base AMRFinder trouvÃ©e: $amrfinder_found"
        AMRFINDER_DB="$amrfinder_found"
    else
        echo "âš ï¸  Base AMRFinder NON TROUVÃ‰E"
        need_setup=true
    fi

    # VÃ©rifier CARD (RGI)
    if [[ -n "$card_found" ]]; then
        echo "âœ… Base CARD trouvÃ©e: $card_found"
        CARD_DB="$card_found"
    else
        echo "âš ï¸  Base CARD (RGI) NON TROUVÃ‰E"
        need_setup=true
    fi

    # VÃ©rifier PointFinder
    if [[ -n "$pointfinder_found" ]]; then
        echo "âœ… Base PointFinder trouvÃ©e: $pointfinder_found"
        POINTFINDER_DB="$pointfinder_found"
    else
        echo "âš ï¸  Base PointFinder NON TROUVÃ‰E"
        need_setup=true
    fi

    # VÃ©rifier MLST
    if [[ -n "$mlst_found" ]]; then
        echo "âœ… Base MLST trouvÃ©e: $mlst_found"
        MLST_DB="$mlst_found"
    else
        echo "âš ï¸  Base MLST NON TROUVÃ‰E"
        need_setup=true
    fi

    # VÃ©rifier Abricate (ResFinder, PlasmidFinder, CARD, NCBI, VFDB)
    if [[ -n "$abricate_found" ]]; then
        echo "âœ… Bases Abricate trouvÃ©es (ResFinder, PlasmidFinder, CARD, NCBI, VFDB)"
    else
        echo "âš ï¸  Bases Abricate NON TROUVÃ‰ES"
        echo "   (ResFinder, PlasmidFinder, CARD, NCBI, VFDB via abricate)"
        need_setup=true
    fi

    echo ""

    # Si mode force, on continue sans demander
    if [[ "$FORCE_MODE" == true ]]; then
        if [[ "$need_setup" == true ]]; then
            echo "Mode --force: TÃ©lÃ©chargement automatique des bases manquantes..."
            echo ""

            if [[ -z "$kraken_found" ]]; then
                echo "Installation de Kraken2 dans l'architecture du pipeline..."
                mkdir -p "$DB_DIR/kraken2_db"
                download_kraken2_db "$DB_DIR/kraken2_db" "minikraken"
                KRAKEN_DB="$DB_DIR/kraken2_db"
            fi

            if [[ -z "$amrfinder_found" ]]; then
                echo "Installation d'AMRFinder dans l'architecture du pipeline..."
                mkdir -p "$DB_DIR/amrfinder_db"
                download_amrfinder_db "$DB_DIR/amrfinder_db"
                AMRFINDER_DB="$DB_DIR/amrfinder_db"
            fi

            if [[ -z "$card_found" ]]; then
                echo "Installation de CARD (RGI) dans l'architecture du pipeline..."
                mkdir -p "$DB_DIR/card_db"
                download_card_db "$DB_DIR/card_db"
                CARD_DB="$DB_DIR/card_db"
            fi

            if [[ -z "$pointfinder_found" ]]; then
                echo "Installation de PointFinder dans l'architecture du pipeline..."
                download_pointfinder_db "$DB_DIR"
                POINTFINDER_DB="$DB_DIR/pointfinder_db"
            fi

            if [[ -z "$mlst_found" ]]; then
                echo "Installation de MLST dans l'architecture du pipeline..."
                mkdir -p "$DB_DIR/mlst_db"
                download_mlst_db "$DB_DIR/mlst_db"
                MLST_DB="$DB_DIR/mlst_db"
            fi

            if [[ -z "$abricate_found" ]]; then
                echo "Installation des bases abricate..."
                setup_abricate_dbs
            fi
        fi
        return 0
    fi

    # Mode interactif si des bases manquent
    if [[ "$need_setup" == true ]]; then
        echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
        echo "â•‘     INSTALLATION DES BASES DE DONNÃ‰ES REQUISES                 â•‘"
        echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "Les bases de donnÃ©es sont nÃ©cessaires pour l'analyse."
        echo ""
        echo "Options d'installation:"
        echo ""
        echo "  1) TÃ©lÃ©charger dans le PIPELINE (portable - recommandÃ©)"
        echo "     â†’ $DB_DIR/"
        echo ""
        echo "  2) TÃ©lÃ©charger dans HOME PARTAGÃ‰ (Ã©conomie d'espace)"
        echo "     â†’ $DB_SHARED_DIR/"
        echo ""
        echo "  3) J'ai dÃ©jÃ  les bases ailleurs (spÃ©cifier les chemins)"
        echo ""
        echo "  4) Continuer SANS les bases (certaines analyses Ã©choueront)"
        echo ""
        echo "  5) Quitter"
        echo ""

        read -p "Votre choix (1-5): " db_choice

        case $db_choice in
            1)
                # TÃ©lÃ©charger dans le pipeline (PORTABLE)
                if [[ -z "$kraken_found" ]]; then
                    echo ""
                    echo "Quelle version de Kraken2 voulez-vous ?"
                    echo "  a) Standard (~50 GB) - ComplÃ¨te et prÃ©cise"
                    echo "  b) MiniKraken (~8 GB) - LÃ©gÃ¨re, recommandÃ©e pour dÃ©buter"
                    echo "  c) Virale (~500 MB) - Virus uniquement"
                    read -p "Votre choix (a/b/c): " kraken_choice

                    mkdir -p "$DB_DIR/kraken2_db"
                    case $kraken_choice in
                        a) download_kraken2_db "$DB_DIR/kraken2_db" "standard" ;;
                        b) download_kraken2_db "$DB_DIR/kraken2_db" "minikraken" ;;
                        c) download_kraken2_db "$DB_DIR/kraken2_db" "viral" ;;
                        *) download_kraken2_db "$DB_DIR/kraken2_db" "minikraken" ;;
                    esac
                    KRAKEN_DB=$(find_kraken2_db)
                fi

                if [[ -z "$amrfinder_found" ]]; then
                    echo ""
                    echo "Installation d'AMRFinder dans le pipeline..."
                    mkdir -p "$DB_DIR/amrfinder_db"
                    download_amrfinder_db "$DB_DIR/amrfinder_db"
                    AMRFINDER_DB="$DB_DIR/amrfinder_db"
                fi

                if [[ -z "$card_found" ]]; then
                    echo ""
                    echo "Installation de CARD (RGI) dans le pipeline..."
                    mkdir -p "$DB_DIR/card_db"
                    download_card_db "$DB_DIR/card_db"
                    CARD_DB="$DB_DIR/card_db"
                fi

                if [[ -z "$pointfinder_found" ]]; then
                    echo ""
                    echo "Installation de PointFinder dans le pipeline..."
                    download_pointfinder_db "$DB_DIR"
                    POINTFINDER_DB="$DB_DIR/pointfinder_db"
                fi

                if [[ -z "$mlst_found" ]]; then
                    echo ""
                    echo "Installation de MLST dans le pipeline..."
                    mkdir -p "$DB_DIR/mlst_db"
                    download_mlst_db "$DB_DIR/mlst_db"
                    MLST_DB="$DB_DIR/mlst_db"
                fi

                if [[ -z "$abricate_found" ]]; then
                    echo ""
                    echo "Installation des bases abricate (ResFinder, PlasmidFinder, etc.)..."
                    setup_abricate_dbs
                fi
                ;;
            2)
                # TÃ©lÃ©charger dans HOME partagÃ©
                mkdir -p "$DB_SHARED_DIR"

                if [[ -z "$kraken_found" ]]; then
                    echo ""
                    echo "Quelle version de Kraken2 voulez-vous ?"
                    echo "  a) Standard (~50 GB)"
                    echo "  b) MiniKraken (~8 GB) - RecommandÃ©e"
                    echo "  c) Virale (~500 MB)"
                    read -p "Votre choix (a/b/c): " kraken_choice

                    mkdir -p "$DB_SHARED_DIR/kraken2_db"
                    case $kraken_choice in
                        a) download_kraken2_db "$DB_SHARED_DIR/kraken2_db" "standard" ;;
                        b) download_kraken2_db "$DB_SHARED_DIR/kraken2_db" "minikraken" ;;
                        c) download_kraken2_db "$DB_SHARED_DIR/kraken2_db" "viral" ;;
                        *) download_kraken2_db "$DB_SHARED_DIR/kraken2_db" "minikraken" ;;
                    esac
                    KRAKEN_DB=$(find_kraken2_db)
                fi

                if [[ -z "$amrfinder_found" ]]; then
                    echo ""
                    echo "Installation d'AMRFinder dans HOME partagÃ©..."
                    mkdir -p "$DB_SHARED_DIR/amrfinder_db"
                    download_amrfinder_db "$DB_SHARED_DIR/amrfinder_db"
                    AMRFINDER_DB="$DB_SHARED_DIR/amrfinder_db"
                fi

                if [[ -z "$card_found" ]]; then
                    echo ""
                    echo "Installation de CARD (RGI) dans HOME partagÃ©..."
                    mkdir -p "$DB_SHARED_DIR/card_db"
                    download_card_db "$DB_SHARED_DIR/card_db"
                    CARD_DB="$DB_SHARED_DIR/card_db"
                fi

                if [[ -z "$pointfinder_found" ]]; then
                    echo ""
                    echo "Installation de PointFinder dans HOME partagÃ©..."
                    download_pointfinder_db "$DB_SHARED_DIR"
                    POINTFINDER_DB="$DB_SHARED_DIR/pointfinder_db"
                fi

                if [[ -z "$mlst_found" ]]; then
                    echo ""
                    echo "Installation de MLST dans HOME partagÃ©..."
                    mkdir -p "$DB_SHARED_DIR/mlst_db"
                    download_mlst_db "$DB_SHARED_DIR/mlst_db"
                    MLST_DB="$DB_SHARED_DIR/mlst_db"
                fi

                if [[ -z "$abricate_found" ]]; then
                    echo ""
                    echo "Installation des bases abricate (ResFinder, PlasmidFinder, etc.)..."
                    setup_abricate_dbs
                fi
                ;;
            3)
                # Chemins personnalisÃ©s
                if [[ -z "$kraken_found" ]]; then
                    echo ""
                    read -p "Chemin vers la base Kraken2: " custom_kraken
                    if [[ -d "$custom_kraken" ]] && [[ -f "$custom_kraken/hash.k2d" ]]; then
                        KRAKEN_DB="$custom_kraken"
                        echo "âœ… Base Kraken2 configurÃ©e: $KRAKEN_DB"
                    else
                        echo "âŒ Base Kraken2 invalide (hash.k2d non trouvÃ©)"
                    fi
                fi

                if [[ -z "$amrfinder_found" ]]; then
                    echo ""
                    read -p "Chemin vers la base AMRFinder: " custom_amr
                    if [[ -d "$custom_amr" ]]; then
                        AMRFINDER_DB="$custom_amr"
                        echo "âœ… Base AMRFinder configurÃ©e: $AMRFINDER_DB"
                    else
                        echo "âŒ Chemin AMRFinder invalide"
                    fi
                fi
                ;;
            4)
                # Continuer sans bases
                echo ""
                echo "âš ï¸  Attention: Certaines analyses Ã©choueront sans les bases de donnÃ©es."
                echo "   - Kraken2 (classification taxonomique) sera ignorÃ©"
                echo "   - AMRFinder sera ignorÃ©"
                echo ""
                KRAKEN_DB=""
                AMRFINDER_DB=""
                ;;
            5)
                echo "ExÃ©cution annulÃ©e."
                exit 0
                ;;
            *)
                echo "Option invalide. Utilisation de l'option 1 par dÃ©faut."
                # Fallback to option 1 (portable)
                if [[ -z "$kraken_found" ]]; then
                    mkdir -p "$DB_DIR/kraken2_db"
                    download_kraken2_db "$DB_DIR/kraken2_db" "minikraken"
                    KRAKEN_DB=$(find_kraken2_db)
                fi
                if [[ -z "$amrfinder_found" ]]; then
                    mkdir -p "$DB_DIR/amrfinder_db"
                    download_amrfinder_db "$DB_DIR/amrfinder_db"
                    AMRFINDER_DB="$DB_DIR/amrfinder_db"
                fi
                ;;
        esac
    fi

    echo ""
    echo "Configuration des bases de donnÃ©es:"
    echo "  KRAKEN_DB: ${KRAKEN_DB:-NON CONFIGURÃ‰}"
    echo "  AMRFINDER_DB: ${AMRFINDER_DB:-NON CONFIGURÃ‰}"
    echo "  CARD_DB: ${CARD_DB:-NON CONFIGURÃ‰}"
    echo "  POINTFINDER_DB: ${POINTFINDER_DB:-NON CONFIGURÃ‰}"
    echo "  MLST_DB: ${MLST_DB:-NON CONFIGURÃ‰}"
    if [[ -n "$abricate_found" ]]; then
        echo "  ABRICATE_DBs: âœ… InstallÃ©es (ResFinder, PlasmidFinder, CARD, NCBI, VFDB)"
    else
        echo "  ABRICATE_DBs: âš ï¸  NON INSTALLÃ‰ES"
    fi
    echo ""
}

#===============================================================================
# SECTION 7 : GESTION DES VERSIONS ET RÃ‰SULTATS
#===============================================================================

# Fonction pour vÃ©rifier les anciens rÃ©sultats
check_old_results() {
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_info "VÃ‰RIFICATION DES ANCIENS RÃ‰SULTATS"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    local old_results=$(find "$WORK_DIR/outputs" -maxdepth 1 -type d -name "${SAMPLE_ID}_*" 2>/dev/null | sort -r)
    
    if [[ -z "$old_results" ]]; then
        log_info "Aucun rÃ©sultat antÃ©rieur trouvÃ© pour $SAMPLE_ID"
        return 0
    fi
    
    log_warn "RÃ©sultats antÃ©rieurs dÃ©tectÃ©s:"
    echo "$old_results" | while read -r dir; do
        local size=$(du -sh "$dir" 2>/dev/null | cut -f1)
        local last_modified=$(stat -f %Sm -t '%Y-%m-%d %H:%M:%S' "$dir" 2>/dev/null || stat -c %y "$dir" 2>/dev/null | cut -d' ' -f1-2)
        log_warn "  - $(basename "$dir") (${size})"
    done
    
    return 1  # Indique qu'il y a des anciens rÃ©sultats
}

# Fonction pour archiver les rÃ©sultats
archive_old_results() {
    local source_dir=$1
    local archive_name="${ARCHIVE_DIR}/$(basename "$source_dir")_archive_$(date '+%Y%m%d_%H%M%S').tar.gz"
    
    mkdir -p "$ARCHIVE_DIR"
    
    log_info "Archivage en cours: $source_dir"
    log_info "Destination: $archive_name"
    
    if tar -czf "$archive_name" -C "$(dirname "$source_dir")" "$(basename "$source_dir")" 2>&1 | tee -a "$LOG_FILE"; then
        log_success "Archivage rÃ©ussi"
        log_info "Taille de l'archive: $(du -sh "$archive_name" | cut -f1)"
        return 0
    else
        log_error "Erreur lors de l'archivage"
        return 1
    fi
}

# Fonction pour nettoyer les anciens rÃ©sultats
cleanup_old_results() {
    local source_dir=$1
    
    log_info "Nettoyage de: $source_dir"
    
    if rm -rf "$source_dir" 2>&1 | tee -a "$LOG_FILE"; then
        log_success "Nettoyage rÃ©ussi"
        return 0
    else
        log_error "Erreur lors du nettoyage"
        return 1
    fi
}

# Fonction pour afficher les options de gestion interactives
interactive_result_management() {
    local old_results=$(find "$WORK_DIR/outputs" -maxdepth 1 -type d -name "${SAMPLE_ID}_*" 2>/dev/null | sort -r)

    if [[ -z "$old_results" ]]; then
        return 0  # Pas de rÃ©sultats antÃ©rieurs
    fi

    # Mode force : continuer automatiquement sans demander
    if [[ "$FORCE_MODE" == true ]]; then
        log_info "Mode --force actif : crÃ©ation d'une nouvelle version sans confirmation"
        log_info "Les anciens rÃ©sultats resteront dans: $WORK_DIR/outputs/"
        return 0
    fi

    log_info ""
    log_info "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    log_info "â•‘         GESTION DES RÃ‰SULTATS ANTÃ‰RIEURS                       â•‘"
    log_info "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_info ""
    log_warn "âš ï¸  Des rÃ©sultats antÃ©rieurs ont Ã©tÃ© dÃ©tectÃ©s pour $SAMPLE_ID"
    log_info ""
    log_info "Options:"
    log_info "  1) Continuer (crÃ©er une nouvelle version)"
    log_info "  2) Archiver les anciens rÃ©sultats PUIS crÃ©er une nouvelle version"
    log_info "  3) Nettoyer les anciens rÃ©sultats PUIS crÃ©er une nouvelle version"
    log_info "  4) Archiver ET nettoyer PUIS crÃ©er une nouvelle version"
    log_info "  5) Quitter sans rien faire"
    log_info ""

    read -p "Choisissez une option (1-5): " choice
    
    case $choice in
        1)
            log_info "âœ… Nouvelle version crÃ©Ã©e: $RESULTS_VERSION"
            log_info "Les anciens rÃ©sultats resteront dans: $WORK_DIR/outputs/"
            ;;
        2)
            log_info "Archivage en cours des anciens rÃ©sultats..."
            echo "$old_results" | while read -r dir; do
                archive_old_results "$dir"
            done
            log_success "âœ… Anciens rÃ©sultats archivÃ©s dans: $ARCHIVE_DIR"
            ;;
        3)
            log_warn "âš ï¸  ATTENTION: Les anciens rÃ©sultats vont Ãªtre SUPPRIMÃ‰S"
            read -p "ÃŠtes-vous sÃ»r? (oui/non): " confirm
            if [[ "$confirm" == "oui" ]]; then
                echo "$old_results" | while read -r dir; do
                    cleanup_old_results "$dir"
                done
                log_success "âœ… Anciens rÃ©sultats supprimÃ©s"
            else
                log_info "OpÃ©ration annulÃ©e"
            fi
            ;;
        4)
            log_info "Archivage et nettoyage en cours..."
            echo "$old_results" | while read -r dir; do
                archive_old_results "$dir" && cleanup_old_results "$dir"
            done
            log_success "âœ… Anciens rÃ©sultats archivÃ©s et supprimÃ©s"
            ;;
        5)
            log_error "ExÃ©cution annulÃ©e par l'utilisateur"
            exit 0
            ;;
        *)
            log_error "Option invalide"
            exit 1
            ;;
    esac
    
    log_info ""
}

# Fonction pour vÃ©rifier les prÃ©requis
check_prerequisites() {
    log_info "VÃ©rification des prÃ©requis..."

    # VÃ©rifier les fichiers input
    if [[ ! -f "$READ1" ]] && [[ ! -f "${READ1}.gz" ]]; then
        log_error "Fichier READ1 introuvable: $READ1"
        return 1
    fi

    # VÃ©rifier READ2 seulement en mode paired-end
    if [[ "$IS_SINGLE_END" != true ]]; then
        if [[ ! -f "$READ2" ]] && [[ ! -f "${READ2}.gz" ]]; then
            log_error "Fichier READ2 introuvable: $READ2"
            return 1
        fi
    fi

    log_success "Tous les prÃ©requis sont satisfaits"
    return 0
}

# Fonction pour crÃ©er les environnements conda si nÃ©cessaire
create_env_if_needed() {
    local env_name=$1
    local packages=$2
    
    if conda env list | grep -q "^${env_name} "; then
        log_success "Environnement '$env_name' existe dÃ©jÃ "
    else
        log_info "CrÃ©ation de l'environnement '$env_name'..."
        conda create -n "$env_name" -c bioconda -c conda-forge $packages -y 2>&1 | tee -a "$LOG_FILE"
        if [ ${PIPESTATUS[0]} -eq 0 ]; then
            log_success "Environnement '$env_name' crÃ©Ã© avec succÃ¨s"
        else
            log_error "Erreur lors de la crÃ©ation de '$env_name'"
            return 1
        fi
    fi
}

#===============================================================================
# SECTION 8 : AFFICHAGE DU DÃ‰MARRAGE
#===============================================================================

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "PIPELINE ARG v3.2 - DÃ‰MARRAGE"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info ""
log_info "Configuration:"
log_info "  Ã‰chantillon: $SAMPLE_ID"
log_info "  Type d'entrÃ©e: $INPUT_TYPE"
log_info "  FASTA prÃ©-assemblÃ©: $IS_ASSEMBLED_INPUT"
log_info "  Mode Prokka: $PROKKA_MODE"
if [[ "$PROKKA_MODE" == "custom" ]] && [[ -n "$PROKKA_GENUS" ]]; then
    log_info "    â†’ Genre: $PROKKA_GENUS"
    log_info "    â†’ EspÃ¨ce: ${PROKKA_SPECIES:-non spÃ©cifiÃ©e}"
fi
log_info "  Version: $RESULTS_VERSION"
log_info "  RÃ©pertoire: $RESULTS_DIR"
log_info "  Threads: $THREADS"
log_info "  Archive: $ARCHIVE_DIR"
log_info ""

if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
    log_warn "Mode FASTA assemblÃ© dÃ©tectÃ©:"
    log_warn "  - Module 1 (QC) sera IGNORÃ‰"
    log_warn "  - Module 2 (Assemblage) sera IGNORÃ‰"
    log_warn "  - Module 5 (Variant Calling) sera IGNORÃ‰"
    log_info ""
fi

#===============================================================================
# SECTION 9 : GESTION DES ANCIENS RÃ‰SULTATS
#===============================================================================

if check_old_results; then
    log_info "Aucun ancien rÃ©sultat Ã  gÃ©rer"
else
    # Il y a des anciens rÃ©sultats
    interactive_result_management
fi

#===============================================================================
# SECTION 9.5 : CONFIGURATION DES BASES DE DONNÃ‰ES
#===============================================================================

# Appeler la fonction de configuration des bases de donnÃ©es
# Cette fonction vÃ©rifie si les DB existent et propose de les tÃ©lÃ©charger si nÃ©cessaire
interactive_database_setup

#===============================================================================
# SECTION 10 : TÃ‰LÃ‰CHARGEMENT/PRÃ‰PARATION DES DONNÃ‰ES
#===============================================================================

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "Ã‰TAPE 0 : TÃ‰LÃ‰CHARGEMENT/PRÃ‰PARATION DES DONNÃ‰ES"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

mkdir -p "$DATA_DIR"

# Variables pour stocker les chemins des fichiers
READ1=""
READ2=""
ASSEMBLY_FASTA=""
IS_SINGLE_END=false

case "$INPUT_TYPE" in
    sra)
        # ============ MODE SRA (FASTQ) ============
        log_info "Mode SRA dÃ©tectÃ© - TÃ©lÃ©chargement des reads FASTQ..."

        # VÃ©rifier si les fichiers existent dÃ©jÃ  localement (paired-end)
        if [[ -f "$DATA_DIR/${SAMPLE_ID}_1.fastq" ]]; then
            log_success "Fichiers FASTQ paired-end trouvÃ©s localement"
            READ1="$DATA_DIR/${SAMPLE_ID}_1.fastq"
            READ2="$DATA_DIR/${SAMPLE_ID}_2.fastq"
            IS_SINGLE_END=false
        elif [[ -f "$DATA_DIR/${SAMPLE_ID}_1.fastq.gz" ]]; then
            log_success "Fichiers FASTQ paired-end (.gz) trouvÃ©s localement"
            READ1="$DATA_DIR/${SAMPLE_ID}_1.fastq.gz"
            READ2="$DATA_DIR/${SAMPLE_ID}_2.fastq.gz"
            IS_SINGLE_END=false
        # VÃ©rifier si fichier single-end existe
        elif [[ -f "$DATA_DIR/${SAMPLE_ID}.fastq" ]]; then
            log_success "Fichier FASTQ single-end trouvÃ© localement"
            READ1="$DATA_DIR/${SAMPLE_ID}.fastq"
            READ2=""
            IS_SINGLE_END=true
        elif [[ -f "$DATA_DIR/${SAMPLE_ID}.fastq.gz" ]]; then
            log_success "Fichier FASTQ single-end (.gz) trouvÃ© localement"
            READ1="$DATA_DIR/${SAMPLE_ID}.fastq.gz"
            READ2=""
            IS_SINGLE_END=true
        else
            # TÃ©lÃ©charger avec prefetch dans un rÃ©pertoire temporaire
            TEMP_DOWNLOAD_DIR=$(mktemp -d)
            log_info "TÃ©lÃ©chargement de l'Ã©chantillon $SAMPLE_ID dans $TEMP_DOWNLOAD_DIR..."

            # Utiliser pushd/popd pour la gestion correcte des rÃ©pertoires
            pushd "$TEMP_DOWNLOAD_DIR" > /dev/null || { log_error "Impossible d'accÃ©der Ã  $TEMP_DOWNLOAD_DIR"; exit 1; }

            prefetch "$SAMPLE_ID" --output-directory . 2>&1 | tee -a "$LOG_FILE"
            if [[ ${PIPESTATUS[0]} -ne 0 ]]; then
                log_error "Ã‰chec du tÃ©lÃ©chargement SRA (prefetch) pour $SAMPLE_ID"
                popd > /dev/null
                rm -rf "$TEMP_DOWNLOAD_DIR"
                exit 1
            fi

            # Convertir en FASTQ
            log_info "Conversion en FASTQ..."
            fasterq-dump "$SAMPLE_ID" --split-files --outdir . 2>&1 | tee -a "$LOG_FILE"
            if [[ ${PIPESTATUS[0]} -ne 0 ]]; then
                log_error "Ã‰chec de la conversion FASTQ (fasterq-dump) pour $SAMPLE_ID"
                popd > /dev/null
                rm -rf "$TEMP_DOWNLOAD_DIR"
                exit 1
            fi

            # DÃ©tecter automatiquement single-end vs paired-end
            if [[ -f "${SAMPLE_ID}_1.fastq" ]] && [[ -f "${SAMPLE_ID}_2.fastq" ]]; then
                # Mode paired-end
                log_info "DonnÃ©es paired-end dÃ©tectÃ©es"
                mv "${SAMPLE_ID}_1.fastq" "$DATA_DIR/${SAMPLE_ID}_1.fastq"
                mv "${SAMPLE_ID}_2.fastq" "$DATA_DIR/${SAMPLE_ID}_2.fastq"
                READ1="$DATA_DIR/${SAMPLE_ID}_1.fastq"
                READ2="$DATA_DIR/${SAMPLE_ID}_2.fastq"
                IS_SINGLE_END=false
            elif [[ -f "${SAMPLE_ID}.fastq" ]]; then
                # Mode single-end (fichier sans suffixe)
                log_info "DonnÃ©es single-end dÃ©tectÃ©es"
                mv "${SAMPLE_ID}.fastq" "$DATA_DIR/${SAMPLE_ID}.fastq"
                READ1="$DATA_DIR/${SAMPLE_ID}.fastq"
                READ2=""
                IS_SINGLE_END=true
            elif [[ -f "${SAMPLE_ID}_1.fastq" ]]; then
                # Mode single-end (fichier avec _1 mais pas de _2)
                log_info "DonnÃ©es single-end dÃ©tectÃ©es (format _1)"
                mv "${SAMPLE_ID}_1.fastq" "$DATA_DIR/${SAMPLE_ID}.fastq"
                READ1="$DATA_DIR/${SAMPLE_ID}.fastq"
                READ2=""
                IS_SINGLE_END=true
            else
                log_error "Aucun fichier FASTQ trouvÃ© aprÃ¨s conversion"
                ls -la . | tee -a "$LOG_FILE"
                popd > /dev/null
                rm -rf "$TEMP_DOWNLOAD_DIR"
                exit 1
            fi

            # Revenir au rÃ©pertoire original
            popd > /dev/null

            # Nettoyer le rÃ©pertoire temporaire
            rm -rf "$TEMP_DOWNLOAD_DIR"
        fi
        ;;

    genbank)
        # ============ MODE GENBANK (FASTA) ============
        log_info "Mode GenBank dÃ©tectÃ© - TÃ©lÃ©chargement de la sÃ©quence..."

        # VÃ©rifier si le fichier existe dÃ©jÃ 
        if [[ -f "$DATA_DIR/${SAMPLE_ID}.fasta" ]]; then
            log_success "Fichier FASTA trouvÃ© localement"
            ASSEMBLY_FASTA="$DATA_DIR/${SAMPLE_ID}.fasta"
        else
            download_genbank_sequence "$SAMPLE_ID" "$DATA_DIR"
            if [[ $? -ne 0 ]] || [[ -z "$DOWNLOADED_FILE" ]]; then
                log_error "Ã‰chec du tÃ©lÃ©chargement de la sÃ©quence GenBank"
                exit 1
            fi
            ASSEMBLY_FASTA="$DOWNLOADED_FILE"
        fi
        ;;

    assembly)
        # ============ MODE ASSEMBLAGE NCBI (FASTA) ============
        log_info "Mode Assemblage NCBI dÃ©tectÃ© - TÃ©lÃ©chargement de l'assemblage..."

        # VÃ©rifier si le fichier existe dÃ©jÃ 
        if [[ -f "$DATA_DIR/${SAMPLE_ID}_genomic.fna" ]]; then
            log_success "Fichier assemblage trouvÃ© localement"
            ASSEMBLY_FASTA="$DATA_DIR/${SAMPLE_ID}_genomic.fna"
        elif [[ -f "$DATA_DIR/${SAMPLE_ID}.fasta" ]]; then
            log_success "Fichier FASTA trouvÃ© localement"
            ASSEMBLY_FASTA="$DATA_DIR/${SAMPLE_ID}.fasta"
        else
            download_ncbi_assembly "$SAMPLE_ID" "$DATA_DIR"
            if [[ $? -ne 0 ]] || [[ -z "$DOWNLOADED_FILE" ]]; then
                log_error "Ã‰chec du tÃ©lÃ©chargement de l'assemblage NCBI"
                exit 1
            fi
            ASSEMBLY_FASTA="$DOWNLOADED_FILE"
        fi
        ;;

    local_fasta)
        # ============ MODE FICHIER LOCAL (FASTA) ============
        log_info "Mode fichier local dÃ©tectÃ© - Configuration du fichier FASTA..."

        if [[ -f "$LOCAL_FASTA_PATH" ]]; then
            setup_local_fasta "$LOCAL_FASTA_PATH" "$DATA_DIR" "$SAMPLE_ID"
            if [[ $? -ne 0 ]] || [[ -z "$DOWNLOADED_FILE" ]]; then
                log_error "Ã‰chec de la configuration du fichier FASTA local"
                exit 1
            fi
            ASSEMBLY_FASTA="$DOWNLOADED_FILE"
        else
            log_error "Fichier FASTA introuvable: $LOCAL_FASTA_PATH"
            exit 1
        fi
        ;;

    *)
        log_error "Type d'entrÃ©e non supportÃ©: $INPUT_TYPE"
        exit 1
        ;;
esac

# Afficher les fichiers disponibles
log_info ""
log_info "Fichiers disponibles:"
if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
    log_info "  FASTA assemblÃ©: $ASSEMBLY_FASTA"
    ls -lh "$ASSEMBLY_FASTA" 2>/dev/null | tee -a "$LOG_FILE" || true
elif [[ "$IS_SINGLE_END" == true ]]; then
    log_info "  Mode: Single-end"
    log_info "  Read: $READ1"
    ls -lh "$READ1" 2>/dev/null | tee -a "$LOG_FILE" || true
else
    log_info "  Mode: Paired-end"
    log_info "  Read 1: $READ1"
    log_info "  Read 2: $READ2"
    ls -lh "$READ1" "$READ2" 2>/dev/null | tee -a "$LOG_FILE" || true
fi

log_success "DonnÃ©es prÃªtes"

#===============================================================================
# SECTION 11 : VÃ‰RIFICATION ET CRÃ‰ATION DES ENVIRONNEMENTS CONDA
#===============================================================================

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "VÃ‰RIFICATION ET CRÃ‰ATION DES ENVIRONNEMENTS CONDA"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# CrÃ©er uniquement les environnements nÃ©cessaires selon le type d'entrÃ©e
if [[ "$IS_ASSEMBLED_INPUT" == false ]]; then
    # Module 01 : QC et Nettoyage (seulement si reads)
    create_env_if_needed "qc_arg" \
        "fastqc=0.12.1 fastp=0.23.4 kraken2=2.1.3 multiqc=1.19"

    # Module 02 : Assemblage (seulement si reads)
    create_env_if_needed "assembly_arg" \
        "spades=3.15.5 quast=5.2.0 seqkit=2.5.1"

    # Module 05 : Variant Calling (seulement si reads)
    create_env_if_needed "variant_arg" \
        "snippy=4.6.0 samtools=1.18 bcftools=1.18"
fi

# Module 03 : Annotation (toujours requis)
create_env_if_needed "annotation_arg" \
    "prokka=1.14.6"

# Module 04 : DÃ©tection ARG (toujours requis)
# kma : dÃ©tection ARG haute sensibilitÃ© sur reads bruts
# blast : recherche de sÃ©quences ARG dans les reads
create_env_if_needed "arg_detection" \
    "ncbi-amrfinderplus=4.2 abricate=1.0.1 kma blast"

# Module 06 : Analyse et InterprÃ©tation (toujours requis)
create_env_if_needed "analysis_arg" \
    "python=3.11 pandas matplotlib seaborn openpyxl biopython"

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "RÃ‰SUMÃ‰ DES ENVIRONNEMENTS"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
conda env list | grep -E "^(qc_arg|assembly_arg|annotation_arg|arg_detection|variant_arg|analysis_arg)" || log_warn "Aucun environnement trouvÃ©"

log_success "Configuration des environnements terminÃ©e"

#===============================================================================
# MODULE 1 : CONTRÃ”LE QUALITÃ‰ (QC) - IGNORÃ‰ SI FASTA ASSEMBLÃ‰
#===============================================================================

if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_warn "MODULE 1 : CONTRÃ”LE QUALITÃ‰ (QC) - IGNORÃ‰ (entrÃ©e FASTA assemblÃ©e)"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
else
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_info "MODULE 1 : CONTRÃ”LE QUALITÃ‰ (QC)"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    # VÃ©rifier les prÃ©requis
    check_prerequisites || { log_error "PrÃ©requis non satisfaits"; exit 1; }

# Activer l'environnement conda
conda activate qc_arg

#------- 1.1 FastQC sur reads bruts -------
log_info "1.1 FastQC sur reads bruts..."

if [[ "$IS_SINGLE_END" == true ]]; then
    fastqc \
        --outdir "$RESULTS_DIR"/01_qc/fastqc_raw \
        --threads "$THREADS" \
        "$READ1" 2>&1 | tee -a "$LOG_FILE"
    open_file_safe "$RESULTS_DIR/01_qc/fastqc_raw/${SAMPLE_ID}_fastqc.html" "FastQC Report"
else
    fastqc \
        --outdir "$RESULTS_DIR"/01_qc/fastqc_raw \
        --threads "$THREADS" \
        "$READ1" \
        "$READ2" 2>&1 | tee -a "$LOG_FILE"
    open_file_safe "$RESULTS_DIR/01_qc/fastqc_raw/${SAMPLE_ID}_1_fastqc.html" "FastQC Read 1 Report"
    open_file_safe "$RESULTS_DIR/01_qc/fastqc_raw/${SAMPLE_ID}_2_fastqc.html" "FastQC Read 2 Report"
fi

log_success "FastQC brut terminÃ©"

#------- 1.2 Nettoyage avec Fastp -------
log_info "1.2 Nettoyage avec Fastp..."

if [[ "$IS_SINGLE_END" == true ]]; then
    # Mode single-end
    fastp \
        --in1 "$READ1" \
        --out1 "$RESULTS_DIR"/01_qc/fastp/"${SAMPLE_ID}"_clean.fastq.gz \
        --json "$RESULTS_DIR"/01_qc/fastp/"${SAMPLE_ID}"_fastp.json \
        --html "$RESULTS_DIR"/01_qc/fastp/"${SAMPLE_ID}"_fastp.html \
        --thread "$THREADS" \
        --qualified_quality_phred 20 \
        --unqualified_percent_limit 40 \
        --length_required 30 \
        --dedup \
        --dup_calc_accuracy 4 \
        --cut_front \
        --cut_tail \
        --cut_window_size 4 \
        --cut_mean_quality 20 2>&1 | tee -a "$LOG_FILE"

    # Variable pour le read nettoyÃ©
    CLEAN_R1="$RESULTS_DIR/01_qc/fastp/${SAMPLE_ID}_clean.fastq.gz"
    CLEAN_R2=""
else
    # Mode paired-end
    fastp \
        --in1 "$READ1" \
        --in2 "$READ2" \
        --out1 "$RESULTS_DIR"/01_qc/fastp/"${SAMPLE_ID}"_clean_R1.fastq.gz \
        --out2 "$RESULTS_DIR"/01_qc/fastp/"${SAMPLE_ID}"_clean_R2.fastq.gz \
        --json "$RESULTS_DIR"/01_qc/fastp/"${SAMPLE_ID}"_fastp.json \
        --html "$RESULTS_DIR"/01_qc/fastp/"${SAMPLE_ID}"_fastp.html \
        --thread "$THREADS" \
        --qualified_quality_phred 20 \
        --unqualified_percent_limit 40 \
        --length_required 30 \
        --detect_adapter_for_pe \
        --dedup \
        --dup_calc_accuracy 4 \
        --correction \
        --cut_front \
        --cut_tail \
        --cut_window_size 4 \
        --cut_mean_quality 20 2>&1 | tee -a "$LOG_FILE"

    # Variables pour les reads nettoyÃ©s
    CLEAN_R1="$RESULTS_DIR/01_qc/fastp/${SAMPLE_ID}_clean_R1.fastq.gz"
    CLEAN_R2="$RESULTS_DIR/01_qc/fastp/${SAMPLE_ID}_clean_R2.fastq.gz"
fi

log_success "Nettoyage Fastp terminÃ©"

# VÃ©rifier que fastp a produit des reads nettoyÃ©s (fichier non vide)
if [[ ! -s "$CLEAN_R1" ]]; then
    log_error "Fastp a filtrÃ© 100% des reads : le fichier nettoyÃ© est vide ($CLEAN_R1)"
    log_error "Les scores de qualitÃ© sont trop bas pour le seuil configurÃ© (--qualified_quality_phred 20)"
    log_error "Solutions possibles :"
    log_error "  1. VÃ©rifiez la qualitÃ© brute des reads (voir le rapport FastQC ci-dessus)"
    log_error "  2. Ce jeu de donnÃ©es n'est peut-Ãªtre pas compatible avec ce pipeline (ex: donnÃ©es ONT/PacBio)"
    log_error "  3. Le tÃ©lÃ©chargement SRA a peut-Ãªtre produit des donnÃ©es corrompues"
    exit 1
fi

open_file_safe "$RESULTS_DIR/01_qc/fastp/${SAMPLE_ID}_fastp.html" "Fastp QC Report"

#------- 1.3 Classification taxonomique avec Kraken2 -------
log_info "1.3 Classification taxonomique avec Kraken2..."

# VÃ©rifier la base de donnÃ©es
if [[ -z "$KRAKEN_DB" ]]; then
    log_warn "Kraken2 IGNORÃ‰ (base de donnÃ©es non configurÃ©e)"
elif [[ ! -d "$KRAKEN_DB" ]]; then
    log_warn "Base Kraken2 non trouvÃ©e: $KRAKEN_DB"
    log_info "ExÃ©cutez le pipeline avec l'option de tÃ©lÃ©chargement des bases de donnÃ©es."
else
    if [[ "$IS_SINGLE_END" == true ]]; then
        kraken2 \
            --db "$KRAKEN_DB" \
            "$READ1" \
            --output "$RESULTS_DIR"/01_qc/kraken2/"${SAMPLE_ID}"_kraken2.out \
            --report "$RESULTS_DIR"/01_qc/kraken2/"${SAMPLE_ID}"_kraken2.report \
            --threads "$THREADS" \
            --use-names 2>&1 | tee -a "$LOG_FILE"
    else
        kraken2 \
            --db "$KRAKEN_DB" \
            --paired "$READ1" "$READ2" \
            --output "$RESULTS_DIR"/01_qc/kraken2/"${SAMPLE_ID}"_kraken2.out \
            --report "$RESULTS_DIR"/01_qc/kraken2/"${SAMPLE_ID}"_kraken2.report \
            --threads "$THREADS" \
            --use-names 2>&1 | tee -a "$LOG_FILE"
    fi

    log_info "Top 10 espÃ¨ces dÃ©tectÃ©es:"
    head -20 "$RESULTS_DIR"/01_qc/kraken2/"${SAMPLE_ID}"_kraken2.report 2>&1 | tee -a "$LOG_FILE"

    # Extraction de l'espÃ¨ce pour Prokka (si mode auto)
    if [[ "$PROKKA_MODE" == "auto" ]]; then
        extract_species_from_kraken2 "$RESULTS_DIR/01_qc/kraken2/${SAMPLE_ID}_kraken2.report" || true
    fi

    log_success "Kraken2 terminÃ©"
fi

#------- 1.4 FastQC sur reads nettoyÃ©s -------
log_info "1.4 FastQC sur reads nettoyÃ©s..."

if [[ "$IS_SINGLE_END" == true ]]; then
    fastqc \
        --outdir "$RESULTS_DIR"/01_qc/fastqc_clean \
        --threads "$THREADS" \
        "$CLEAN_R1" 2>&1 | tee -a "$LOG_FILE"
else
    fastqc \
        --outdir "$RESULTS_DIR"/01_qc/fastqc_clean \
        --threads "$THREADS" \
        "$CLEAN_R1" \
        "$CLEAN_R2" 2>&1 | tee -a "$LOG_FILE"
fi

log_success "FastQC nettoyÃ© terminÃ©"

#------- 1.5 Rapport MultiQC -------
log_info "1.5 GÃ©nÃ©ration du rapport MultiQC..."

multiqc \
    "$RESULTS_DIR"/01_qc/fastqc_raw \
    "$RESULTS_DIR"/01_qc/fastqc_clean \
    "$RESULTS_DIR"/01_qc/fastp \
    "$RESULTS_DIR"/01_qc/kraken2 \
    --outdir "$RESULTS_DIR"/01_qc/multiqc \
    --filename "${SAMPLE_ID}"_multiqc_report \
    --title "QC Report - $SAMPLE_ID" \
    --force 2>&1 | tee -a "$LOG_FILE"

log_success "Rapport MultiQC: $RESULTS_DIR/01_qc/multiqc/${SAMPLE_ID}_multiqc_report.html"

open_file_safe "$RESULTS_DIR/01_qc/multiqc/${SAMPLE_ID}_multiqc_report.html" "MultiQC Report"

    # DÃ©sactiver l'environnement
    conda deactivate

    log_success "MODULE 1 TERMINÃ‰"
fi  # Fin du bloc conditionnel Module 1

#===============================================================================
# MODULE 2 : ASSEMBLAGE DU GÃ‰NOME - IGNORÃ‰ SI FASTA ASSEMBLÃ‰
#===============================================================================

if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_warn "MODULE 2 : ASSEMBLAGE DU GÃ‰NOME - IGNORÃ‰ (entrÃ©e FASTA assemblÃ©e)"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    # Copier le FASTA assemblÃ© vers le rÃ©pertoire d'assemblage filtrÃ©
    log_info "Copie du FASTA assemblÃ© vers le rÃ©pertoire de travail..."
    cp "$ASSEMBLY_FASTA" "$RESULTS_DIR/02_assembly/filtered/${SAMPLE_ID}_filtered.fasta"
    log_success "FASTA assemblÃ© prÃªt pour l'annotation"
    
    #------- Classification taxonomique avec Kraken2 sur FASTA assemblÃ© -------
    log_info "Classification taxonomique avec Kraken2 sur le gÃ©nome assemblÃ©..."

    # Activer l'environnement conda pour Kraken2
    conda activate qc_arg

    # VÃ©rifier la base de donnÃ©es Kraken2
    if [[ -z "$KRAKEN_DB" ]]; then
        log_warn "Kraken2 IGNORÃ‰ (base de donnÃ©es non configurÃ©e)"
    elif [[ ! -d "$KRAKEN_DB" ]]; then
        log_warn "Base Kraken2 non trouvÃ©e: $KRAKEN_DB"
        log_info "ExÃ©cutez le pipeline avec l'option de tÃ©lÃ©chargement des bases de donnÃ©es."
    else
        # CrÃ©er le rÃ©pertoire kraken2 s'il n'existe pas
        mkdir -p "$RESULTS_DIR/01_qc/kraken2"
        
        # ExÃ©cuter Kraken2 sur le fichier FASTA assemblÃ©
        kraken2 \
            --db "$KRAKEN_DB" \
            "$RESULTS_DIR/02_assembly/filtered/${SAMPLE_ID}_filtered.fasta" \
            --output "$RESULTS_DIR/01_qc/kraken2/${SAMPLE_ID}_kraken2.out" \
            --report "$RESULTS_DIR/01_qc/kraken2/${SAMPLE_ID}_kraken2.report" \
            --threads "$THREADS" \
            --use-names 2>&1 | tee -a "$LOG_FILE"
        
        log_info "Top 10 espÃ¨ces dÃ©tectÃ©es dans le gÃ©nome assemblÃ©:"
        head -20 "$RESULTS_DIR/01_qc/kraken2/${SAMPLE_ID}_kraken2.report" 2>&1 | tee -a "$LOG_FILE"

        # Extraction de l'espÃ¨ce pour Prokka (si mode auto)
        if [[ "$PROKKA_MODE" == "auto" ]]; then
            extract_species_from_kraken2 "$RESULTS_DIR/01_qc/kraken2/${SAMPLE_ID}_kraken2.report" || true
        fi

        log_success "Classification Kraken2 terminÃ©e"
    fi
    
    conda deactivate
else
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_info "MODULE 2 : ASSEMBLAGE DU GÃ‰NOME"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    conda activate assembly_arg

#------- 2.1 Assemblage avec SPAdes (AVEC --isolate) -------
log_info "2.1 Assemblage SPAdes (mode isolate pour culture pure)..."

if [[ "$IS_SINGLE_END" == true ]]; then
    # Mode single-end
    log_info "  Mode single-end dÃ©tectÃ©"
    spades.py \
        -s "$CLEAN_R1" \
        -o "$RESULTS_DIR"/02_assembly/spades \
        --threads "$THREADS" \
        --memory 16 \
        --isolate \
        --cov-cutoff auto 2>&1 | tee -a "$LOG_FILE"
else
    # Mode paired-end
    spades.py \
        -1 "$CLEAN_R1" \
        -2 "$CLEAN_R2" \
        -o "$RESULTS_DIR"/02_assembly/spades \
        --threads "$THREADS" \
        --memory 16 \
        --isolate \
        --cov-cutoff auto 2>&1 | tee -a "$LOG_FILE"
fi

# VÃ©rifier que SPAdes a produit des fichiers
if [[ ! -f "$RESULTS_DIR/02_assembly/spades/contigs.fasta" ]]; then
    log_error "Ã‰CHEC SPAdes: Fichier contigs.fasta non crÃ©Ã©"
    log_error "  Consultez le log SPAdes: $RESULTS_DIR/02_assembly/spades/spades.log"
    exit 1
fi

# Copier les fichiers principaux
cp "$RESULTS_DIR"/02_assembly/spades/contigs.fasta \
   "$RESULTS_DIR"/02_assembly/spades/"${SAMPLE_ID}"_contigs.fasta

if [[ -f "$RESULTS_DIR/02_assembly/spades/scaffolds.fasta" ]]; then
    cp "$RESULTS_DIR"/02_assembly/spades/scaffolds.fasta \
       "$RESULTS_DIR"/02_assembly/spades/"${SAMPLE_ID}"_scaffolds.fasta
fi

log_success "Assemblage SPAdes terminÃ©"

#------- 2.2 Filtrage des contigs (>= 500 bp) -------
log_info "2.2 Filtrage des contigs (>= 500 bp)..."

seqkit seq \
    -m 500 \
    "$RESULTS_DIR"/02_assembly/spades/"${SAMPLE_ID}"_contigs.fasta \
    > "$RESULTS_DIR"/02_assembly/filtered/"${SAMPLE_ID}"_filtered.fasta

# VÃ©rification critique : le fichier filtrÃ© contient-il des sÃ©quences ?
FILTERED_CONTIGS_COUNT=$(grep -c "^>" "$RESULTS_DIR"/02_assembly/filtered/"${SAMPLE_ID}"_filtered.fasta 2>/dev/null || echo "0")

if [[ "$FILTERED_CONTIGS_COUNT" -eq 0 ]]; then
    log_error "Ã‰CHEC ASSEMBLAGE: Aucun contig >= 500 bp produit"
    log_error "  Les donnÃ©es d'entrÃ©e sont probablement insuffisantes ou de mauvaise qualitÃ©"
    log_error "  VÃ©rifiez:"
    log_error "    - La qualitÃ© des reads (FastQC)"
    log_error "    - Le nombre de reads (minimum ~100k pour bactÃ©ries)"
    log_error "    - Le type de donnÃ©es (WGS vs amplicon)"
    log_error ""
    log_error "Pipeline arrÃªtÃ©. Consultez le log SPAdes pour plus de dÃ©tails:"
    log_error "  $RESULTS_DIR/02_assembly/spades/spades.log"
    exit 1
fi

log_success "Filtrage des contigs terminÃ© ($FILTERED_CONTIGS_COUNT contigs >= 500 bp)"

#------- 2.3 Statistiques d'assemblage avec QUAST -------
log_info "2.3 Statistiques d'assemblage avec QUAST..."

quast.py \
    "$RESULTS_DIR"/02_assembly/filtered/"${SAMPLE_ID}"_filtered.fasta \
    -o "$RESULTS_DIR"/02_assembly/quast \
    --threads "$THREADS" 2>&1 | tee -a "$LOG_FILE"

log_success "Statistiques QUAST gÃ©nÃ©rÃ©es"

    conda deactivate

    log_success "MODULE 2 TERMINÃ‰"
fi  # Fin du bloc conditionnel Module 2

#===============================================================================
# MODULE 3 : ANNOTATION DU GÃ‰NOME
#===============================================================================

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "MODULE 3 : ANNOTATION DU GÃ‰NOME"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

conda activate annotation_arg

#------- 3.1 Annotation avec Prokka -------
log_info "3.1 Annotation avec Prokka..."
log_info "  Mode Prokka: $PROKKA_MODE"

# Construction des arguments Prokka selon le mode choisi
PROKKA_ARGS="--outdir $RESULTS_DIR/03_annotation/prokka"
PROKKA_ARGS="$PROKKA_ARGS --prefix $SAMPLE_ID"
PROKKA_ARGS="$PROKKA_ARGS --cpu $THREADS"
PROKKA_ARGS="$PROKKA_ARGS --locustag ARG"
PROKKA_ARGS="$PROKKA_ARGS --force"

case "$PROKKA_MODE" in
    auto)
        # Utiliser les valeurs dÃ©tectÃ©es par Kraken2
        if [[ -n "$PROKKA_GENUS" ]]; then
            log_info "  Genre dÃ©tectÃ©: $PROKKA_GENUS"
            PROKKA_ARGS="$PROKKA_ARGS --genus $PROKKA_GENUS"
            if [[ -n "$PROKKA_SPECIES" ]] && [[ "$PROKKA_SPECIES" != "sp." ]]; then
                log_info "  EspÃ¨ce dÃ©tectÃ©e: $PROKKA_SPECIES"
                PROKKA_ARGS="$PROKKA_ARGS --species $PROKKA_SPECIES"
            fi
        else
            log_warn "  Aucune espÃ¨ce dÃ©tectÃ©e par Kraken2, mode gÃ©nÃ©rique utilisÃ©"
        fi
        ;;
    generic)
        # Mode universel - pas de --genus/--species
        log_info "  Mode gÃ©nÃ©rique (toutes bactÃ©ries)"
        ;;
    ecoli)
        # Mode legacy E. coli K-12
        log_info "  Mode Escherichia coli K-12"
        PROKKA_ARGS="$PROKKA_ARGS --genus Escherichia --species coli --strain K-12"
        ;;
    custom)
        # Mode personnalisÃ© avec genus/species fournis par l'utilisateur
        if [[ -n "$PROKKA_GENUS" ]]; then
            log_info "  Genre personnalisÃ©: $PROKKA_GENUS"
            PROKKA_ARGS="$PROKKA_ARGS --genus $PROKKA_GENUS"
            if [[ -n "$PROKKA_SPECIES" ]]; then
                log_info "  EspÃ¨ce personnalisÃ©e: $PROKKA_SPECIES"
                PROKKA_ARGS="$PROKKA_ARGS --species $PROKKA_SPECIES"
            fi
        else
            log_warn "  Mode custom sans genre spÃ©cifiÃ©, utilisation du mode gÃ©nÃ©rique"
        fi
        ;;
    *)
        log_warn "  Mode Prokka inconnu: $PROKKA_MODE, utilisation du mode gÃ©nÃ©rique"
        ;;
esac

# ExÃ©cution de Prokka avec les arguments construits
log_info "  Commande: prokka $PROKKA_ARGS <fasta>"
prokka $PROKKA_ARGS "$RESULTS_DIR"/02_assembly/filtered/"${SAMPLE_ID}"_filtered.fasta 2>&1 | tee -a "$LOG_FILE"

log_success "Annotation Prokka terminÃ©e"

#------- 3.2 Statistiques d'annotation -------
log_info "3.2 Statistiques d'annotation..."

log_success "Statistiques d'annotation disponibles"

conda deactivate

log_success "MODULE 3 TERMINÃ‰"

#===============================================================================
# MODULE 3.3 : TYPAGE MLST (Sequence Type)
#===============================================================================

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "MODULE 3.3 : TYPAGE MLST (Multi-Locus Sequence Typing)"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# CrÃ©er le rÃ©pertoire de sortie MLST
mkdir -p "$RESULTS_DIR/03_annotation/mlst"

# Variables pour stocker les rÃ©sultats MLST
MLST_SCHEME=""
MLST_ST=""
MLST_ALLELES=""

# Activer l'environnement assembly_arg oÃ¹ mlst est installÃ©
conda activate assembly_arg

# Configurer PERL5LIB pour mlst (nÃ©cessaire si installÃ© manuellement)
# Initialiser PERL5LIB si non dÃ©fini pour Ã©viter unbound variable
export PERL5LIB="${PERL5LIB:-}:${CONDA_PREFIX:-}/lib/perl5/site_perl:${CONDA_PREFIX:-}/lib/perl5"

# VÃ©rifier si mlst est disponible
if command -v mlst &> /dev/null; then
    log_info "3.3.1 ExÃ©cution du typage MLST..."

    # Fichier d'entrÃ©e (contigs filtrÃ©s ou assemblage fourni)
    if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
        MLST_INPUT="$RESULTS_DIR/03_annotation/prokka/${SAMPLE_ID}.fna"
    else
        MLST_INPUT="$RESULTS_DIR/02_assembly/filtered/${SAMPLE_ID}_filtered.fasta"
    fi

    # ExÃ©cution de mlst
    MLST_OUTPUT="$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst.tsv"

    # DÃ©finir le chemin de la base MLST si non dÃ©fini
    if [[ -z "${MLST_DB:-}" ]]; then
        MLST_DB=$(find_mlst_db)
    fi

    if [[ -f "$MLST_INPUT" ]]; then
        # Utiliser --datadir si une base personnalisÃ©e est dÃ©finie
        if [[ -n "$MLST_DB" ]] && [[ -d "$MLST_DB/db" ]]; then
            mlst --threads "$THREADS" --datadir "$MLST_DB/db/pubmlst" --blastdb "$MLST_DB/db/blast/mlst.fa" "$MLST_INPUT" > "$MLST_OUTPUT" 2>> "$LOG_FILE"
        else
            mlst --threads "$THREADS" "$MLST_INPUT" > "$MLST_OUTPUT" 2>> "$LOG_FILE"
        fi

        if [[ -s "$MLST_OUTPUT" ]]; then
            # Parser les rÃ©sultats
            MLST_SCHEME=$(cut -f2 "$MLST_OUTPUT" | head -1)
            MLST_ST=$(cut -f3 "$MLST_OUTPUT" | head -1)
            MLST_ALLELES=$(cut -f4- "$MLST_OUTPUT" | head -1)

            log_success "Typage MLST terminÃ©"
            log_info "  â†’ SchÃ©ma: $MLST_SCHEME"
            log_info "  â†’ Sequence Type: ST$MLST_ST"
            log_info "  â†’ AllÃ¨les: $MLST_ALLELES"

            # CrÃ©er un fichier de rÃ©sumÃ© lisible
            cat > "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" << EOF
=== RÃ‰SULTATS MLST ===
Ã‰chantillon: $SAMPLE_ID
SchÃ©ma: $MLST_SCHEME
Sequence Type: ST$MLST_ST
AllÃ¨les: $MLST_ALLELES

InterprÃ©tation:
EOF

            # Ajouter des informations contextuelles selon le ST
            case "$MLST_SCHEME" in
                saureus)
                    case "$MLST_ST" in
                        8) echo "  ST8 = Clone USA300 (CA-MRSA Ã©pidÃ©mique)" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        5) echo "  ST5 = Clone pandÃ©mique HA-MRSA" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        22) echo "  ST22 = Clone EMRSA-15" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        36) echo "  ST36 = Clone USA200" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        239) echo "  ST239 = Clone BrÃ©silien/Hongrois" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        398) echo "  ST398 = Clone LA-MRSA (animaux)" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        *) echo "  ST$MLST_ST = Voir PubMLST pour plus d'informations" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                    esac
                    ;;
                klebsiella|kpneumoniae)
                    case "$MLST_ST" in
                        258) echo "  ST258 = Clone KPC Ã©pidÃ©mique mondial" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        11) echo "  ST11 = Clone KPC asiatique" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        15) echo "  ST15 = Clone ESBL rÃ©pandu" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        147) echo "  ST147 = Clone NDM Ã©mergent" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        307) echo "  ST307 = Clone KPC Ã©mergent" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        *) echo "  ST$MLST_ST = Voir PubMLST pour plus d'informations" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                    esac
                    ;;
                ecoli)
                    case "$MLST_ST" in
                        131) echo "  ST131 = Clone ESBL/FQ-R pandÃ©mique" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        410) echo "  ST410 = Clone carbapÃ©nÃ©mase Ã©mergent" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        69) echo "  ST69 = Clone MDR" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        10) echo "  ST10 = Clone commun, souvent ESBL" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        167) echo "  ST167 = Clone NDM Ã©mergent" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                        *) echo "  ST$MLST_ST = Voir PubMLST pour plus d'informations" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt" ;;
                    esac
                    ;;
                *)
                    echo "  ST$MLST_ST = Consulter PubMLST (https://pubmlst.org)" >> "$RESULTS_DIR/03_annotation/mlst/${SAMPLE_ID}_mlst_summary.txt"
                    ;;
            esac

        else
            log_warn "Aucun rÃ©sultat MLST gÃ©nÃ©rÃ© (schÃ©ma non reconnu ou donnÃ©es insuffisantes)"
            MLST_ST="-"
            MLST_SCHEME="-"
        fi
    else
        log_error "Fichier d'entrÃ©e MLST non trouvÃ©: $MLST_INPUT"
        MLST_ST="-"
        MLST_SCHEME="-"
    fi
else
    log_warn "mlst non installÃ© - typage ignorÃ©"
    log_info "  Pour installer: conda install -c bioconda mlst"
    MLST_ST="-"
    MLST_SCHEME="-"
fi

log_success "MODULE 3.3 TERMINÃ‰"

#===============================================================================
# MODULE 3.5 : DÃ‰TECTION ARG SUR READS BRUTS (HAUTE SENSIBILITÃ‰)
#===============================================================================

# Cette Ã©tape dÃ©tecte les ARG directement sur les reads bruts pour capturer
# les gÃ¨nes Ã  faible couverture qui pourraient Ãªtre perdus lors de l'assemblage

if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_warn "MODULE 3.5 : DÃ‰TECTION ARG SUR READS - IGNORÃ‰ (entrÃ©e FASTA)"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
else
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_info "MODULE 3.5 : DÃ‰TECTION ARG SUR READS BRUTS (HAUTE SENSIBILITÃ‰)"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    mkdir -p "$RESULTS_DIR/04_arg_detection/reads_based"

    conda activate arg_detection

    #------- 3.5.1 DÃ©tection ARG sur reads avec KMA (si disponible) -------
    if command -v kma > /dev/null 2>&1; then
        log_info "3.5.1 DÃ©tection ARG sur reads avec KMA..."

        # VÃ©rifier/crÃ©er les bases KMA
        KMA_DB_DIR="$DB_DIR/kma_db"

        # Si la base n'existe pas, la crÃ©er automatiquement
        if [[ ! -f "$KMA_DB_DIR/resfinder.name" ]]; then
            log_info "  Base KMA non trouvÃ©e, crÃ©ation automatique..."
            setup_kma_database
        fi

        if [[ -f "$KMA_DB_DIR/resfinder.name" ]]; then
            log_info "  Base KMA prÃªte: $KMA_DB_DIR"

            if [[ "$IS_SINGLE_END" == true ]]; then
                kma -i "$CLEAN_R1" \
                    -o "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_kma" \
                    -t_db "$KMA_DB_DIR/resfinder" \
                    -t "$THREADS" \
                    -1t1 \
                    -mem_mode \
                    -and 2>&1 | tee -a "$LOG_FILE"
            else
                kma -ipe "$CLEAN_R1" "$CLEAN_R2" \
                    -o "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_kma" \
                    -t_db "$KMA_DB_DIR/resfinder" \
                    -t "$THREADS" \
                    -1t1 \
                    -mem_mode \
                    -and 2>&1 | tee -a "$LOG_FILE"
            fi

            if [[ -f "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_kma.res" ]]; then
                log_success "DÃ©tection KMA terminÃ©e"
                log_info "RÃ©sultats KMA:"
                head -20 "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_kma.res" 2>&1 | tee -a "$LOG_FILE"
            fi
        else
            log_warn "  Impossible de crÃ©er la base KMA (bases abricate manquantes?)"
            log_warn "  ExÃ©cutez d'abord: abricate --setupdb"
        fi
    else
        log_info "KMA non disponible, Ã©tape ignorÃ©e"
        log_info "  Pour l'installer: conda install -c bioconda kma"
    fi

    #------- 3.5.2 Mapping BLAST des reads contre bases ARG -------
    log_info "3.5.2 Recherche BLAST des reads contre bases ARG..."

    # CrÃ©er un Ã©chantillon de reads pour BLAST rapide
    SAMPLE_SIZE=50000
    log_info "  Ã‰chantillonnage de $SAMPLE_SIZE reads pour analyse BLAST..."

    READS_SAMPLE="$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_sample.fasta"

    # Note: On dÃ©sactive temporairement pipefail car zcat + head cause SIGPIPE
    set +o pipefail
    if [[ "$IS_SINGLE_END" == true ]]; then
        if [[ "$CLEAN_R1" == *.gz ]]; then
            zcat "$CLEAN_R1" 2>/dev/null | head -$((SAMPLE_SIZE * 4)) | \
                awk 'NR%4==1{print ">"substr($0,2)} NR%4==2{print}' > "$READS_SAMPLE"
        else
            head -$((SAMPLE_SIZE * 4)) "$CLEAN_R1" | \
                awk 'NR%4==1{print ">"substr($0,2)} NR%4==2{print}' > "$READS_SAMPLE"
        fi
    else
        if [[ "$CLEAN_R1" == *.gz ]]; then
            zcat "$CLEAN_R1" 2>/dev/null | head -$((SAMPLE_SIZE * 4)) | \
                awk 'NR%4==1{print ">"substr($0,2)} NR%4==2{print}' > "$READS_SAMPLE"
        else
            head -$((SAMPLE_SIZE * 4)) "$CLEAN_R1" | \
                awk 'NR%4==1{print ">"substr($0,2)} NR%4==2{print}' > "$READS_SAMPLE"
        fi
    fi
    set -o pipefail

    READS_COUNT=$(grep -c "^>" "$READS_SAMPLE" 2>/dev/null || echo "0")
    log_info "  Reads Ã©chantillonnÃ©s: $READS_COUNT"

    # BLAST contre les sÃ©quences ARG connues (utiliser la base abricate)
    # RÃ©cupÃ©rer le chemin des bases abricate (mÃªme mÃ©thode que setup_kma_database)
    ABRICATE_DB_PATH=$(abricate --help 2>&1 | grep -oP '\-\-datadir.*\[\K[^\]]+' | head -1)
    if [[ -z "$ABRICATE_DB_PATH" ]] || [[ ! -d "$ABRICATE_DB_PATH" ]]; then
        # Fallback sur chemins portables
        for path in "$HOME/abricate/db" "${CONDA_PREFIX:-}/share/abricate/db" "/usr/local/share/abricate/db"; do
            if [[ -d "$path" ]]; then
                ABRICATE_DB_PATH="$path"
                break
            fi
        done
    fi

    if [[ -n "$ABRICATE_DB_PATH" ]] && [[ -d "$ABRICATE_DB_PATH/resfinder" ]]; then
        log_info "  BLAST contre ResFinder database..."

        # CrÃ©er une base BLAST temporaire
        RESFINDER_SEQS="$ABRICATE_DB_PATH/resfinder/sequences"

        if [[ -f "$RESFINDER_SEQS" ]]; then
            makeblastdb -in "$RESFINDER_SEQS" -dbtype nucl -out /tmp/resfinder_blast_db 2>/dev/null

            blastn -query "$READS_SAMPLE" \
                -db /tmp/resfinder_blast_db \
                -outfmt "6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore stitle" \
                -max_target_seqs 1 \
                -evalue 1e-10 \
                -num_threads "$THREADS" \
                -out "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_blast.tsv" 2>&1 | tee -a "$LOG_FILE"

            # RÃ©sumer les hits
            if [[ -f "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_blast.tsv" ]]; then
                BLAST_HITS=$(wc -l < "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_blast.tsv")
                log_info "  Hits BLAST trouvÃ©s: $BLAST_HITS"

                if [[ $BLAST_HITS -gt 0 ]]; then
                    log_info "  GÃ¨nes ARG dÃ©tectÃ©s dans les reads (par frÃ©quence):"
                    cut -f2 "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_blast.tsv" | \
                        sort | uniq -c | sort -rn | head -10 | \
                        while read count gene; do
                            log_info "    $gene: $count reads"
                        done

                    # CrÃ©er un rÃ©sumÃ©
                    echo "# RÃ©sumÃ© dÃ©tection ARG sur reads - $SAMPLE_ID" > "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_summary.tsv"
                    echo "# Date: $(date)" >> "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_summary.tsv"
                    echo "# Reads analysÃ©s: $READS_COUNT" >> "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_summary.tsv"
                    echo "#" >> "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_summary.tsv"
                    echo "Gene	Read_Count	Estimated_Coverage" >> "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_summary.tsv"

                    cut -f2 "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_blast.tsv" | \
                        sort | uniq -c | sort -rn | \
                        awk -v total="$READS_COUNT" '{
                            gene=$2;
                            count=$1;
                            est_cov=(count * 150 / 1000);
                            printf "%s\t%d\t%.1fx\n", gene, count, est_cov
                        }' >> "$RESULTS_DIR/04_arg_detection/reads_based/${SAMPLE_ID}_reads_summary.tsv"

                    log_success "RÃ©sumÃ© sauvegardÃ©: ${SAMPLE_ID}_reads_summary.tsv"
                fi
            fi

            # Nettoyage
            rm -f /tmp/resfinder_blast_db.* 2>/dev/null
        else
            log_warn "  SÃ©quences ResFinder non trouvÃ©es"
        fi
    else
        log_warn "  Base abricate non trouvÃ©e pour BLAST"
    fi

    # Nettoyage
    rm -f "$READS_SAMPLE" 2>/dev/null

    conda deactivate

    log_success "MODULE 3.5 TERMINÃ‰"
fi

#===============================================================================
# MODULE 4 : DÃ‰TECTION DES GÃˆNES DE RÃ‰SISTANCE AUX ANTIBIOTIQUES (ARG)
#===============================================================================

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "MODULE 4 : DÃ‰TECTION DES GÃˆNES ARG"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

conda activate arg_detection

#------- 4.1 AMRFinderPlus -------
log_info "4.1 AMRFinderPlus (v4.2) avec virulence et stress..."

if [[ -n "$AMRFINDER_DB" ]]; then
    mkdir -p "$RESULTS_DIR"/04_arg_detection/amrfinderplus

    # VÃ©rifier que la base de donnÃ©es existe (sans mise Ã  jour automatique)
    if [[ -d "$AMRFINDER_DB" ]] && [[ -n "$(ls -A "$AMRFINDER_DB" 2>/dev/null)" ]]; then
        log_info "  Base AMRFinder trouvÃ©e: $AMRFINDER_DB"
    else
        log_warn "  Base AMRFinder vide ou introuvable"
        log_warn "  Pour installer/mettre Ã  jour: amrfinder --force_update"
    fi

    # DÃ©tecter l'organisme Ã  partir de Kraken2 pour les dÃ©tections spÃ©cifiques
    # Organismes supportÃ©s par AMRFinder: Escherichia, Salmonella, Klebsiella, Staphylococcus_aureus, etc.
    AMRFINDER_ORGANISM=""
    if [[ -n "$DETECTED_SPECIES" ]]; then
        case "$DETECTED_SPECIES" in
            *"Escherichia"*|*"E. coli"*|*"E.coli"*)
                AMRFINDER_ORGANISM="Escherichia" ;;
            *"Salmonella"*)
                AMRFINDER_ORGANISM="Salmonella" ;;
            *"Klebsiella pneumoniae"*)
                AMRFINDER_ORGANISM="Klebsiella_pneumoniae" ;;
            *"Klebsiella oxytoca"*)
                AMRFINDER_ORGANISM="Klebsiella_oxytoca" ;;
            *"Staphylococcus aureus"*)
                AMRFINDER_ORGANISM="Staphylococcus_aureus" ;;
            *"Pseudomonas aeruginosa"*)
                AMRFINDER_ORGANISM="Pseudomonas_aeruginosa" ;;
            *"Acinetobacter baumannii"*)
                AMRFINDER_ORGANISM="Acinetobacter_baumannii" ;;
            *"Enterococcus faecalis"*)
                AMRFINDER_ORGANISM="Enterococcus_faecalis" ;;
            *"Enterococcus faecium"*)
                AMRFINDER_ORGANISM="Enterococcus_faecium" ;;
            *"Campylobacter"*)
                AMRFINDER_ORGANISM="Campylobacter" ;;
            *"Neisseria gonorrhoeae"*)
                AMRFINDER_ORGANISM="Neisseria_gonorrhoeae" ;;
            *"Neisseria meningitidis"*)
                AMRFINDER_ORGANISM="Neisseria_meningitidis" ;;
            *"Streptococcus pneumoniae"*)
                AMRFINDER_ORGANISM="Streptococcus_pneumoniae" ;;
            *"Streptococcus pyogenes"*)
                AMRFINDER_ORGANISM="Streptococcus_pyogenes" ;;
            *"Streptococcus agalactiae"*)
                AMRFINDER_ORGANISM="Streptococcus_agalactiae" ;;
            *"Vibrio cholerae"*)
                AMRFINDER_ORGANISM="Vibrio_cholerae" ;;
            *"Clostridioides difficile"*|*"Clostridium difficile"*)
                AMRFINDER_ORGANISM="Clostridioides_difficile" ;;
        esac
    fi

    # Construire la commande AMRFinder avec options avancÃ©es
    AMRFINDER_OPTS="--plus"  # Active virulence, stress, et autres gÃ¨nes

    if [[ -n "$AMRFINDER_ORGANISM" ]]; then
        AMRFINDER_OPTS+=" --organism $AMRFINDER_ORGANISM"
        log_info "  Organisme dÃ©tectÃ©: $AMRFINDER_ORGANISM (mutations spÃ©cifiques activÃ©es)"
    else
        log_info "  Organisme non reconnu - dÃ©tection gÃ©nÃ©rique"
    fi

    log_info "  ExÃ©cution d'AMRFinder avec --plus (AMR + virulence + stress)..."
    amrfinder \
        --nucleotide "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna \
        --output "$RESULTS_DIR"/04_arg_detection/amrfinderplus/"${SAMPLE_ID}"_amrfinderplus.tsv \
        --threads "$THREADS" \
        $AMRFINDER_OPTS 2>&1 | tee -a "$LOG_FILE"

    # Compter les rÃ©sultats par type
    if [[ -f "$RESULTS_DIR/04_arg_detection/amrfinderplus/${SAMPLE_ID}_amrfinderplus.tsv" ]]; then
        AMRF_TOTAL=$(tail -n +2 "$RESULTS_DIR/04_arg_detection/amrfinderplus/${SAMPLE_ID}_amrfinderplus.tsv" | wc -l)
        AMRF_VIR=$(grep -c "VIRULENCE" "$RESULTS_DIR/04_arg_detection/amrfinderplus/${SAMPLE_ID}_amrfinderplus.tsv" 2>/dev/null || echo "0")
        AMRF_STRESS=$(grep -c "STRESS" "$RESULTS_DIR/04_arg_detection/amrfinderplus/${SAMPLE_ID}_amrfinderplus.tsv" 2>/dev/null || echo "0")
        AMRF_AMR=$((AMRF_TOTAL - AMRF_VIR - AMRF_STRESS))
        log_success "AMRFinderPlus terminÃ©: $AMRF_TOTAL gÃ¨nes ($AMRF_AMR AMR, $AMRF_VIR virulence, $AMRF_STRESS stress)"
    else
        log_success "AMRFinderPlus terminÃ©"
    fi
else
    log_warn "AMRFinderPlus IGNORÃ‰ (base de donnÃ©es non configurÃ©e)"
    log_warn "  Pour configurer: dÃ©finir AMRFINDER_DB ou exÃ©cuter amrfinder --force_update"
fi

#------- 4.2 ABRicate ResFinder -------
log_info "4.2 ABRicate ResFinder..."

abricate \
    --db resfinder \
    "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna \
    > "$RESULTS_DIR"/04_arg_detection/resfinder/"${SAMPLE_ID}"_resfinder.tsv 2>&1 | tee -a "$LOG_FILE"

log_success "ResFinder terminÃ©"

#------- 4.3 ABRicate PlasmidFinder -------
log_info "4.3 ABRicate PlasmidFinder..."

abricate \
    --db plasmidfinder \
    "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna \
    > "$RESULTS_DIR"/04_arg_detection/plasmidfinder/"${SAMPLE_ID}"_plasmidfinder.tsv 2>&1 | tee -a "$LOG_FILE"

log_success "PlasmidFinder terminÃ©"

#------- 4.4 ABRicate CARD -------
log_info "4.4 ABRicate CARD..."

abricate \
    --db card \
    "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna \
    > "$RESULTS_DIR"/04_arg_detection/card/"${SAMPLE_ID}"_card.tsv 2>&1 | tee -a "$LOG_FILE"

log_success "CARD terminÃ©"

#------- 4.5 ABRicate NCBI -------
log_info "4.5 ABRicate NCBI..."

abricate \
    --db ncbi \
    "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna \
    > "$RESULTS_DIR"/04_arg_detection/ncbi/"${SAMPLE_ID}"_ncbi.tsv 2>&1 | tee -a "$LOG_FILE"

log_success "NCBI terminÃ©"

#------- 4.6 ABRicate VFDB (Virulence Factor Database) -------
log_info "4.6 ABRicate VFDB (facteurs de virulence)..."

mkdir -p "$RESULTS_DIR"/04_arg_detection/vfdb

abricate \
    --db vfdb \
    "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna \
    > "$RESULTS_DIR"/04_arg_detection/vfdb/"${SAMPLE_ID}"_vfdb.tsv 2>&1 | tee -a "$LOG_FILE"

# Compter les gÃ¨nes de virulence trouvÃ©s
if [[ -f "$RESULTS_DIR/04_arg_detection/vfdb/${SAMPLE_ID}_vfdb.tsv" ]]; then
    VFDB_COUNT=$(grep -v "^#" "$RESULTS_DIR/04_arg_detection/vfdb/${SAMPLE_ID}_vfdb.tsv" | tail -n +2 | wc -l)
    log_success "VFDB terminÃ©: $VFDB_COUNT facteurs de virulence dÃ©tectÃ©s"
else
    log_success "VFDB terminÃ©"
fi

#------- 4.7 RGI (Resistance Gene Identifier) avec CARD -------
log_info "4.7 RGI/CARD (dÃ©tection avancÃ©e avec modÃ¨les homologue/variant/overexpression)..."

mkdir -p "$RESULTS_DIR"/04_arg_detection/rgi

# VÃ©rifier si RGI est disponible
if command -v rgi &> /dev/null; then
    # DÃ©finir le chemin de la base CARD
    if [[ -z "${CARD_DB:-}" ]]; then
        CARD_DB=$(find_card_db)
    fi

    # Si toujours pas de base, proposer le tÃ©lÃ©chargement
    if [[ -z "$CARD_DB" ]] || [[ ! -f "$CARD_DB/card.json" ]]; then
        log_warn "  Base CARD non trouvÃ©e - tÃ©lÃ©chargement automatique..."
        mkdir -p "$DB_DIR/card_db"
        download_card_db "$DB_DIR/card_db"
        CARD_DB="$DB_DIR/card_db"
    fi

    # VÃ©rifier si la base CARD est valide
    if [[ -f "$CARD_DB/card.json" ]]; then
        # Obtenir la version depuis loaded_databases.json si disponible
        if [[ -f "$CARD_DB/loaded_databases.json" ]]; then
            RGI_DB_VERSION=$(grep -o '"data_version": "[^"]*"' "$CARD_DB/loaded_databases.json" | head -1 | cut -d'"' -f4)
        else
            RGI_DB_VERSION="inconnue"
        fi
        log_info "  Base CARD v$RGI_DB_VERSION dÃ©tectÃ©e: $CARD_DB"

        # ExÃ©cuter RGI main
        rgi main \
            --input_sequence "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna \
            --output_file "$RESULTS_DIR"/04_arg_detection/rgi/"${SAMPLE_ID}"_rgi \
            --local \
            --clean \
            -n "$THREADS" \
            --alignment_tool DIAMOND \
            --include_nudge 2>> "$LOG_FILE" || log_warn "  RGI a rencontrÃ© des avertissements"

        if [[ -f "$RESULTS_DIR/04_arg_detection/rgi/${SAMPLE_ID}_rgi.txt" ]]; then
            RGI_COUNT=$(tail -n +2 "$RESULTS_DIR/04_arg_detection/rgi/${SAMPLE_ID}_rgi.txt" | wc -l)
            log_success "RGI terminÃ© - $RGI_COUNT gÃ¨nes dÃ©tectÃ©s"

            # Extraire les gÃ¨nes intrinsÃ¨ques (efflux pumps, etc.)
            log_info "  Analyse des mÃ©canismes de rÃ©sistance..."
            grep -i "efflux\|overexpression\|intrinsic" "$RESULTS_DIR/04_arg_detection/rgi/${SAMPLE_ID}_rgi.txt" > "$RESULTS_DIR/04_arg_detection/rgi/${SAMPLE_ID}_intrinsic.txt" 2>/dev/null || true
            INTRINSIC_COUNT=$(wc -l < "$RESULTS_DIR/04_arg_detection/rgi/${SAMPLE_ID}_intrinsic.txt" 2>/dev/null || echo "0")
            log_info "  â†’ GÃ¨nes intrinsÃ¨ques/efflux: $INTRINSIC_COUNT"
        else
            log_warn "  Fichier de sortie RGI non gÃ©nÃ©rÃ©"
        fi
    else
        log_warn "  Base CARD non chargÃ©e - exÃ©cuter: rgi auto_load --clean --local"
    fi
else
    log_warn "RGI non installÃ© - pour installer: pip install rgi && rgi auto_load --clean --local"
fi

#------- 4.7 PointFinder (mutations chromosomiques) -------
log_info "4.7 PointFinder (mutations chromosomiques SNP)..."

mkdir -p "$RESULTS_DIR"/04_arg_detection/pointfinder

# DÃ©terminer l'espÃ¨ce pour PointFinder
POINTFINDER_SPECIES=""
# Utiliser ${VAR:-} pour Ã©viter unbound variable avec set -u
if [[ -n "${KRAKEN_SPECIES:-}" ]]; then
    # Mapper l'espÃ¨ce Kraken vers les espÃ¨ces PointFinder supportÃ©es
    case "$KRAKEN_SPECIES" in
        *"Escherichia coli"*|*"E. coli"*)
            POINTFINDER_SPECIES="escherichia_coli"
            ;;
        *"Salmonella"*)
            POINTFINDER_SPECIES="salmonella"
            ;;
        *"Staphylococcus aureus"*|*"S. aureus"*)
            POINTFINDER_SPECIES="staphylococcus_aureus"
            ;;
        *"Campylobacter"*)
            POINTFINDER_SPECIES="campylobacter"
            ;;
        *"Klebsiella"*)
            POINTFINDER_SPECIES="klebsiella"
            ;;
        *"Enterococcus faecalis"*)
            POINTFINDER_SPECIES="enterococcus_faecalis"
            ;;
        *"Enterococcus faecium"*)
            POINTFINDER_SPECIES="enterococcus_faecium"
            ;;
        *"Mycobacterium tuberculosis"*)
            POINTFINDER_SPECIES="mycobacterium_tuberculosis"
            ;;
        *"Neisseria gonorrhoeae"*)
            POINTFINDER_SPECIES="neisseria_gonorrhoeae"
            ;;
        *)
            log_info "  EspÃ¨ce '$KRAKEN_SPECIES' non supportÃ©e par PointFinder"
            ;;
    esac
fi

# DÃ©finir le chemin de la base PointFinder
if [[ -z "${POINTFINDER_DB:-}" ]]; then
    POINTFINDER_DB=$(find_pointfinder_db)
fi

# Si toujours pas de base, proposer le tÃ©lÃ©chargement
if [[ -z "$POINTFINDER_DB" ]] || [[ ! -f "$POINTFINDER_DB/config" ]]; then
    log_warn "  Base PointFinder non trouvÃ©e - tÃ©lÃ©chargement automatique..."
    download_pointfinder_db "$DB_DIR"
    POINTFINDER_DB="$DB_DIR/pointfinder_db"
fi

if [[ -n "$POINTFINDER_SPECIES" ]] && [[ -d "$POINTFINDER_DB/$POINTFINDER_SPECIES" ]]; then
    log_info "  Analyse PointFinder pour: $POINTFINDER_SPECIES"

    # ExÃ©cuter ResFinder avec PointFinder
    python3 -m resfinder \
        --inputfasta "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna \
        --outputPath "$RESULTS_DIR"/04_arg_detection/pointfinder \
        --species "$POINTFINDER_SPECIES" \
        --point \
        --db_path_point "$POINTFINDER_DB" \
        --ignore_missing_species 2>> "$LOG_FILE" || log_warn "  PointFinder a rencontrÃ© des avertissements"

    # VÃ©rifier les rÃ©sultats
    if [[ -f "$RESULTS_DIR/04_arg_detection/pointfinder/PointFinder_results.txt" ]]; then
        POINT_COUNT=$(grep -c "mutation" "$RESULTS_DIR/04_arg_detection/pointfinder/PointFinder_results.txt" 2>/dev/null || echo "0")
        log_success "PointFinder terminÃ© - $POINT_COUNT mutations dÃ©tectÃ©es"
    elif [[ -f "$RESULTS_DIR/04_arg_detection/pointfinder/pointfinder_results.txt" ]]; then
        POINT_COUNT=$(tail -n +2 "$RESULTS_DIR/04_arg_detection/pointfinder/pointfinder_results.txt" | wc -l)
        log_success "PointFinder terminÃ© - $POINT_COUNT mutations dÃ©tectÃ©es"
    else
        log_info "  Aucune mutation chromosomique dÃ©tectÃ©e"
    fi
else
    if [[ -z "$POINTFINDER_SPECIES" ]]; then
        log_info "  PointFinder ignorÃ© (espÃ¨ce non supportÃ©e)"
    else
        log_warn "  Base PointFinder non trouvÃ©e pour $POINTFINDER_SPECIES"
    fi
fi

#------- 4.8 SynthÃ¨se ARG -------
log_info "4.8 SynthÃ¨se des rÃ©sultats ARG..."

{
    echo "Sample ID: $SAMPLE_ID"
    echo "Date: $(date)"
    echo ""
    echo "=== AMRFinderPlus ==="
    wc -l < "$RESULTS_DIR"/04_arg_detection/amrfinderplus/"${SAMPLE_ID}"_amrfinderplus.tsv 2>/dev/null || echo "0"
    echo ""
    echo "=== ResFinder ==="
    wc -l < "$RESULTS_DIR"/04_arg_detection/resfinder/"${SAMPLE_ID}"_resfinder.tsv 2>/dev/null || echo "0"
    echo ""
    echo "=== PlasmidFinder ==="
    wc -l < "$RESULTS_DIR"/04_arg_detection/plasmidfinder/"${SAMPLE_ID}"_plasmidfinder.tsv 2>/dev/null || echo "0"
    echo ""
    echo "=== RGI/CARD ==="
    if [[ -f "$RESULTS_DIR/04_arg_detection/rgi/${SAMPLE_ID}_rgi.txt" ]]; then
        tail -n +2 "$RESULTS_DIR/04_arg_detection/rgi/${SAMPLE_ID}_rgi.txt" | wc -l
        echo "  dont gÃ¨nes intrinsÃ¨ques/efflux:"
        wc -l < "$RESULTS_DIR/04_arg_detection/rgi/${SAMPLE_ID}_intrinsic.txt" 2>/dev/null || echo "  0"
    else
        echo "Non exÃ©cutÃ©"
    fi
    echo ""
    echo "=== PointFinder (mutations SNP) ==="
    if [[ -d "$RESULTS_DIR/04_arg_detection/pointfinder" ]]; then
        find "$RESULTS_DIR/04_arg_detection/pointfinder" -name "*results*" -exec wc -l {} \; 2>/dev/null | head -1 || echo "Aucune mutation"
    else
        echo "Non exÃ©cutÃ© (espÃ¨ce non supportÃ©e)"
    fi
} > "$RESULTS_DIR"/04_arg_detection/synthesis/"${SAMPLE_ID}"_ARG_synthesis.tsv

log_success "SynthÃ¨se ARG terminÃ©e"

conda deactivate

log_success "MODULE 4 TERMINÃ‰"

#===============================================================================
# MODULE 5 : VARIANT CALLING - IGNORÃ‰ SI FASTA ASSEMBLÃ‰
#===============================================================================

if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_warn "MODULE 5 : VARIANT CALLING - IGNORÃ‰ (entrÃ©e FASTA assemblÃ©e)"
    log_info "  (Pas de reads disponibles pour le variant calling)"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
else
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    log_info "MODULE 5 : VARIANT CALLING"
    log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

    conda activate variant_arg

    #------- 5.1 PrÃ©paration du gÃ©nome de rÃ©fÃ©rence -------
    log_info "5.1 PrÃ©paration du gÃ©nome de rÃ©fÃ©rence..."

    SNIPPY_WORK="$RESULTS_DIR"/05_variant_calling/snippy

    mkdir -p "$SNIPPY_WORK"

    # TÃ©lÃ©charger ou rÃ©cupÃ©rer la rÃ©fÃ©rence appropriÃ©e pour l'espÃ¨ce dÃ©tectÃ©e
    log_info "Recherche de la rÃ©fÃ©rence pour l'espÃ¨ce dÃ©tectÃ©e..."
    if [[ -n "$PROKKA_GENUS" ]] && [[ "$PROKKA_GENUS" != "Bacteria" ]]; then
        log_info "  EspÃ¨ce dÃ©tectÃ©e: $PROKKA_GENUS $PROKKA_SPECIES"
        # || true pour Ã©viter l'arrÃªt du script si la rÃ©fÃ©rence n'est pas trouvÃ©e
        get_or_download_reference "$PROKKA_GENUS" "$PROKKA_SPECIES" || true
    else
        log_warn "  Aucune espÃ¨ce spÃ©cifique dÃ©tectÃ©e"
        # Essayer avec la rÃ©fÃ©rence par dÃ©faut
        if [[ -f "$REFERENCE_DIR/ecoli_k12.fasta" ]]; then
            REFERENCE_GENOME="$REFERENCE_DIR/ecoli_k12.fasta"
            log_info "  Utilisation de la rÃ©fÃ©rence par dÃ©faut: E. coli K-12"
        else
            REFERENCE_GENOME=""
        fi
    fi

    # Utiliser la rÃ©fÃ©rence trouvÃ©e/tÃ©lÃ©chargÃ©e ou fallback sur l'assemblage
    if [[ -n "$REFERENCE_GENOME" ]] && [[ -f "$REFERENCE_GENOME" ]]; then
        log_success "RÃ©fÃ©rence utilisÃ©e: $REFERENCE_GENOME"
        cp "$REFERENCE_GENOME" "$SNIPPY_WORK"/reference.fa
    else
        log_warn "Aucune rÃ©fÃ©rence disponible. Utilisation de l'assemblage comme rÃ©fÃ©rence."
        log_warn "  Note: Les variants seront relatifs Ã  l'assemblage lui-mÃªme"
        cp "$RESULTS_DIR"/03_annotation/prokka/"${SAMPLE_ID}".fna "$SNIPPY_WORK"/reference.fa
    fi

    log_success "RÃ©fÃ©rence prÃ©parÃ©e"

    #------- 5.2 Variant Calling avec Snippy -------
    log_info "5.2 Variant Calling avec Snippy..."

    if [[ "$IS_SINGLE_END" == true ]]; then
        # Mode single-end
        log_info "  Mode single-end dÃ©tectÃ©"
        snippy \
            --outdir "$SNIPPY_WORK" \
            --prefix "$SAMPLE_ID" \
            --reference "$SNIPPY_WORK"/reference.fa \
            --se "$CLEAN_R1" \
            --cpus "$THREADS" \
            --force 2>&1 | tee -a "$LOG_FILE"
    else
        # Mode paired-end
        snippy \
            --outdir "$SNIPPY_WORK" \
            --prefix "$SAMPLE_ID" \
            --reference "$SNIPPY_WORK"/reference.fa \
            --R1 "$CLEAN_R1" \
            --R2 "$CLEAN_R2" \
            --cpus "$THREADS" \
            --force 2>&1 | tee -a "$LOG_FILE"
    fi

    log_success "Variant Calling terminÃ©"

    #------- 5.3 Copie des rÃ©sultats -------
    log_info "5.3 Organisation des rÃ©sultats variants..."

    if [[ -f "$SNIPPY_WORK"/"${SAMPLE_ID}".vcf ]]; then
        cp "$SNIPPY_WORK"/"${SAMPLE_ID}".vcf "$RESULTS_DIR"/05_variant_calling/"${SAMPLE_ID}"_variants.vcf
        log_success "Fichier VCF copiÃ©"
    fi

    conda deactivate

    log_success "MODULE 5 TERMINÃ‰"
fi  # Fin du bloc conditionnel Module 5

#===============================================================================
# MODULE 6 : ANALYSE ET GÃ‰NÃ‰RATION DE RAPPORTS
#===============================================================================

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "MODULE 6 : ANALYSE ET GÃ‰NÃ‰RATION DE RAPPORTS"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

conda activate analysis_arg

#------- 6.1 GÃ©nÃ©ration des mÃ©tadonnÃ©es -------
log_info "6.1 GÃ©nÃ©ration des mÃ©tadonnÃ©es..."

# Utiliser SCRIPT_DIR dÃ©jÃ  dÃ©fini au dÃ©but du script
METADATA_SCRIPT="$PYTHON_DIR/generate_metadata.py"

if [[ -f "$METADATA_SCRIPT" ]]; then
    # Passer l'espÃ¨ce dÃ©tectÃ©e si disponible
    if [[ -n "$DETECTED_SPECIES" ]]; then
        export KRAKEN_DETECTED_SPECIES="$DETECTED_SPECIES"
    fi
    
    python3 "$METADATA_SCRIPT" "$RESULTS_DIR" "$SAMPLE_ID" "$INPUT_TYPE" "$INPUT_ARG" "$THREADS" 2>&1 | tee -a "$LOG_FILE"
    log_success "MÃ©tadonnÃ©es gÃ©nÃ©rÃ©es: $RESULTS_DIR/METADATA.json"
else
    log_warn "Script de gÃ©nÃ©ration de mÃ©tadonnÃ©es non trouvÃ©: $METADATA_SCRIPT"
fi

#------- 6.2 GÃ©nÃ©ration des rapports -------
log_info "6.2 GÃ©nÃ©ration des rapports..."

{
    echo "================================================================================"
    echo "RAPPORT D'ANALYSE PIPELINE ARG v3.2"
    echo "================================================================================"
    echo ""
    echo "Ã‰chantillon: $SAMPLE_ID"
    echo "Type d'entrÃ©e: $INPUT_TYPE"
    echo "Version: $RESULTS_VERSION"
    echo "Date: $(date)"
    echo "RÃ©pertoire de rÃ©sultats: $RESULTS_DIR"
    echo ""
    echo "================================================================================"
    echo "RÃ‰SUMÃ‰ DES RÃ‰SULTATS"
    echo "================================================================================"
    echo ""
    if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
        echo "1. CONTRÃ”LE QUALITÃ‰"
        echo "   - IGNORÃ‰ (entrÃ©e FASTA assemblÃ©e)"
        echo ""
        echo "2. ASSEMBLAGE"
        echo "   - IGNORÃ‰ (entrÃ©e FASTA assemblÃ©e)"
    else
        echo "1. CONTRÃ”LE QUALITÃ‰"
        echo "   - FastQC: ComplÃ©tÃ©"
        echo "   - Fastp: ComplÃ©tÃ©"
        echo "   - Kraken2: ComplÃ©tÃ© (si disponible)"
        echo ""
        echo "2. ASSEMBLAGE"
        echo "   - SPAdes: ComplÃ©tÃ© (mode isolate)"
        echo "   - QUAST: ComplÃ©tÃ©"
    fi
    echo ""
    echo "3. ANNOTATION"
    echo "   - Prokka: ComplÃ©tÃ©"
    echo ""
    echo "4. DÃ‰TECTION ARG"
    echo "   - AMRFinderPlus: ComplÃ©tÃ©"
    echo "   - ResFinder: ComplÃ©tÃ©"
    echo "   - PlasmidFinder: ComplÃ©tÃ©"
    echo "   - CARD: ComplÃ©tÃ©"
    echo "   - NCBI: ComplÃ©tÃ©"
    echo ""
    if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
        echo "5. VARIANT CALLING"
        echo "   - IGNORÃ‰ (entrÃ©e FASTA assemblÃ©e)"
    else
        echo "5. VARIANT CALLING"
        echo "   - Snippy: ComplÃ©tÃ©"
    fi
    echo ""
    echo "6. ANALYSE ET RAPPORTS"
    echo "   - Rapport texte: ComplÃ©tÃ©"
    echo "   - Rapport HTML professionnel: ComplÃ©tÃ©"
    echo "   - Extraction features ML: ComplÃ©tÃ©"
    echo ""
    echo "================================================================================"
    echo "FICHIERS PRINCIPAUX GÃ‰NÃ‰RÃ‰S"
    echo "================================================================================"
    echo ""
    if [[ "$IS_ASSEMBLED_INPUT" == false ]]; then
        ls -1 "$RESULTS_DIR"/01_qc/fastqc_raw/*.html 2>/dev/null | head -2 | sed 's|^|  - |' || true
    fi
    ls -1 "$RESULTS_DIR"/02_assembly/filtered/*.fasta 2>/dev/null | head -1 | sed 's|^|  - |' || true
    ls -1 "$RESULTS_DIR"/03_annotation/prokka/*.gff 2>/dev/null | head -1 | sed 's|^|  - |' || true
    ls -1 "$RESULTS_DIR"/04_arg_detection/*/*.tsv 2>/dev/null | head -3 | sed 's|^|  - |' || true
    if [[ "$IS_ASSEMBLED_INPUT" == false ]]; then
        ls -1 "$RESULTS_DIR"/05_variant_calling/*_variants.vcf 2>/dev/null | head -1 | sed 's|^|  - |' || true
    fi
    echo "  - 06_analysis/features_ml.csv (ML features)"
} > "$RESULTS_DIR"/06_analysis/reports/"${SAMPLE_ID}"_summary.txt

log_success "Rapport texte gÃ©nÃ©rÃ©"

open_file_safe "$RESULTS_DIR/06_analysis/reports/${SAMPLE_ID}_summary.txt" "Pipeline Summary Report"

#------- 6.2 GÃ©nÃ©ration du rapport ARG professionnel -------
log_info "6.2 GÃ©nÃ©ration du rapport ARG professionnel..."

ARG_REPORT_SCRIPT="$PYTHON_DIR/generate_arg_report.py"

# Utiliser DETECTED_SPECIES dÃ©jÃ  extraite par extract_species_from_kraken2()
# Si elle n'a pas Ã©tÃ© dÃ©finie, essayer de l'extraire maintenant
if [[ -z "$DETECTED_SPECIES" ]]; then
    KRAKEN_REPORT="$RESULTS_DIR/01_qc/kraken2/${SAMPLE_ID}_kraken2.report"
    if [[ -f "$KRAKEN_REPORT" ]]; then
        extract_species_from_kraken2 "$KRAKEN_REPORT" || true
    fi
fi

if [[ -f "$ARG_REPORT_SCRIPT" ]]; then
    # Passer l'espÃ¨ce dÃ©tectÃ©e au script Python via variable d'environnement
    if [[ -n "$DETECTED_SPECIES" ]]; then
        export KRAKEN_DETECTED_SPECIES="$DETECTED_SPECIES"
        log_info "EspÃ¨ce passÃ©e au script de rapport: $DETECTED_SPECIES"
    else
        log_info "Aucune espÃ¨ce dÃ©tectÃ©e par Kraken2 (ou rapport non disponible)"
    fi

    # Passer les rÃ©sultats MLST au script Python
    if [[ -n "$MLST_ST" ]] && [[ "$MLST_ST" != "-" ]]; then
        export MLST_SCHEME="$MLST_SCHEME"
        export MLST_ST="$MLST_ST"
        export MLST_ALLELES="$MLST_ALLELES"
        log_info "MLST passÃ© au script de rapport: $MLST_SCHEME / ST$MLST_ST"
    fi
    
    log_info "ExÃ©cution du script de gÃ©nÃ©ration de rapport HTML..."
    if python3 "$ARG_REPORT_SCRIPT" "$RESULTS_DIR" "$SAMPLE_ID" 2>&1 | tee -a "$LOG_FILE"; then
        if [[ -f "$RESULTS_DIR/06_analysis/reports/${SAMPLE_ID}_ARG_professional_report.html" ]]; then
            log_success "Rapport ARG professionnel gÃ©nÃ©rÃ©"
            open_file_safe "$RESULTS_DIR/06_analysis/reports/${SAMPLE_ID}_ARG_professional_report.html" "ARG Professional Report"
        else
            log_warn "Rapport ARG professionnel: Le fichier HTML n'a pas Ã©tÃ© crÃ©Ã©"
            log_warn "VÃ©rifiez les erreurs ci-dessus dans le journal"
        fi
    else
        log_error "Erreur lors de l'exÃ©cution du script de gÃ©nÃ©ration de rapport"
        log_error "VÃ©rifiez que Python3 et les dÃ©pendances sont installÃ©es"
    fi
else
    log_warn "Script de rapport ARG non trouvÃ©: $ARG_REPORT_SCRIPT"
    log_warn "Le rapport HTML ne sera pas gÃ©nÃ©rÃ©"
fi

#------- 6.4 Extraction des features pour Machine Learning -------
log_info "6.4 Extraction des features pour Machine Learning..."

FEATURES_SCRIPT="$PYTHON_DIR/collect_features.py"

# RÃ©pertoire pour le dataset global ML (accumulation multi-Ã©chantillons)
ML_DATASET_DIR="$SCRIPT_DIR/ml_datasets"
mkdir -p "$ML_DATASET_DIR"
GLOBAL_ML_DATASET="$ML_DATASET_DIR/global_features_dataset.csv"

if [[ -f "$FEATURES_SCRIPT" ]]; then
    # PrÃ©parer les paramÃ¨tres
    SPECIES_PARAM="${DETECTED_SPECIES:-unknown}"
    MLST_PARAM="${MLST_ST:--}"

    log_info "Extraction des features ML pour: $SAMPLE_ID"
    log_info "  EspÃ¨ce: $SPECIES_PARAM"
    log_info "  MLST ST: $MLST_PARAM"

    # ExÃ©cuter l'extraction
    if python3 "$FEATURES_SCRIPT" \
        --results-dir "$RESULTS_DIR" \
        --sample-id "$SAMPLE_ID" \
        --species "$SPECIES_PARAM" \
        --mlst-st "$MLST_PARAM" \
        --output "$RESULTS_DIR/06_analysis/features_ml.csv" \
        --global-dataset "$GLOBAL_ML_DATASET" 2>&1 | tee -a "$LOG_FILE"; then

        if [[ -f "$RESULTS_DIR/06_analysis/features_ml.csv" ]]; then
            log_success "Features ML extraites: $RESULTS_DIR/06_analysis/features_ml.csv"
            log_info "Dataset global mis Ã  jour: $GLOBAL_ML_DATASET"

            # Afficher un rÃ©sumÃ© rapide
            if command -v head &> /dev/null; then
                FEATURE_COUNT=$(head -1 "$RESULTS_DIR/06_analysis/features_ml.csv" | tr ',' '\n' | wc -l)
                log_info "Nombre de features extraites: $FEATURE_COUNT"
            fi
        else
            log_warn "Features ML: Le fichier CSV n'a pas Ã©tÃ© crÃ©Ã©"
        fi
    else
        log_error "Erreur lors de l'extraction des features ML"
    fi
else
    log_warn "Script d'extraction ML non trouvÃ©: $FEATURES_SCRIPT"
    log_warn "Les features ML ne seront pas extraites"
fi

conda deactivate

log_success "MODULE 6 TERMINÃ‰"

#===============================================================================
# RÃ‰SUMÃ‰ FINAL
#===============================================================================

log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_info "PIPELINE ARG v3.2 - EXÃ‰CUTION COMPLÃˆTE"
log_info "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

log_success "TOUS LES MODULES COMPLÃ‰TÃ‰S AVEC SUCCÃˆS"

log_info ""
log_info "Configuration utilisÃ©e:"
log_info "   Ã‰chantillon: $SAMPLE_ID"
log_info "   Type d'entrÃ©e: $INPUT_TYPE"
if [[ "$IS_ASSEMBLED_INPUT" == true ]]; then
    log_info "   Modules exÃ©cutÃ©s: Annotation, DÃ©tection ARG, Analyse"
    log_info "   Modules ignorÃ©s: QC, Assemblage, Variant Calling"
else
    log_info "   Modules exÃ©cutÃ©s: QC, Assemblage, Annotation, DÃ©tection ARG, Variant Calling, Analyse"
fi
log_info ""
log_info "Fichiers de rÃ©sultats disponibles dans:"
log_info "   $RESULTS_DIR"
log_info ""
log_info "Logs disponibles dans:"
log_info "   $LOG_DIR"
log_info ""
log_info "Fichier principal de log:"
log_info "   $LOG_FILE"
log_info ""
log_info "Archives stockÃ©es dans:"
log_info "   $ARCHIVE_DIR"
log_info ""

# Afficher le rÃ©sumÃ© des fichiers gÃ©nÃ©rÃ©s
log_info "Fichiers principaux gÃ©nÃ©rÃ©s:"
find "$RESULTS_DIR" -type f \( -name "*.html" -o -name "*_report.*" -o -name "*_summary.*" \) 2>/dev/null | while read f; do
    log_info "  âœ“ $(basename "$f")"
done

log_success "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
log_success "Pipeline ARG v3.2 - TERMINÃ‰ AVEC SUCCÃˆS!"
log_success "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

