"""
API FastAPI pour le Pipeline ARG
"""
from fastapi import FastAPI, HTTPException, UploadFile, File, status
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Optional
import os
import shutil
import subprocess
import threading
import time
import re as re_module

from models import (
    LaunchAnalysisRequest,
    JobResponse,
    JobStatusResponse,
    JobListResponse,
    JobListItem,
    AnalysisResults,
    DeduplicatedGene,
    DeduplicationStats,
    ErrorResponse,
    JobStatus,
    InputType
)
from database import db
from pipeline_launcher import PipelineLauncher
from output_parser import OutputParser

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration paths
BACKEND_DIR = Path(__file__).parent
PROJECT_ROOT = BACKEND_DIR.parent
PIPELINE_DIR = PROJECT_ROOT / "pipeline"
# Using WEB version without interactive prompts and xdg-open
PIPELINE_SCRIPT = PIPELINE_DIR / "MANUAL_MEGA_MONOLITHIC_PIPELINE_v3.2_WEB.sh"

# VÃ©rification que le pipeline existe
if not PIPELINE_SCRIPT.exists():
    logger.error(f"ERREUR CRITIQUE: Pipeline script non trouvÃ©: {PIPELINE_SCRIPT}")
    raise FileNotFoundError(f"Pipeline script manquant: {PIPELINE_SCRIPT}")

# Initialiser le launcher
launcher = PipelineLauncher(
    pipeline_script=str(PIPELINE_SCRIPT),
    work_dir=str(PIPELINE_DIR)
)


# Lifespan context manager pour startup/shutdown
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestion du cycle de vie de l'application"""
    # Startup
    logger.info("=" * 60)
    logger.info("ðŸš€ DÃ©marrage API Pipeline ARG v3.2")
    logger.info("=" * 60)
    logger.info(f"Pipeline script: {PIPELINE_SCRIPT}")
    logger.info(f"Work directory: {PIPELINE_DIR}")
    logger.info(f"Database: {BACKEND_DIR / 'jobs.db'}")

    # Initialiser la base de donnÃ©es
    await db.initialize()
    logger.info("âœ… Base de donnÃ©es initialisÃ©e")

    # Nettoyer les jobs zombies (optionnel)
    await db.cleanup_stale_jobs(max_age_hours=24)
    logger.info("âœ… Nettoyage jobs zombies effectuÃ©")

    logger.info("âœ… API prÃªte Ã  recevoir des requÃªtes")

    yield

    # Shutdown
    logger.info("ðŸ›‘ ArrÃªt de l'API")


# CrÃ©er l'application FastAPI
app = FastAPI(
    title="Pipeline ARG API",
    description="API pour lancer et monitorer le pipeline de dÃ©tection de gÃ¨nes de rÃ©sistance antimicrobienne",
    version="1.0.0",
    lifespan=lifespan
)

# Configuration CORS
ALLOWED_ORIGINS = os.environ.get(
    "CORS_ORIGINS",
    "http://localhost:8080,http://localhost:8000"
).split(",")

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "DELETE"],
    allow_headers=["Content-Type", "Cache-Control", "Pragma", "Expires"],
)


# ============================================================================
# ROUTES API
# ============================================================================

@app.get("/")
async def root():
    """Endpoint racine - informations API"""
    return {
        "name": "Pipeline ARG API",
        "version": "1.0.0",
        "status": "running",
        "pipeline_version": "3.2",
        "endpoints": {
            "launch": "POST /api/launch",
            "status": "GET /api/status/{job_id}",
            "results": "GET /api/results/{job_id}",
            "jobs": "GET /api/jobs",
            "health": "GET /health"
        }
    }


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "pipeline_script_exists": PIPELINE_SCRIPT.exists()
    }


# RÃ©pertoire pour les fichiers uploadÃ©s
UPLOAD_DIR = PIPELINE_DIR / "data" / "uploads"
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

ALLOWED_EXTENSIONS = {'.fasta', '.fa', '.fna', '.fastq', '.fq', '.fasta.gz', '.fastq.gz', '.fa.gz', '.fq.gz'}

@app.post("/api/upload")
async def upload_file(file: UploadFile = File(...)):
    """
    Upload un fichier FASTA/FASTQ pour analyse

    Returns:
        Le chemin du fichier sur le serveur Ã  utiliser comme sample_id
    """
    if not file.filename:
        raise HTTPException(status_code=400, detail="Nom de fichier manquant")

    # Valider l'extension
    filename = file.filename.lower()
    valid = False
    for ext in ALLOWED_EXTENSIONS:
        if filename.endswith(ext):
            valid = True
            break
    if not valid:
        raise HTTPException(
            status_code=400,
            detail="Format non supportÃ©. Extensions acceptÃ©es : .fasta, .fa, .fna, .fastq, .fq (et .gz)"
        )

    # Nettoyer le nom de fichier (sÃ©curitÃ©)
    safe_filename = re_module.sub(r'[^a-zA-Z0-9._-]', '_', file.filename)
    dest_path = UPLOAD_DIR / safe_filename

    # Ã‰crire le fichier
    try:
        with open(dest_path, "wb") as f:
            content = await file.read()
            if len(content) == 0:
                raise HTTPException(status_code=400, detail="Fichier vide")
            f.write(content)

        logger.info(f"Fichier uploadÃ© : {dest_path} ({len(content)} bytes)")

        return {
            "filename": safe_filename,
            "path": str(dest_path),
            "size": len(content)
        }
    except HTTPException:
        raise
    except OSError:
        raise HTTPException(status_code=500, detail="Erreur lors de l'enregistrement du fichier")


@app.post("/api/launch", response_model=JobResponse, status_code=status.HTTP_201_CREATED)
async def launch_analysis(request: LaunchAnalysisRequest):
    """
    Lance une nouvelle analyse ARG

    Args:
        request: ParamÃ¨tres de l'analyse

    Returns:
        JobResponse avec job_id et statut initial

    Raises:
        HTTPException 400: Si les paramÃ¨tres sont invalides
        HTTPException 500: Si erreur lors du lancement
    """
    try:
        logger.info(f"ðŸ“¥ Nouvelle requÃªte d'analyse: {request.sample_id}")

        # CrÃ©er le job dans la base de donnÃ©es
        job_id = await db.create_job(
            sample_id=request.sample_id,
            threads=request.threads,
            prokka_mode=request.prokka_mode.value,
            prokka_genus=request.prokka_genus,
            prokka_species=request.prokka_species
        )

        logger.info(f"âœ… Job crÃ©Ã©: {job_id}")

        # DÃ©finir callback de complÃ©tion
        async def on_complete(exit_code: int, stdout: str, stderr: str):
            """Callback appelÃ© quand le pipeline se termine"""
            if exit_code == 0:
                await db.update_job_status(
                    job_id=job_id,
                    status=JobStatus.COMPLETED,
                    completed_at=datetime.now(),
                    exit_code=exit_code
                )
                logger.info(f"âœ… Job {job_id} terminÃ© avec succÃ¨s")
            else:
                # Extraire message d'erreur du stderr et des logs
                error_msg = "Erreur inconnue"

                # Essayer de rÃ©cupÃ©rer les derniÃ¨res lignes du log pipeline
                try:
                    job_data = await db.get_job(job_id)
                    if job_data and job_data.get('run_number'):
                        log_tail = await launcher.get_log_tail(
                            sample_id=request.sample_id,
                            run_number=job_data['run_number'],
                            lines=50
                        )
                        if log_tail:
                            # Chercher les lignes d'erreur (ERROR, FAILED, Exception)
                            error_lines = [
                                line for line in log_tail.split('\n')
                                if any(x in line for x in ['[ERROR]', 'FAILED', 'Exception', 'Error'])
                            ]
                            if error_lines:
                                error_msg = '\n'.join(error_lines[-3:])  # 3 derniÃ¨res erreurs
                            else:
                                error_msg = log_tail[-500:]  # Sinon, derniers 500 chars
                except Exception as e:
                    logger.warning(f"Impossible de lire logs pour erreur: {e}")

                # Fallback sur stderr si pas de log
                if error_msg == "Erreur inconnue" and stderr:
                    error_msg = stderr[-500:]

                await db.update_job_status(
                    job_id=job_id,
                    status=JobStatus.FAILED,
                    completed_at=datetime.now(),
                    exit_code=exit_code,
                    error_message=error_msg
                )
                logger.error(f"âŒ Job {job_id} Ã©chouÃ© (exit code: {exit_code}): {error_msg[:100]}")

        # Lancer le pipeline
        launch_result = await launcher.launch(
            sample_id=request.sample_id,
            threads=request.threads,
            prokka_mode=request.prokka_mode.value,
            prokka_genus=request.prokka_genus,
            prokka_species=request.prokka_species,
            force=request.force,
            on_complete=on_complete
        )

        # Mettre Ã  jour le job avec les infos du lancement
        await db.update_job_status(
            job_id=job_id,
            status=JobStatus.RUNNING,
            started_at=datetime.now(),
            pid=launch_result['pid'],
            input_type=launch_result['input_type'],
            run_number=launch_result['run_number'],
            output_dir=launch_result['output_dir']
        )

        logger.info(f"ðŸš€ Pipeline lancÃ© (PID: {launch_result['pid']}, Run: {launch_result['run_number']})")

        # Retourner la rÃ©ponse
        job = await db.get_job(job_id)
        return JobResponse(
            job_id=job_id,
            sample_id=request.sample_id,
            status=JobStatus.RUNNING,
            created_at=job['created_at'],
            message=f"Analyse lancÃ©e avec succÃ¨s (Run #{launch_result['run_number']})"
        )

    except Exception as e:
        logger.error(f"âŒ Erreur lancement analyse: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur lors du lancement de l'analyse"
        )


@app.get("/api/status/{job_id}", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """
    RÃ©cupÃ¨re le statut d'un job

    Args:
        job_id: ID du job

    Returns:
        JobStatusResponse avec statut actuel, progression, logs, etc.

    Raises:
        HTTPException 404: Si job non trouvÃ©
    """
    try:
        # RÃ©cupÃ©rer le job
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} non trouvÃ©"
            )

        # Estimer la progression selon le statut
        progress = None
        current_step = None
        logs_preview = None

        if job['status'] == JobStatus.COMPLETED.value:
            # Job terminÃ© = 100%
            progress = 100
            current_step = "Analyse terminÃ©e avec succÃ¨s"

            # RÃ©cupÃ©rer les derniers logs mÃªme si complÃ©tÃ©
            if job['run_number']:
                logs_preview = await launcher.get_log_tail(
                    sample_id=job['sample_id'],
                    run_number=job['run_number'],
                    lines=30
                )

        elif job['status'] == JobStatus.FAILED.value:
            # Job Ã©chouÃ© = estimer oÃ¹ il s'est arrÃªtÃ©
            if job['input_type'] and job['run_number']:
                progress = await launcher.estimate_progress(
                    sample_id=job['sample_id'],
                    run_number=job['run_number'],
                    input_type=InputType(job['input_type'])
                )
            else:
                progress = 0

            current_step = f"Ã‰chec: {job['error_message'][:80] if job['error_message'] else 'Erreur inconnue'}"

            # RÃ©cupÃ©rer les logs pour voir l'erreur
            if job['run_number']:
                logs_preview = await launcher.get_log_tail(
                    sample_id=job['sample_id'],
                    run_number=job['run_number'],
                    lines=30
                )

        elif job['status'] == JobStatus.RUNNING.value:
            # Job en cours = estimer progression
            if job['input_type'] and job['run_number']:
                progress = await launcher.estimate_progress(
                    sample_id=job['sample_id'],
                    run_number=job['run_number'],
                    input_type=InputType(job['input_type'])
                )
            else:
                progress = 0

            # RÃ©cupÃ©rer aperÃ§u des logs
            if job['run_number']:
                logs_preview = await launcher.get_log_tail(
                    sample_id=job['sample_id'],
                    run_number=job['run_number'],
                    lines=30
                )

                # Extraire l'Ã©tape actuelle du log
                if logs_preview:
                    # Chercher derniÃ¨re ligne avec [INFO]
                    for line in reversed(logs_preview.split('\n')):
                        if '[INFO]' in line:
                            current_step = line.split('[INFO]')[-1].strip()[:100]
                            break

        else:
            # PENDING ou autre statut
            progress = 0
            current_step = "En attente de dÃ©marrage"

        return JobStatusResponse(
            job_id=job_id,
            sample_id=job['sample_id'],
            status=JobStatus(job['status']),
            input_type=InputType(job['input_type']) if job['input_type'] else None,
            run_number=job['run_number'],
            progress=progress,
            current_step=current_step,
            created_at=job['created_at'],
            started_at=job['started_at'],
            completed_at=job['completed_at'],
            exit_code=job['exit_code'],
            error_message=job['error_message'],
            logs_preview=logs_preview
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Erreur rÃ©cupÃ©ration statut: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur lors de la rÃ©cupÃ©ration du statut"
        )


@app.get("/api/results/{job_id}", response_model=AnalysisResults)
async def get_job_results(job_id: str):
    """
    RÃ©cupÃ¨re les rÃ©sultats d'une analyse terminÃ©e

    Args:
        job_id: ID du job

    Returns:
        AnalysisResults avec gÃ¨nes ARG dÃ©tectÃ©s, stats assemblage, etc.

    Raises:
        HTTPException 404: Si job non trouvÃ©
        HTTPException 400: Si job pas encore terminÃ©
        HTTPException 500: Si erreur parsing rÃ©sultats
    """
    try:
        # RÃ©cupÃ©rer le job
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} non trouvÃ©"
            )

        # VÃ©rifier que le job est terminÃ©
        if job['status'] != JobStatus.COMPLETED.value:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Job {job_id} pas encore terminÃ© (statut: {job['status']})"
            )

        # Parser les rÃ©sultats
        parser = OutputParser(job['output_dir'])

        # Parser dÃ©tection ARG (brut par outil)
        arg_detection = parser.parse_all_arg_detection()

        # Parser dÃ©tection ARG avec dÃ©duplication (comme le rapport HTML)
        deduplicated_data = parser.parse_all_arg_deduplicated()
        deduplicated_genes = [
            DeduplicatedGene(**gene) for gene in deduplicated_data['genes']
        ]
        dedup_stats = DeduplicationStats(
            total_raw=deduplicated_data['stats']['total_raw'],
            total_deduplicated=deduplicated_data['stats']['total_deduplicated'],
            duplicates_removed=deduplicated_data['stats']['duplicates_removed'],
            by_type=deduplicated_data['stats']['by_type']
        )

        # Parser stats assemblage (si disponible)
        assembly_stats = parser.parse_assembly_stats()

        # Parser informations taxonomiques
        taxonomy_info = parser.parse_taxonomy()
        ncbi_info = parser.fetch_ncbi_organism(job['sample_id'], job['input_type'])
        if taxonomy_info and ncbi_info:
            taxonomy_info['ncbi'] = ncbi_info
        elif ncbi_info:
            taxonomy_info = {'ncbi': ncbi_info, 'source': 'NCBI'}
        mlst_info = parser.parse_mlst()

        # Trouver rapport HTML
        report_html_path = parser.get_report_html_path()

        # Calculer statistiques globales
        total_arg_genes_raw = sum(r.num_genes for r in arg_detection.values())
        total_unique_genes = deduplicated_data['stats']['total_deduplicated']
        unique_resistance_types = parser.get_unique_resistance_types(arg_detection)

        return AnalysisResults(
            job_id=job_id,
            sample_id=job['sample_id'],
            run_number=job['run_number'],
            input_type=InputType(job['input_type']),
            assembly_stats=assembly_stats,
            arg_detection=arg_detection,
            deduplicated_genes=deduplicated_genes,
            deduplication_stats=dedup_stats,
            total_arg_genes=total_arg_genes_raw,
            total_unique_genes=total_unique_genes,
            unique_resistance_types=unique_resistance_types,
            taxonomy=taxonomy_info,
            mlst=mlst_info,
            report_html_path=report_html_path,
            output_directory=job['output_dir'],
            completed_at=job['completed_at']
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Erreur rÃ©cupÃ©ration rÃ©sultats: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur lors du parsing des rÃ©sultats"
        )


@app.get("/api/jobs", response_model=JobListResponse)
async def list_jobs(
    status_filter: Optional[JobStatus] = None,
    limit: int = 100,
    offset: int = 0
):
    """
    Liste tous les jobs avec filtres optionnels

    Args:
        status_filter: Filtrer par statut (optionnel)
        limit: Nombre maximum de rÃ©sultats (dÃ©faut: 100)
        offset: Offset pour pagination (dÃ©faut: 0)

    Returns:
        JobListResponse avec liste des jobs
    """
    try:
        # RÃ©cupÃ©rer les jobs
        jobs = await db.get_jobs(status=status_filter, limit=limit, offset=offset)
        total = await db.count_jobs(status=status_filter)

        # Convertir en JobListItem
        job_items = [
            JobListItem(
                job_id=job['id'],
                sample_id=job['sample_id'],
                status=JobStatus(job['status']),
                input_type=InputType(job['input_type']) if job['input_type'] else None,
                created_at=job['created_at'],
                completed_at=job['completed_at']
            )
            for job in jobs
        ]

        return JobListResponse(
            total=total,
            jobs=job_items
        )

    except Exception as e:
        logger.error(f"âŒ Erreur listing jobs: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur lors de la rÃ©cupÃ©ration de la liste des jobs"
        )


@app.delete("/api/jobs/{job_id}")
async def delete_job(job_id: str):
    """
    Supprime un job spÃ©cifique

    Args:
        job_id: ID du job Ã  supprimer

    Returns:
        Message de confirmation

    Raises:
        HTTPException 404: Si job non trouvÃ©
    """
    try:
        # VÃ©rifier que le job existe
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} non trouvÃ©"
            )

        # Supprimer le job
        await db.delete_job(job_id)
        logger.info(f"ðŸ—‘ï¸ Job {job_id} supprimÃ©")

        return {"message": f"Job {job_id} supprimÃ© avec succÃ¨s"}

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Erreur suppression job: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur lors de la suppression du job"
        )


@app.delete("/api/jobs")
async def delete_all_jobs():
    """
    Supprime TOUS les jobs (pour nettoyage)

    Returns:
        Message de confirmation avec nombre de jobs supprimÃ©s
    """
    try:
        count = await db.delete_all_jobs()
        logger.warning(f"ðŸ—‘ï¸ Tous les jobs supprimÃ©s ({count} jobs)")

        return {"message": f"{count} jobs supprimÃ©s avec succÃ¨s"}

    except Exception as e:
        logger.error(f"âŒ Erreur suppression tous jobs: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur lors de la suppression de tous les jobs"
        )


# ============================================================================
# ARRÃŠT DE JOB
# ============================================================================

@app.post("/api/jobs/{job_id}/stop")
async def stop_job(job_id: str):
    """
    ArrÃªte un job en cours d'exÃ©cution

    Args:
        job_id: ID du job Ã  arrÃªter

    Returns:
        Message de confirmation

    Raises:
        HTTPException 404: Si job non trouvÃ©
        HTTPException 400: Si job pas en cours d'exÃ©cution
    """
    try:
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} non trouvÃ©"
            )

        if job['status'] != JobStatus.RUNNING.value:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"Job {job_id} n'est pas en cours d'exÃ©cution (statut: {job['status']})"
            )

        # RÃ©cupÃ©rer le PID
        pid = job.get('pid')

        # Tenter de tuer le processus si PID disponible
        process_killed = False
        if pid:
            try:
                process_killed = await launcher.kill_job(pid)
            except Exception as e:
                logger.warning(f"Impossible de tuer le processus {pid}: {e}")

        # TOUJOURS mettre Ã  jour le statut dans la base de donnÃ©es
        await db.update_job_status(
            job_id=job_id,
            status=JobStatus.FAILED,
            completed_at=datetime.now(),
            error_message="ArrÃªtÃ© manuellement par l'utilisateur"
        )

        logger.info(f"ðŸ›‘ Job {job_id} marquÃ© comme arrÃªtÃ© (processus tuÃ©: {process_killed})")

        return {
            "message": f"Job {job_id} arrÃªtÃ© avec succÃ¨s",
            "success": True,
            "process_killed": process_killed
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Erreur arrÃªt job: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur lors de l'arrÃªt du job"
        )


# ============================================================================
# LISTE DES FICHIERS D'UN JOB
# ============================================================================

@app.get("/api/jobs/{job_id}/files")
async def list_job_files(job_id: str):
    """
    Liste tous les fichiers gÃ©nÃ©rÃ©s par un job

    Args:
        job_id: ID du job

    Returns:
        Liste des fichiers avec leurs mÃ©tadonnÃ©es
    """
    try:
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Job {job_id} non trouvÃ©"
            )

        output_dir = job.get('output_dir')
        if not output_dir or not Path(output_dir).exists():
            return {"files": [], "output_dir": output_dir, "message": "RÃ©pertoire de sortie non trouvÃ©"}

        output_path = Path(output_dir)
        files = []

        # Parcourir tous les fichiers rÃ©cursivement
        for file_path in output_path.rglob("*"):
            if file_path.is_file():
                # DÃ©terminer le type de fichier
                suffix = file_path.suffix.lower()
                file_type = "other"
                icon = "ðŸ“„"

                if suffix in ['.tsv', '.csv']:
                    file_type = "data"
                    icon = "ðŸ“Š"
                elif suffix in ['.html', '.htm']:
                    file_type = "report"
                    icon = "ðŸ“‘"
                elif suffix in ['.log', '.txt']:
                    file_type = "log"
                    icon = "ðŸ“‹"
                elif suffix in ['.fasta', '.fna', '.fa', '.faa', '.ffn']:
                    file_type = "sequence"
                    icon = "ðŸ§¬"
                elif suffix in ['.gff', '.gff3', '.gbk', '.gb']:
                    file_type = "annotation"
                    icon = "ðŸ“"
                elif suffix in ['.json']:
                    file_type = "json"
                    icon = "ðŸ”§"
                elif suffix in ['.png', '.jpg', '.jpeg', '.svg', '.pdf']:
                    file_type = "image"
                    icon = "ðŸ–¼ï¸"

                # Chemin relatif depuis output_dir
                relative_path = file_path.relative_to(output_path)

                # CatÃ©gorie basÃ©e sur le dossier parent
                parts = relative_path.parts
                category = parts[0] if len(parts) > 1 else "root"

                files.append({
                    "name": file_path.name,
                    "path": str(file_path),
                    "relative_path": str(relative_path),
                    "size": file_path.stat().st_size,
                    "size_human": format_size(file_path.stat().st_size),
                    "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
                    "type": file_type,
                    "icon": icon,
                    "category": category,
                    "extension": suffix
                })

        # Trier par catÃ©gorie puis par nom
        files.sort(key=lambda x: (x['category'], x['name']))

        # Grouper par catÃ©gorie
        categories = {}
        for f in files:
            cat = f['category']
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(f)

        return {
            "output_dir": str(output_path),
            "total_files": len(files),
            "total_size": format_size(sum(f['size'] for f in files)),
            "files": files,
            "categories": categories
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Erreur listing fichiers job: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Erreur lors du listing des fichiers"
        )


@app.get("/api/jobs/{job_id}/files/download/{file_path:path}")
async def download_job_file(job_id: str, file_path: str):
    """
    TÃ©lÃ©charge un fichier spÃ©cifique d'un job
    """
    from fastapi.responses import FileResponse

    try:
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} non trouvÃ©")

        output_dir = job.get('output_dir')
        if not output_dir:
            raise HTTPException(status_code=404, detail="RÃ©pertoire de sortie non trouvÃ©")

        full_path = Path(output_dir) / file_path

        # SÃ©curitÃ©: vÃ©rifier que le chemin est bien dans output_dir
        if not str(full_path.resolve()).startswith(str(Path(output_dir).resolve())):
            raise HTTPException(status_code=403, detail="AccÃ¨s non autorisÃ©")

        if not full_path.exists():
            raise HTTPException(status_code=404, detail=f"Fichier non trouvÃ©: {file_path}")

        return FileResponse(
            path=str(full_path),
            filename=full_path.name,
            media_type="application/octet-stream"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur tÃ©lÃ©chargement fichier: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors du tÃ©lÃ©chargement du fichier")


@app.get("/api/jobs/{job_id}/files/view/{file_path:path}")
async def view_job_file(job_id: str, file_path: str, lines: int = 500):
    """
    Affiche le contenu d'un fichier texte d'un job
    """
    try:
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} non trouvÃ©")

        output_dir = job.get('output_dir')
        if not output_dir:
            raise HTTPException(status_code=404, detail="RÃ©pertoire de sortie non trouvÃ©")

        full_path = Path(output_dir) / file_path

        # SÃ©curitÃ©
        if not str(full_path.resolve()).startswith(str(Path(output_dir).resolve())):
            raise HTTPException(status_code=403, detail="AccÃ¨s non autorisÃ©")

        if not full_path.exists():
            raise HTTPException(status_code=404, detail=f"Fichier non trouvÃ©: {file_path}")

        # VÃ©rifier taille
        size = full_path.stat().st_size
        if size > 10 * 1024 * 1024:  # 10 MB max
            return {
                "error": "Fichier trop volumineux pour affichage",
                "size": format_size(size),
                "path": str(full_path)
            }

        # Lire le contenu
        try:
            content = full_path.read_text(encoding='utf-8', errors='ignore')
            content_lines = content.split('\n')

            return {
                "name": full_path.name,
                "path": str(full_path),
                "size": format_size(size),
                "total_lines": len(content_lines),
                "lines_returned": min(lines, len(content_lines)),
                "content": '\n'.join(content_lines[:lines]),
                "truncated": len(content_lines) > lines
            }
        except Exception as e:
            return {
                "error": f"Impossible de lire le fichier: {e}",
                "path": str(full_path)
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur affichage fichier: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors de l'affichage du fichier")


@app.get("/api/jobs/{job_id}/files/serve/{file_path:path}")
async def serve_job_file(job_id: str, file_path: str):
    """
    Sert un fichier directement (HTML, images, etc.) pour affichage dans le navigateur
    """
    from fastapi.responses import FileResponse
    import mimetypes

    try:
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} non trouvÃ©")

        output_dir = job.get('output_dir')
        if not output_dir:
            raise HTTPException(status_code=404, detail="RÃ©pertoire de sortie non trouvÃ©")

        full_path = Path(output_dir) / file_path

        # SÃ©curitÃ© : vÃ©rifier que le chemin est bien dans le rÃ©pertoire de sortie
        if not str(full_path.resolve()).startswith(str(Path(output_dir).resolve())):
            raise HTTPException(status_code=403, detail="AccÃ¨s non autorisÃ©")

        if not full_path.exists():
            raise HTTPException(status_code=404, detail=f"Fichier non trouvÃ©: {file_path}")

        # DÃ©terminer le type MIME
        mime_type, _ = mimetypes.guess_type(str(full_path))
        if mime_type is None:
            mime_type = "application/octet-stream"

        # Pour les fichiers HTML, servir avec le bon content-type
        return FileResponse(
            path=full_path,
            media_type=mime_type,
            filename=full_path.name
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur service fichier: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors du service du fichier")


# ============================================================================
# GESTION DES BASES DE DONNÃ‰ES
# ============================================================================

def _validate_db_key(db_key: str) -> str:
    """Valide et retourne la clÃ© de base de donnÃ©es"""
    if not re_module.match(r'^[a-z0-9_]+$', db_key):
        raise HTTPException(status_code=400, detail="ClÃ© de base invalide")
    if db_key not in DATABASES_CONFIG:
        raise HTTPException(status_code=404, detail=f"Base '{db_key}' non trouvÃ©e")
    return db_key


DATABASES_DIR = PIPELINE_DIR / "databases"

# DÃ©tecter conda et l'environnement arg_detection
CONDA_BASE = subprocess.getoutput("conda info --base 2>/dev/null").strip()
CONDA_INIT = f"{CONDA_BASE}/etc/profile.d/conda.sh" if CONDA_BASE else None
CONDA_ARG_ENV = "arg_detection"


def _conda_wrap(cmd: str, env: str = None) -> str:
    """Wrap une commande avec l'activation conda si disponible"""
    if CONDA_INIT and Path(CONDA_INIT).exists():
        if env:
            return f"source {CONDA_INIT} && conda activate {env} && {cmd}"
        return f"source {CONDA_INIT} && {cmd}"
    if env:
        return f"conda run -n {env} {cmd}"
    return cmd

# Configuration des bases de donnÃ©es
DATABASES_CONFIG = {
    "kraken2": {
        "name": "Kraken2",
        "description": "Classification taxonomique",
        "path": "kraken2_db",
        "check_files": ["hash.k2d"],
        "size_estimate": "8 GB (minikraken) / 70 GB (standard)",
        "update_cmd": "download_kraken2_db"
    },
    "amrfinder": {
        "name": "AMRFinderPlus",
        "description": "DÃ©tection ARG (NCBI)",
        "path": "amrfinder_db",
        "check_files": ["AMRProt", "AMR.LIB"],
        "size_estimate": "~200 MB",
        "update_cmd": "amrfinder_update --force_update --database {path}"
    },
    "card": {
        "name": "CARD",
        "description": "Comprehensive Antibiotic Resistance Database",
        "path": "card_db",
        "check_files": ["card.json", "protein_fasta_protein_homolog_model.fasta"],
        "size_estimate": "~1 GB",
        "update_cmd": "download_card_db"
    },
    "pointfinder": {
        "name": "PointFinder",
        "description": "Mutations de rÃ©sistance",
        "path": "pointfinder_db",
        "check_files": ["config"],
        "size_estimate": "~3 MB",
        "update_cmd": "git clone https://bitbucket.org/genomicepidemiology/pointfinder_db.git {path}"
    },
    "mlst": {
        "name": "MLST",
        "description": "Multi-Locus Sequence Typing",
        "path": "mlst_db",
        "check_files": ["pubmlst"],
        "size_estimate": "~200 MB",
        "update_cmd": "download_mlst_db"
    },
    "kma": {
        "name": "KMA/ResFinder",
        "description": "Index KMA pour ResFinder",
        "path": "kma_db",
        "check_files": ["resfinder.name"],
        "size_estimate": "~60 MB",
        "update_cmd": "setup_kma_database"
    }
}

# Suivi des tÃ©lÃ©chargements en cours
db_download_tasks = {}  # {db_key: {active, status, progress, message, ...}}
db_download_lock = threading.Lock()


def _update_download_progress(db_key: str, **kwargs):
    """Met Ã  jour la progression d'un tÃ©lÃ©chargement"""
    with db_download_lock:
        if db_key in db_download_tasks:
            db_download_tasks[db_key].update(kwargs)


def _run_db_download(db_key: str):
    """ExÃ©cute le tÃ©lÃ©chargement d'une base de donnÃ©es en arriÃ¨re-plan"""
    config = DATABASES_CONFIG[db_key]
    db_path = DATABASES_DIR / config["path"]
    db_path.mkdir(parents=True, exist_ok=True)

    logger.info(f"[DB Download] DÃ©but tÃ©lÃ©chargement: {db_key}")

    try:
        if db_key == "amrfinder":
            cmd = _conda_wrap(f"amrfinder_update --force_update --database {db_path}", CONDA_ARG_ENV)
            fallback = _conda_wrap(f"amrfinder --force_update --database {db_path}", CONDA_ARG_ENV)
            _download_with_command(db_key, f"bash -c '{cmd}'",
                                   fallback_cmd=f"bash -c '{fallback}'",
                                   timeout=1800)

        elif db_key == "pointfinder":
            _update_download_progress(db_key, message="Suppression ancienne version...", progress=-1)
            if db_path.exists():
                shutil.rmtree(db_path)
            _download_with_command(db_key,
                                   ["git", "clone", "https://bitbucket.org/genomicepidemiology/pointfinder_db.git", str(db_path)],
                                   timeout=600, use_shell=False)

        elif db_key == "card":
            _download_with_wget(db_key, "https://card.mcmaster.ca/latest/data",
                                db_path, "card-data.tar.bz2", extract_cmd="tar -xjf")

        elif db_key == "mlst":
            cmd = _conda_wrap("mlst --update 2>&1 || echo 'MLST update done'", CONDA_ARG_ENV)
            _download_with_command(db_key, f"bash -c '{cmd}'", timeout=1800)

        elif db_key == "kraken2":
            # Supprimer d'Ã©ventuelles archives corrompues
            corrupted = db_path / "kraken2_db.tar.gz"
            if corrupted.exists():
                corrupted.unlink()
            _download_with_wget(db_key, "https://genome-idx.s3.amazonaws.com/kraken/k2_standard_08gb_20231009.tar.gz",
                                db_path, "kraken2_db.tar.gz", extract_cmd="tar -xzf")

        elif db_key == "kma":
            _download_kma_database(db_key, db_path)

        # VÃ©rifier rÃ©sultat final
        new_status = get_db_status(db_key)
        success = new_status["ready"]

        _update_download_progress(
            db_key,
            active=True,
            status="completed" if success else "failed",
            progress=100 if success else 0,
            message=f"{'TerminÃ© avec succÃ¨s' if success else 'Ã‰chec - fichiers manquants'}",
        )
        logger.info(f"[DB Download] {db_key} terminÃ©: {'succÃ¨s' if success else 'Ã©chec'}")

    except Exception as e:
        logger.error(f"[DB Download] Erreur {db_key}: {e}")
        _update_download_progress(
            db_key,
            active=True,
            status="failed",
            progress=0,
            message=f"Erreur: {str(e)[:200]}",
            error=str(e)[:500],
        )

    # Garder le statut final visible pendant 30s puis nettoyer
    time.sleep(30)
    with db_download_lock:
        db_download_tasks.pop(db_key, None)


def _download_with_wget(db_key: str, url: str, db_path: Path, archive_name: str, extract_cmd: str):
    """TÃ©lÃ©chargement avec wget et suivi de progression via taille du fichier"""
    archive_path = db_path / archive_name

    # Ã‰tape 1: Obtenir la taille totale via HEAD request
    _update_download_progress(db_key, message="RÃ©cupÃ©ration des informations...", progress=0)
    total_bytes = 0
    try:
        import urllib.request
        req = urllib.request.Request(url, method='HEAD')
        with urllib.request.urlopen(req, timeout=30) as resp:
            total_bytes = int(resp.headers.get('Content-Length', 0))
        if total_bytes:
            logger.info(f"[DB Download] {db_key}: taille totale = {format_size(total_bytes)}")
    except Exception as e:
        logger.warning(f"[DB Download] {db_key}: impossible d'obtenir la taille: {e}")

    _update_download_progress(
        db_key,
        status="downloading",
        message="TÃ©lÃ©chargement en cours...",
        total_bytes=total_bytes,
        downloaded_bytes=0,
    )

    # Ã‰tape 2: Lancer wget
    process = subprocess.Popen(
        ["wget", "-O", archive_name, url],
        cwd=str(db_path), stdout=subprocess.PIPE, stderr=subprocess.PIPE
    )

    # Ã‰tape 3: Monitorer la taille du fichier pendant le tÃ©lÃ©chargement
    last_size = 0
    last_time = time.time()
    while process.poll() is None:
        time.sleep(1)
        try:
            if archive_path.exists():
                current_size = archive_path.stat().st_size
                now = time.time()
                elapsed = now - last_time
                speed = (current_size - last_size) / elapsed if elapsed > 0 else 0
                progress = int((current_size / total_bytes) * 100) if total_bytes > 0 else -1

                _update_download_progress(
                    db_key,
                    progress=min(progress, 99) if progress >= 0 else -1,
                    downloaded_bytes=current_size,
                    speed=format_size(int(speed)) + "/s" if speed > 0 else "",
                    message=f"TÃ©lÃ©chargement: {format_size(current_size)}" +
                            (f" / {format_size(total_bytes)}" if total_bytes > 0 else ""),
                )
                last_size = current_size
                last_time = now
        except Exception:
            pass

    # VÃ©rifier le rÃ©sultat wget
    if process.returncode != 0:
        stderr = process.stderr.read().decode(errors="replace")[-500:]
        raise Exception(f"wget a Ã©chouÃ© (code {process.returncode}): {stderr}")

    # Ã‰tape 4: Extraction
    _update_download_progress(
        db_key,
        status="extracting",
        progress=-1,
        message="Extraction de l'archive...",
        speed="",
    )
    logger.info(f"[DB Download] {db_key}: extraction en cours...")
    extract_parts = extract_cmd.split() + [archive_name]
    extract_result = subprocess.run(
        extract_parts,
        cwd=str(db_path), capture_output=True, text=True, timeout=1800
    )
    if extract_result.returncode != 0:
        raise Exception(f"Extraction Ã©chouÃ©e: {extract_result.stderr[:300]}")

    # Ã‰tape 5: Nettoyage
    _update_download_progress(db_key, message="Nettoyage...", progress=95)
    try:
        archive_path.unlink(missing_ok=True)
    except Exception:
        pass


def _download_kma_database(db_key: str, db_path: Path):
    """CrÃ©e les index KMA Ã  partir des bases abricate (resfinder, card, ncbi)"""
    _update_download_progress(db_key, status="downloading", progress=-1,
                              message="Recherche des bases abricate...", speed="")

    # Trouver le rÃ©pertoire des bases abricate via conda
    abricate_db_dir = None
    try:
        result = subprocess.run(
            f"bash -c '{_conda_wrap('abricate --datadir', CONDA_ARG_ENV)}'",
            shell=True, capture_output=True, text=True, timeout=30
        )
        if result.returncode == 0 and result.stdout.strip():
            candidate = Path(result.stdout.strip())
            if candidate.is_dir():
                abricate_db_dir = candidate
    except Exception as e:
        logger.warning(f"[DB Download] kma: abricate --datadir failed: {e}")

    # Fallback: chemins connus
    if not abricate_db_dir:
        for p in [
            Path(CONDA_BASE) / "envs" / CONDA_ARG_ENV / "db" if CONDA_BASE else None,
            Path.home() / "miniconda3" / "envs" / CONDA_ARG_ENV / "db",
            Path.home() / "anaconda3" / "envs" / CONDA_ARG_ENV / "db",
        ]:
            if p and p.is_dir():
                abricate_db_dir = p
                break

    if not abricate_db_dir:
        raise Exception("Bases abricate non trouvÃ©es. Installez abricate dans l'env conda 'arg_detection'.")

    logger.info(f"[DB Download] kma: bases abricate trouvÃ©es: {abricate_db_dir}")
    db_path.mkdir(parents=True, exist_ok=True)

    # CrÃ©er les index KMA pour chaque base
    databases_to_index = ["resfinder", "card", "ncbi"]
    for i, db_name in enumerate(databases_to_index):
        seq_file = abricate_db_dir / db_name / "sequences"
        if not seq_file.exists():
            logger.warning(f"[DB Download] kma: sÃ©quences non trouvÃ©es: {seq_file}")
            continue

        progress_pct = int((i / len(databases_to_index)) * 80)
        _update_download_progress(db_key, progress=progress_pct,
                                  message=f"Indexation KMA: {db_name}... ({i+1}/{len(databases_to_index)})")

        result = subprocess.run(
            ["kma", "index", "-i", str(seq_file), "-o", str(db_path / db_name)],
            capture_output=True, text=True, timeout=300
        )
        if result.returncode != 0:
            logger.warning(f"[DB Download] kma index {db_name} failed: {result.stderr[:200]}")
        elif (db_path / f"{db_name}.name").exists():
            logger.info(f"[DB Download] kma: index crÃ©Ã©: {db_name}")

    # VÃ©rification finale
    if not (db_path / "resfinder.name").exists():
        raise Exception("Ã‰chec crÃ©ation index KMA resfinder")

    _update_download_progress(db_key, progress=90, message="Index KMA crÃ©Ã©s avec succÃ¨s")


def _download_with_command(db_key: str, cmd, fallback_cmd=None, timeout: int = 1800, use_shell: bool = True):
    """ExÃ©cute une commande de tÃ©lÃ©chargement avec progression indÃ©terminÃ©e.

    Args:
        cmd: Commande (str si use_shell=True, list si use_shell=False)
        fallback_cmd: Commande alternative (mÃªme format que cmd)
        timeout: Timeout en secondes
        use_shell: True pour les commandes nÃ©cessitant shell (conda), False sinon
    """
    _update_download_progress(
        db_key,
        status="downloading",
        progress=-1,
        message="Installation en cours...",
        speed="",
    )

    process = subprocess.Popen(cmd, shell=use_shell, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    # Monitorer le process avec messages de vie
    start_time = time.time()
    while process.poll() is None:
        time.sleep(2)
        elapsed = int(time.time() - start_time)
        mins, secs = divmod(elapsed, 60)
        _update_download_progress(
            db_key,
            message=f"Installation en cours... ({mins}m{secs:02d}s)",
        )
        if elapsed > timeout:
            process.kill()
            raise Exception(f"Timeout aprÃ¨s {timeout}s")

    if process.returncode != 0 and fallback_cmd:
        logger.info(f"[DB Download] {db_key}: commande principale Ã©chouÃ©e, essai fallback...")
        _update_download_progress(db_key, message="Essai mÃ©thode alternative...")
        process = subprocess.Popen(fallback_cmd, shell=use_shell, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        start_time = time.time()
        while process.poll() is None:
            time.sleep(2)
            elapsed = int(time.time() - start_time)
            mins, secs = divmod(elapsed, 60)
            _update_download_progress(
                db_key,
                message=f"MÃ©thode alternative... ({mins}m{secs:02d}s)",
            )
            if elapsed > timeout:
                process.kill()
                raise Exception(f"Timeout aprÃ¨s {timeout}s")

    if process.returncode != 0:
        stderr = process.stderr.read().decode(errors="replace")[-500:]
        raise Exception(f"Commande Ã©chouÃ©e (code {process.returncode}): {stderr}")


def get_db_status(db_key: str) -> dict:
    """VÃ©rifie le statut d'une base de donnÃ©es"""
    config = DATABASES_CONFIG.get(db_key)
    if not config:
        return None

    db_path = DATABASES_DIR / config["path"]

    # VÃ©rifier si le dossier existe
    exists = db_path.exists()

    # VÃ©rifier si les fichiers requis sont prÃ©sents
    ready = False
    if exists:
        for check_file in config["check_files"]:
            # Chercher le fichier (peut Ãªtre dans un sous-dossier)
            found = list(db_path.rglob(check_file))
            if found:
                ready = True
                break

    # Calculer la taille
    size = 0
    if exists:
        try:
            for f in db_path.rglob("*"):
                if f.is_file():
                    size += f.stat().st_size
        except OSError:
            pass

    # Date de derniÃ¨re modification
    last_updated = None
    if exists:
        try:
            last_updated = datetime.fromtimestamp(db_path.stat().st_mtime).isoformat()
        except OSError:
            pass

    return {
        "key": db_key,
        "name": config["name"],
        "description": config["description"],
        "path": str(db_path),
        "exists": exists,
        "ready": ready,
        "size_bytes": size,
        "size_human": format_size(size),
        "size_estimate": config["size_estimate"],
        "last_updated": last_updated
    }


def format_size(size_bytes: int) -> str:
    """Formate une taille en bytes en format lisible"""
    if size_bytes == 0:
        return "0 B"
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if size_bytes < 1024:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.1f} PB"


@app.get("/api/databases")
async def list_databases():
    """Liste toutes les bases de donnÃ©es avec leur statut"""
    databases = []
    for db_key in DATABASES_CONFIG:
        status = get_db_status(db_key)
        if status:
            databases.append(status)

    return {
        "databases_dir": str(DATABASES_DIR),
        "databases": databases
    }


@app.get("/api/databases/{db_key}")
async def get_database_status(db_key: str):
    """RÃ©cupÃ¨re le statut d'une base de donnÃ©es spÃ©cifique"""
    _validate_db_key(db_key)
    db_status = get_db_status(db_key)
    if not db_status:
        raise HTTPException(
            status_code=404,
            detail=f"Base de donnÃ©es '{db_key}' non trouvÃ©e"
        )
    return db_status


@app.post("/api/databases/{db_key}/update")
async def update_database(db_key: str):
    """
    Lance la mise Ã  jour/tÃ©lÃ©chargement d'une base de donnÃ©es en arriÃ¨re-plan.
    Retourne immÃ©diatement. Utiliser GET /api/databases/{db_key}/progress pour suivre.
    """
    _validate_db_key(db_key)
    if db_key not in DATABASES_CONFIG:
        raise HTTPException(
            status_code=404,
            detail=f"Base de donnÃ©es '{db_key}' non trouvÃ©e"
        )

    # VÃ©rifier si un tÃ©lÃ©chargement est dÃ©jÃ  en cours
    with db_download_lock:
        if db_key in db_download_tasks and db_download_tasks[db_key].get("active"):
            current = db_download_tasks[db_key]
            if current.get("status") not in ("completed", "failed"):
                raise HTTPException(
                    status_code=409,
                    detail=f"TÃ©lÃ©chargement dÃ©jÃ  en cours pour '{db_key}'"
                )

    config = DATABASES_CONFIG[db_key]

    # Initialiser le tracking
    with db_download_lock:
        db_download_tasks[db_key] = {
            "active": True,
            "status": "starting",
            "progress": 0,
            "message": "DÃ©marrage...",
            "downloaded_bytes": 0,
            "total_bytes": 0,
            "speed": "",
            "started_at": datetime.now().isoformat(),
            "error": None,
        }

    # Lancer en arriÃ¨re-plan
    thread = threading.Thread(target=_run_db_download, args=(db_key,), daemon=True)
    thread.start()

    logger.info(f"[DB Download] TÃ©lÃ©chargement lancÃ© en arriÃ¨re-plan: {db_key}")

    return {
        "started": True,
        "message": f"TÃ©lÃ©chargement de {config['name']} lancÃ© en arriÃ¨re-plan",
        "db_key": db_key,
    }


@app.get("/api/databases/{db_key}/progress")
async def get_db_download_progress(db_key: str):
    """Retourne la progression du tÃ©lÃ©chargement d'une base de donnÃ©es"""
    _validate_db_key(db_key)
    with db_download_lock:
        if db_key in db_download_tasks:
            return dict(db_download_tasks[db_key])
    return {"active": False}


# ============================================================================
# SUPPRESSION DES FICHIERS DE JOBS
# ============================================================================

@app.delete("/api/jobs/{job_id}/files")
async def delete_job_files(job_id: str, delete_outputs: bool = True, delete_data: bool = False):
    """
    Supprime les fichiers associÃ©s Ã  un job

    Args:
        job_id: ID du job
        delete_outputs: Supprimer les rÃ©sultats (dÃ©faut: True)
        delete_data: Supprimer les donnÃ©es tÃ©lÃ©chargÃ©es SRA/Assembly (dÃ©faut: False)
    """
    try:
        job = await db.get_job(job_id)
        if not job:
            raise HTTPException(status_code=404, detail=f"Job {job_id} non trouvÃ©")

        deleted = []
        errors = []

        # Supprimer les outputs
        if delete_outputs and job.get('output_dir'):
            output_path = Path(job['output_dir'])
            if output_path.exists():
                try:
                    shutil.rmtree(output_path)
                    deleted.append(f"Outputs: {output_path}")
                    logger.info(f"ðŸ—‘ï¸ SupprimÃ© outputs: {output_path}")
                except Exception as e:
                    errors.append(f"Erreur suppression outputs: {e}")

        # Supprimer les donnÃ©es tÃ©lÃ©chargÃ©es (SRA/Assembly)
        if delete_data:
            sample_id = job['sample_id']
            data_dir = PIPELINE_DIR / "data"

            # Chercher les fichiers liÃ©s Ã  cet Ã©chantillon
            patterns = [
                f"{sample_id}*",
                f"*{sample_id}*"
            ]

            for pattern in patterns:
                for f in data_dir.glob(pattern):
                    try:
                        if f.is_dir():
                            shutil.rmtree(f)
                        else:
                            f.unlink()
                        deleted.append(f"Data: {f}")
                        logger.info(f"ðŸ—‘ï¸ SupprimÃ© data: {f}")
                    except Exception as e:
                        errors.append(f"Erreur suppression {f}: {e}")

        return {
            "message": f"Fichiers du job {job_id} supprimÃ©s",
            "deleted": deleted,
            "errors": errors if errors else None
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Erreur suppression fichiers job {job_id}: {e}")
        raise HTTPException(status_code=500, detail="Erreur lors de la suppression des fichiers")


@app.delete("/api/outputs/{sample_id}")
async def delete_sample_outputs(sample_id: str):
    """Supprime tous les outputs d'un Ã©chantillon (tous les runs)"""
    outputs_dir = PIPELINE_DIR / "outputs"
    deleted = []

    for d in outputs_dir.glob(f"{sample_id}*"):
        if d.is_dir():
            try:
                shutil.rmtree(d)
                deleted.append(str(d))
                logger.info(f"ðŸ—‘ï¸ SupprimÃ©: {d}")
            except Exception as e:
                logger.error(f"Erreur suppression {d}: {e}")

    return {
        "message": f"Outputs de {sample_id} supprimÃ©s",
        "deleted": deleted
    }


# ============================================================================
# MAIN (pour lancement direct)
# ============================================================================

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
